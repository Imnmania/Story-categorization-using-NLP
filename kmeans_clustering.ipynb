{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "mp.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load('word2vec.model').wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.Word2VecKeyedVectors"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Word2VecKeyedVectors in module gensim.models.keyedvectors object:\n",
      "\n",
      "class Word2VecKeyedVectors(WordEmbeddingsKeyedVectors)\n",
      " |  Mapping between words and vectors for the :class:`~gensim.models.Word2Vec` model.\n",
      " |  Used to perform operations on the vectors such as vector lookup, distance, similarity etc.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2VecKeyedVectors\n",
      " |      WordEmbeddingsKeyedVectors\n",
      " |      BaseKeyedVectors\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  get_keras_embedding(self, train_embeddings=False)\n",
      " |      Get a Keras 'Embedding' layer with weights set as the Word2Vec model's learned word embeddings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      train_embeddings : bool\n",
      " |          If False, the weights are frozen and stopped from being updated.\n",
      " |          If True, the weights can/will be further trained/updated.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `keras.layers.Embedding`\n",
      " |          Embedding layer.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `Keras <https://pypi.org/project/Keras/>`_ not installed.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      Current method work only if `Keras <https://pypi.org/project/Keras/>`_ installed.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None)\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path used to save the vectors in\n",
      " |      fvocab : str, optional\n",
      " |          Optional file path used to save the vocabulary\n",
      " |      binary : bool, optional\n",
      " |          If True, the data will be saved in binary word2vec format, else it will be saved in plain text.\n",
      " |      total_vec : int, optional\n",
      " |          Optional parameter to explicitly specify total no. of vectors\n",
      " |          (in case word vectors are appended with document vectors afterwards).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(fname_or_handle, **kwargs) from builtins.type\n",
      " |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to file that contains needed object.\n",
      " |      mmap : str, optional\n",
      " |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      " |          via mmap (shared memory) using `mmap='r'.\n",
      " |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.save`\n",
      " |          Save object to file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object\n",
      " |          Object loaded from `fname`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          When called on an object instance instead of class (this is a class method).\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
      " |      Load the input-hidden weight matrix from the original C word2vec-tool format.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      The information stored in the file is incomplete (the binary tree is missing),\n",
      " |      so while you can query for word similarity etc., you cannot continue training\n",
      " |      with a model loaded this way.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to the saved word2vec-format file.\n",
      " |      fvocab : str, optional\n",
      " |          File path to the vocabulary.Word counts are read from `fvocab` filename, if set\n",
      " |          (this is the file generated by `-save-vocab` flag of the original C tool).\n",
      " |      binary : bool, optional\n",
      " |          If True, indicates whether the data is in binary word2vec format.\n",
      " |      encoding : str, optional\n",
      " |          If you trained the C model using non-utf8 encoding for words, specify that encoding in `encoding`.\n",
      " |      unicode_errors : str, optional\n",
      " |          default 'strict', is a string suitable to be passed as the `errors`\n",
      " |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      " |          file may include word tokens truncated in the middle of a multibyte unicode character\n",
      " |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      " |      limit : int, optional\n",
      " |          Sets a maximum number of word-vectors to read from the file. The default,\n",
      " |          None, means read all.\n",
      " |      datatype : type, optional\n",
      " |          (Experimental) Can coerce dimensions to a non-default float type (such as `np.float16`) to save memory.\n",
      " |          Such types may result in much slower bulk operations or incompatibility with optimized routines.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |  \n",
      " |  __init__(self, vector_size)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=<function WordEmbeddingsKeyedVectors.most_similar at 0x7fe4d6270a60>, case_insensitive=True)\n",
      " |      Compute accuracy of the model.\n",
      " |      \n",
      " |      The accuracy is reported (=printed to log and returned as a list) for each\n",
      " |      section separately, plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      questions : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      most_similar : function, optional\n",
      " |          Function used for similarity calculation.\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of dict of (str, (str, str, str)\n",
      " |          Full lists of correct and incorrect predictions divided by sections.\n",
      " |  \n",
      " |  distance(self, w1, w2)\n",
      " |      Compute cosine distance between two words.\n",
      " |      Calculate 1 - :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Distance between `w1` and `w2`.\n",
      " |  \n",
      " |  distances(self, word_or_vector, other_words=())\n",
      " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
      " |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_or_vector : {str, numpy.ndarray}\n",
      " |          Word or vector from which distances are to be computed.\n",
      " |      other_words : iterable of str\n",
      " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
      " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Array containing distances to all words in `other_words` from input `word_or_vector`.\n",
      " |      \n",
      " |      Raises\n",
      " |      -----\n",
      " |      KeyError\n",
      " |          If either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Which word from the given list doesn't go with the others?\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of words.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          The word further away from the mean of all words.\n",
      " |  \n",
      " |  evaluate_word_analogies(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute performance of the model on an analogy test set.\n",
      " |      \n",
      " |      This is modern variant of :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.accuracy`, see\n",
      " |      `discussion on GitHub #1935 <https://github.com/RaRe-Technologies/gensim/pull/1935>`_.\n",
      " |      \n",
      " |      The accuracy is reported (printed to log and returned as a score) for each section separately,\n",
      " |      plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      " |      See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      analogies : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          The overall evaluation score on the entire evaluation set\n",
      " |      sections : list of dict of {str : str or list of tuple of (str, str, str, str)}\n",
      " |          Results broken down by each section of the evaluation set. Each dict contains the name of the section\n",
      " |          under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the\n",
      " |          keys 'correct' and 'incorrect'.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute correlation of the model with human similarity judgments.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      More datasets can be found at\n",
      " |      * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html\n",
      " |      * https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pairs : str\n",
      " |          Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.\n",
      " |          See `test/test_data/wordsim353.tsv` as example.\n",
      " |      delimiter : str, optional\n",
      " |          Separator in `pairs` file.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pearson : tuple of (float, float)\n",
      " |          Pearson correlation coefficient with 2-tailed p-value.\n",
      " |      spearman : tuple of (float, float)\n",
      " |          Spearman rank-order correlation coefficient between the similarities from the dataset and the\n",
      " |          similarities produced by the model itself, with 2-tailed p-value.\n",
      " |      oov_ratio : float\n",
      " |          The ratio of pairs with unknown words.\n",
      " |  \n",
      " |  get_vector(self, word)\n",
      " |      Get the entity's representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entity : str\n",
      " |          Identifier of the entity to return the vector for.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector for the specified entity.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If the given entity identifier doesn't exist.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool, optional\n",
      " |          If True - forget the original vectors and only keep the normalized ones = saves lots of memory!\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      You **cannot continue training** after doing a replace.\n",
      " |      The model becomes effectively read-only: you can call\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`,\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`, etc., but not train.\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Find the top-N most similar words.\n",
      " |      Positive words contribute positively towards the similarity, negative words negatively.\n",
      " |      \n",
      " |      This method computes cosine similarity between a simple mean of the projection\n",
      " |      weight vectors of the given words and the vectors for each word in the model.\n",
      " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      " |      word2vec implementation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Find the top-N most similar words, using the multiplicative combination objective,\n",
      " |      proposed by `Omer Levy and Yoav Goldberg \"Linguistic Regularities in Sparse and Explicit Word Representations\"\n",
      " |      <http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,\n",
      " |      negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n",
      " |      In the common analogy-solving case, of two positive and one negative examples,\n",
      " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
      " |      \n",
      " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
      " |      respectively - a potentially sensible but untested extension of the method.\n",
      " |      With a single positive example, rankings will be the same as in the default\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Compute cosine similarity between two sets of words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ws1 : list of str\n",
      " |          Sequence of words.\n",
      " |      ws2: list of str\n",
      " |          Sequence of words.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Similarities between `ws1` and `ws2`.\n",
      " |  \n",
      " |  relative_cosine_similarity(self, wa, wb, topn=10)\n",
      " |      Compute the relative cosine similarity between two words given top-n similar words,\n",
      " |      by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc \"A Minimally Supervised Approach\n",
      " |      for Synonym Extraction with Word Embeddings\" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.\n",
      " |      \n",
      " |      To calculate relative cosine similarity between two words, equation (1) of the paper is used.\n",
      " |      For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than\n",
      " |      any arbitrary word pairs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      wa: str\n",
      " |          Word for which we have to look top-n similar word.\n",
      " |      wb: str\n",
      " |          Word for which we evaluating relative cosine similarity with wa.\n",
      " |      topn: int, optional\n",
      " |          Number of top-n similar words to look with respect to wa.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.float64\n",
      " |          Relative cosine similarity between wa and wb.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save KeyedVectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the output file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.load`\n",
      " |          Load saved model.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words by vector.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector : numpy.array\n",
      " |          Vector from which similarities are to be computed.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Word\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return. If topn is None, similar_by_word returns\n",
      " |          the vector of similarity scores.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Compute cosine similarity between two words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Cosine similarity between `w1` and `w2`.\n",
      " |  \n",
      " |  similarity_matrix(self, dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100, dtype=<class 'numpy.float32'>)\n",
      " |      Construct a term similarity matrix for computing Soft Cosine Measure.\n",
      " |      \n",
      " |      This creates a sparse term similarity matrix in the :class:`scipy.sparse.csc_matrix` format for computing\n",
      " |      Soft Cosine Measure between documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dictionary : :class:`~gensim.corpora.dictionary.Dictionary`\n",
      " |          A dictionary that specifies the considered terms.\n",
      " |      tfidf : :class:`gensim.models.tfidfmodel.TfidfModel` or None, optional\n",
      " |          A model that specifies the relative importance of the terms in the dictionary. The\n",
      " |          columns of the term similarity matrix will be build in a decreasing order of importance\n",
      " |          of terms, or in the order of term identifiers if None.\n",
      " |      threshold : float, optional\n",
      " |          Only embeddings more similar than `threshold` are considered when retrieving word\n",
      " |          embeddings closest to a given word embedding.\n",
      " |      exponent : float, optional\n",
      " |          Take the word embedding similarities larger than `threshold` to the power of `exponent`.\n",
      " |      nonzero_limit : int, optional\n",
      " |          The maximum number of non-zero elements outside the diagonal in a single column of the\n",
      " |          sparse term similarity matrix.\n",
      " |      dtype : numpy.dtype, optional\n",
      " |          Data-type of the sparse term similarity matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`scipy.sparse.csc_matrix`\n",
      " |          Term similarity matrix.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :func:`gensim.matutils.softcossim`\n",
      " |          The Soft Cosine Measure.\n",
      " |      :class:`~gensim.similarities.docsim.SoftCosineSimilarity`\n",
      " |          A class for performing corpus-based similarity queries with Soft Cosine Measure.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The constructed matrix corresponds to the matrix Mrel defined in section 2.1 of\n",
      " |      `Delphine Charlet and Geraldine Damnati, \"SimBow at SemEval-2017 Task 3: Soft-Cosine Semantic Similarity\n",
      " |      between Questions for Community Question Answering\", 2017\n",
      " |      <http://www.aclweb.org/anthology/S/S17/S17-2051.pdf>`_.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Compute the Word Mover's Distance between two documents.\n",
      " |      \n",
      " |      When using this code, please consider citing the following papers:\n",
      " |      \n",
      " |      * `Ofir Pele and Michael Werman \"A linear time histogram metric for improved SIFT matching\"\n",
      " |        <http://www.cs.huji.ac.il/~werman/Papers/ECCV2008.pdf>`_\n",
      " |      * `Ofir Pele and Michael Werman \"Fast and robust earth mover's distances\"\n",
      " |        <https://ieeexplore.ieee.org/document/5459199/>`_\n",
      " |      * `Matt Kusner et al. \"From Word Embeddings To Document Distances\"\n",
      " |        <http://proceedings.mlr.press/v37/kusnerb15.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document1 : list of str\n",
      " |          Input document.\n",
      " |      document2 : list of str\n",
      " |          Input document.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Word Mover's distance between `document1` and `document2`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method only works if `pyemd <https://pypi.org/project/pyemd/>`_ is installed.\n",
      " |      \n",
      " |      If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)\n",
      " |      will be returned.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `pyemd <https://pypi.org/project/pyemd/>`_  isn't installed.\n",
      " |  \n",
      " |  word_vec(self, word, use_norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word\n",
      " |      use_norm : bool, optional\n",
      " |          If True - resulting vector will be L2-normalized (unit euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word not in vocabulary.\n",
      " |  \n",
      " |  words_closer_than(self, w1, w2)\n",
      " |      Get all words that are closer to `w1` than `w2` is to `w1`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list (str)\n",
      " |          List of words that are closer to `w1` than `w2` is to `w1`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  cosine_similarities(vector_1, vectors_all)\n",
      " |      Compute cosine similarities between one vector and a set of other vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_1 : numpy.ndarray\n",
      " |          Vector from which similarities are to be computed, expected shape (dim,).\n",
      " |      vectors_all : numpy.ndarray\n",
      " |          For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  index2entity\n",
      " |  \n",
      " |  syn0\n",
      " |  \n",
      " |  syn0norm\n",
      " |  \n",
      " |  wv\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseKeyedVectors:\n",
      " |  \n",
      " |  __getitem__(self, entities)\n",
      " |      Get vector representation of `entities`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : {str, list of str}\n",
      " |          Input entity/entities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation for `entities` (1D if `entities` is string, otherwise - 2D).\n",
      " |  \n",
      " |  __setitem__(self, entities, weights)\n",
      " |      Add entities and theirs vectors in a manual way.\n",
      " |      If some entity is already in the vocabulary, old vector is replaced with the new one.\n",
      " |      This method is alias for :meth:`~gensim.models.keyedvectors.BaseKeyedVectors.add` with `replace=True`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : {str, list of str}\n",
      " |          Entities specified by their string ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or 2D np.array of vectors.\n",
      " |  \n",
      " |  add(self, entities, weights, replace=False)\n",
      " |      Append entities and theirs vectors in a manual way.\n",
      " |      If some entity is already in the vocabulary, the old vector is kept unless `replace` flag is True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : list of str\n",
      " |          Entities specified by string ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or a 2D np.array of vectors.\n",
      " |      replace: bool, optional\n",
      " |          Flag indicating whether to replace vectors for entities which already exist in the vocabulary,\n",
      " |          if True - replace vectors, otherwise - keep old vectors.\n",
      " |  \n",
      " |  closer_than(self, entity1, entity2)\n",
      " |      Get all entities that are closer to `entity1` than `entity2` is to `entity1`.\n",
      " |  \n",
      " |  most_similar_to_given(self, entity1, entities_list)\n",
      " |      Get the `entity` from `entities_list` most similar to `entity1`.\n",
      " |  \n",
      " |  rank(self, entity1, entity2)\n",
      " |      Rank of the distance of `entity2` from `entity1`, in relation to distances of all entities from `entity1`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=1000,\n",
       "       n_clusters=2, n_init=50, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=True, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X = w2v.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('solution', 0.9999999403953552),\n",
       " ('happy', 0.998881459236145),\n",
       " ('verbal_abuse', 0.9988809823989868),\n",
       " ('ashamed', 0.998880922794342),\n",
       " ('pics', 0.9988807439804077),\n",
       " ('kiss_me', 0.9988803267478943),\n",
       " ('ripped', 0.998879075050354),\n",
       " ('the_workplace', 0.9988779425621033),\n",
       " ('frozen', 0.998877763748169),\n",
       " ('until', 0.9988775849342346)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.similar_by_vector(model.cluster_centers_[0], topn=10, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cluster_center = model.cluster_centers_[0]\n",
    "negative_cluster_center = model.cluster_centers_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>if_rakesh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>rakesh_said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>this_disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2264</th>\n",
       "      <td>has_caused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>hearing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2266 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "0                i\n",
       "1              was\n",
       "2               in\n",
       "3            class\n",
       "4               11\n",
       "...            ...\n",
       "2261     if_rakesh\n",
       "2262   rakesh_said\n",
       "2263  this_disease\n",
       "2264    has_caused\n",
       "2265       hearing\n",
       "\n",
       "[2266 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = pd.DataFrame(w2v.vocab.keys())\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.columns = ['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>if_rakesh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>rakesh_said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>this_disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2264</th>\n",
       "      <td>has_caused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>hearing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2266 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             words\n",
       "0                i\n",
       "1              was\n",
       "2               in\n",
       "3            class\n",
       "4               11\n",
       "...            ...\n",
       "2261     if_rakesh\n",
       "2262   rakesh_said\n",
       "2263  this_disease\n",
       "2264    has_caused\n",
       "2265       hearing\n",
       "\n",
       "[2266 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arfan/.conda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "words['vectors'] = words.words.apply(lambda x: w2v.wv[f'{x}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i</td>\n",
       "      <td>[-0.075747065, -0.06882905, 0.04576603, -0.057...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was</td>\n",
       "      <td>[-0.07550424, -0.06823647, 0.045346428, -0.057...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>[-0.07571099, -0.06848269, 0.04584541, -0.0574...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>class</td>\n",
       "      <td>[-0.075262405, -0.06871221, 0.045461968, -0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>[-0.07570061, -0.06822603, 0.04535558, -0.0581...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>if_rakesh</td>\n",
       "      <td>[-0.07578841, -0.06932318, 0.045399107, -0.057...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>rakesh_said</td>\n",
       "      <td>[-0.07554184, -0.06841819, 0.04632408, -0.0576...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>this_disease</td>\n",
       "      <td>[-0.07523805, -0.06840317, 0.046243936, -0.057...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2264</th>\n",
       "      <td>has_caused</td>\n",
       "      <td>[-0.074823126, -0.06899181, 0.046322018, -0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>hearing</td>\n",
       "      <td>[-0.07467423, -0.06839141, 0.04522534, -0.0578...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2266 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             words                                            vectors\n",
       "0                i  [-0.075747065, -0.06882905, 0.04576603, -0.057...\n",
       "1              was  [-0.07550424, -0.06823647, 0.045346428, -0.057...\n",
       "2               in  [-0.07571099, -0.06848269, 0.04584541, -0.0574...\n",
       "3            class  [-0.075262405, -0.06871221, 0.045461968, -0.05...\n",
       "4               11  [-0.07570061, -0.06822603, 0.04535558, -0.0581...\n",
       "...            ...                                                ...\n",
       "2261     if_rakesh  [-0.07578841, -0.06932318, 0.045399107, -0.057...\n",
       "2262   rakesh_said  [-0.07554184, -0.06841819, 0.04632408, -0.0576...\n",
       "2263  this_disease  [-0.07523805, -0.06840317, 0.046243936, -0.057...\n",
       "2264    has_caused  [-0.074823126, -0.06899181, 0.046322018, -0.05...\n",
       "2265       hearing  [-0.07467423, -0.06839141, 0.04522534, -0.0578...\n",
       "\n",
       "[2266 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['cluster'] = words.vectors.apply(lambda x: model.predict([np.array(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i</td>\n",
       "      <td>[-0.075747065, -0.06882905, 0.04576603, -0.057...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was</td>\n",
       "      <td>[-0.07550424, -0.06823647, 0.045346428, -0.057...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>[-0.07571099, -0.06848269, 0.04584541, -0.0574...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>class</td>\n",
       "      <td>[-0.075262405, -0.06871221, 0.045461968, -0.05...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>[-0.07570061, -0.06822603, 0.04535558, -0.0581...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>if_rakesh</td>\n",
       "      <td>[-0.07578841, -0.06932318, 0.045399107, -0.057...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>rakesh_said</td>\n",
       "      <td>[-0.07554184, -0.06841819, 0.04632408, -0.0576...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>this_disease</td>\n",
       "      <td>[-0.07523805, -0.06840317, 0.046243936, -0.057...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2264</th>\n",
       "      <td>has_caused</td>\n",
       "      <td>[-0.074823126, -0.06899181, 0.046322018, -0.05...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>hearing</td>\n",
       "      <td>[-0.07467423, -0.06839141, 0.04522534, -0.0578...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2266 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             words                                            vectors cluster\n",
       "0                i  [-0.075747065, -0.06882905, 0.04576603, -0.057...     [1]\n",
       "1              was  [-0.07550424, -0.06823647, 0.045346428, -0.057...     [1]\n",
       "2               in  [-0.07571099, -0.06848269, 0.04584541, -0.0574...     [1]\n",
       "3            class  [-0.075262405, -0.06871221, 0.045461968, -0.05...     [1]\n",
       "4               11  [-0.07570061, -0.06822603, 0.04535558, -0.0581...     [1]\n",
       "...            ...                                                ...     ...\n",
       "2261     if_rakesh  [-0.07578841, -0.06932318, 0.045399107, -0.057...     [1]\n",
       "2262   rakesh_said  [-0.07554184, -0.06841819, 0.04632408, -0.0576...     [1]\n",
       "2263  this_disease  [-0.07523805, -0.06840317, 0.046243936, -0.057...     [1]\n",
       "2264    has_caused  [-0.074823126, -0.06899181, 0.046322018, -0.05...     [1]\n",
       "2265       hearing  [-0.07467423, -0.06839141, 0.04522534, -0.0578...     [1]\n",
       "\n",
       "[2266 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.cluster = words.cluster.apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i</td>\n",
       "      <td>[-0.075747065, -0.06882905, 0.04576603, -0.057...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was</td>\n",
       "      <td>[-0.07550424, -0.06823647, 0.045346428, -0.057...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>[-0.07571099, -0.06848269, 0.04584541, -0.0574...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>class</td>\n",
       "      <td>[-0.075262405, -0.06871221, 0.045461968, -0.05...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>[-0.07570061, -0.06822603, 0.04535558, -0.0581...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>if_rakesh</td>\n",
       "      <td>[-0.07578841, -0.06932318, 0.045399107, -0.057...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>rakesh_said</td>\n",
       "      <td>[-0.07554184, -0.06841819, 0.04632408, -0.0576...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>this_disease</td>\n",
       "      <td>[-0.07523805, -0.06840317, 0.046243936, -0.057...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2264</th>\n",
       "      <td>has_caused</td>\n",
       "      <td>[-0.074823126, -0.06899181, 0.046322018, -0.05...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>hearing</td>\n",
       "      <td>[-0.07467423, -0.06839141, 0.04522534, -0.0578...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2266 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             words                                            vectors  cluster\n",
       "0                i  [-0.075747065, -0.06882905, 0.04576603, -0.057...        1\n",
       "1              was  [-0.07550424, -0.06823647, 0.045346428, -0.057...        1\n",
       "2               in  [-0.07571099, -0.06848269, 0.04584541, -0.0574...        1\n",
       "3            class  [-0.075262405, -0.06871221, 0.045461968, -0.05...        1\n",
       "4               11  [-0.07570061, -0.06822603, 0.04535558, -0.0581...        1\n",
       "...            ...                                                ...      ...\n",
       "2261     if_rakesh  [-0.07578841, -0.06932318, 0.045399107, -0.057...        1\n",
       "2262   rakesh_said  [-0.07554184, -0.06841819, 0.04632408, -0.0576...        1\n",
       "2263  this_disease  [-0.07523805, -0.06840317, 0.046243936, -0.057...        1\n",
       "2264    has_caused  [-0.074823126, -0.06899181, 0.046322018, -0.05...        1\n",
       "2265       hearing  [-0.07467423, -0.06839141, 0.04522534, -0.0578...        1\n",
       "\n",
       "[2266 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words['cluster_value'] = [1 if i==0 1 elif i == 1 else 2 for i in words.cluster]\n",
    "for w in words.cluster:\n",
    "    if w == 0:\n",
    "        words['cluster_value'] = 0\n",
    "    elif w == 1:\n",
    "        words['cluster_value'] = 1\n",
    "    else:\n",
    "        words['cluster_value'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i</td>\n",
       "      <td>[-0.075747065, -0.06882905, 0.04576603, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was</td>\n",
       "      <td>[-0.07550424, -0.06823647, 0.045346428, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>[-0.07571099, -0.06848269, 0.04584541, -0.0574...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>class</td>\n",
       "      <td>[-0.075262405, -0.06871221, 0.045461968, -0.05...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>[-0.07570061, -0.06822603, 0.04535558, -0.0581...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>if_rakesh</td>\n",
       "      <td>[-0.07578841, -0.06932318, 0.045399107, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>rakesh_said</td>\n",
       "      <td>[-0.07554184, -0.06841819, 0.04632408, -0.0576...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>this_disease</td>\n",
       "      <td>[-0.07523805, -0.06840317, 0.046243936, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2264</th>\n",
       "      <td>has_caused</td>\n",
       "      <td>[-0.074823126, -0.06899181, 0.046322018, -0.05...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>hearing</td>\n",
       "      <td>[-0.07467423, -0.06839141, 0.04522534, -0.0578...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2266 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             words                                            vectors  \\\n",
       "0                i  [-0.075747065, -0.06882905, 0.04576603, -0.057...   \n",
       "1              was  [-0.07550424, -0.06823647, 0.045346428, -0.057...   \n",
       "2               in  [-0.07571099, -0.06848269, 0.04584541, -0.0574...   \n",
       "3            class  [-0.075262405, -0.06871221, 0.045461968, -0.05...   \n",
       "4               11  [-0.07570061, -0.06822603, 0.04535558, -0.0581...   \n",
       "...            ...                                                ...   \n",
       "2261     if_rakesh  [-0.07578841, -0.06932318, 0.045399107, -0.057...   \n",
       "2262   rakesh_said  [-0.07554184, -0.06841819, 0.04632408, -0.0576...   \n",
       "2263  this_disease  [-0.07523805, -0.06840317, 0.046243936, -0.057...   \n",
       "2264    has_caused  [-0.074823126, -0.06899181, 0.046322018, -0.05...   \n",
       "2265       hearing  [-0.07467423, -0.06839141, 0.04522534, -0.0578...   \n",
       "\n",
       "      cluster  cluster_value  \n",
       "0           1              1  \n",
       "1           1              1  \n",
       "2           1              1  \n",
       "3           1              1  \n",
       "4           1              1  \n",
       "...       ...            ...  \n",
       "2261        1              1  \n",
       "2262        1              1  \n",
       "2263        1              1  \n",
       "2264        1              1  \n",
       "2265        1              1  \n",
       "\n",
       "[2266 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arfan/.conda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "words['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_value</th>\n",
       "      <th>closeness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i</td>\n",
       "      <td>[-0.075747065, -0.06882905, 0.04576603, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>180.496383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was</td>\n",
       "      <td>[-0.07550424, -0.06823647, 0.045346428, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>210.831901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>[-0.07571099, -0.06848269, 0.04584541, -0.0574...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>340.764600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>class</td>\n",
       "      <td>[-0.075262405, -0.06871221, 0.045461968, -0.05...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>245.770675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>[-0.07570061, -0.06822603, 0.04535558, -0.0581...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>128.665636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>if_rakesh</td>\n",
       "      <td>[-0.07578841, -0.06932318, 0.045399107, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101.455225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>rakesh_said</td>\n",
       "      <td>[-0.07554184, -0.06841819, 0.04632408, -0.0576...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>119.459317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>this_disease</td>\n",
       "      <td>[-0.07523805, -0.06840317, 0.046243936, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>137.255893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2264</th>\n",
       "      <td>has_caused</td>\n",
       "      <td>[-0.074823126, -0.06899181, 0.046322018, -0.05...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>117.103563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>hearing</td>\n",
       "      <td>[-0.07467423, -0.06839141, 0.04522534, -0.0578...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120.045604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2266 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             words                                            vectors  \\\n",
       "0                i  [-0.075747065, -0.06882905, 0.04576603, -0.057...   \n",
       "1              was  [-0.07550424, -0.06823647, 0.045346428, -0.057...   \n",
       "2               in  [-0.07571099, -0.06848269, 0.04584541, -0.0574...   \n",
       "3            class  [-0.075262405, -0.06871221, 0.045461968, -0.05...   \n",
       "4               11  [-0.07570061, -0.06822603, 0.04535558, -0.0581...   \n",
       "...            ...                                                ...   \n",
       "2261     if_rakesh  [-0.07578841, -0.06932318, 0.045399107, -0.057...   \n",
       "2262   rakesh_said  [-0.07554184, -0.06841819, 0.04632408, -0.0576...   \n",
       "2263  this_disease  [-0.07523805, -0.06840317, 0.046243936, -0.057...   \n",
       "2264    has_caused  [-0.074823126, -0.06899181, 0.046322018, -0.05...   \n",
       "2265       hearing  [-0.07467423, -0.06839141, 0.04522534, -0.0578...   \n",
       "\n",
       "      cluster  cluster_value  closeness_score  \n",
       "0           1              1       180.496383  \n",
       "1           1              1       210.831901  \n",
       "2           1              1       340.764600  \n",
       "3           1              1       245.770675  \n",
       "4           1              1       128.665636  \n",
       "...       ...            ...              ...  \n",
       "2261        1              1       101.455225  \n",
       "2262        1              1       119.459317  \n",
       "2263        1              1       137.255893  \n",
       "2264        1              1       117.103563  \n",
       "2265        1              1       120.045604  \n",
       "\n",
       "[2266 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['sentiment_coeff'] = words.closeness_score * words.cluster_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_value</th>\n",
       "      <th>closeness_score</th>\n",
       "      <th>sentiment_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>who</td>\n",
       "      <td>[-0.07556464, -0.06873704, 0.04588552, -0.0578...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>238.925753</td>\n",
       "      <td>238.925753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>once</td>\n",
       "      <td>[-0.0757213, -0.06869393, 0.045420565, -0.0579...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>218.337019</td>\n",
       "      <td>218.337019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>very_close</td>\n",
       "      <td>[-0.074609675, -0.06896993, 0.045199975, -0.05...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>103.731178</td>\n",
       "      <td>103.731178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>younger_brother</td>\n",
       "      <td>[-0.07555058, -0.06919016, 0.04644517, -0.0583...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>107.517364</td>\n",
       "      <td>107.517364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>of_mine</td>\n",
       "      <td>[-0.07719445, -0.06819945, 0.04667286, -0.0576...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53.626643</td>\n",
       "      <td>53.626643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>point</td>\n",
       "      <td>[-0.075444974, -0.06839889, 0.046304647, -0.05...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123.013986</td>\n",
       "      <td>123.013986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>he</td>\n",
       "      <td>[-0.075445764, -0.06874796, 0.045975536, -0.05...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>301.967154</td>\n",
       "      <td>301.967154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>to_pull</td>\n",
       "      <td>[-0.075072676, -0.068783544, 0.04531798, -0.05...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>146.548512</td>\n",
       "      <td>146.548512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>my_cheeks</td>\n",
       "      <td>[-0.074997894, -0.06811643, 0.045605626, -0.05...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>218.510706</td>\n",
       "      <td>218.510706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>at_first</td>\n",
       "      <td>[-0.07547845, -0.06847387, 0.046001263, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>215.475240</td>\n",
       "      <td>215.475240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>it</td>\n",
       "      <td>[-0.07498242, -0.06873257, 0.045585696, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>247.152952</td>\n",
       "      <td>247.152952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>only</td>\n",
       "      <td>[-0.07572719, -0.06819402, 0.04599388, -0.0580...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>227.114586</td>\n",
       "      <td>227.114586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>cheek</td>\n",
       "      <td>[-0.07485219, -0.069285296, 0.046353035, -0.05...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>114.413792</td>\n",
       "      <td>114.413792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>after_some</td>\n",
       "      <td>[-0.07596878, -0.06758069, 0.046058685, -0.058...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>95.943739</td>\n",
       "      <td>95.943739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>days</td>\n",
       "      <td>[-0.075735316, -0.068033405, 0.045753643, -0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>165.588745</td>\n",
       "      <td>165.588745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>pulling</td>\n",
       "      <td>[-0.075733826, -0.06843491, 0.04559923, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>217.371347</td>\n",
       "      <td>217.371347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>can_t</td>\n",
       "      <td>[-0.07528685, -0.06852053, 0.04526976, -0.0576...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>244.181967</td>\n",
       "      <td>244.181967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>how</td>\n",
       "      <td>[-0.07514472, -0.068411484, 0.045160286, -0.05...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>213.784133</td>\n",
       "      <td>213.784133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>annoying</td>\n",
       "      <td>[-0.076168545, -0.068006374, 0.045185804, -0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>136.120686</td>\n",
       "      <td>136.120686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>this_guy</td>\n",
       "      <td>[-0.07588773, -0.06843015, 0.045889072, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>147.114621</td>\n",
       "      <td>147.114621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>becoming</td>\n",
       "      <td>[-0.075679556, -0.06768281, 0.04614217, -0.056...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>116.810744</td>\n",
       "      <td>116.810744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>day</td>\n",
       "      <td>[-0.07551178, -0.0684234, 0.04556286, -0.05768...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>221.990193</td>\n",
       "      <td>221.990193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>as</td>\n",
       "      <td>[-0.07559392, -0.068816364, 0.045891427, -0.05...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>316.283748</td>\n",
       "      <td>316.283748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>are</td>\n",
       "      <td>[-0.07528629, -0.06864059, 0.045936957, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>214.362485</td>\n",
       "      <td>214.362485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>never</td>\n",
       "      <td>[-0.07515622, -0.06816913, 0.045547828, -0.058...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>263.891818</td>\n",
       "      <td>263.891818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>missed</td>\n",
       "      <td>[-0.07482586, -0.06816755, 0.04595259, -0.0581...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>146.060287</td>\n",
       "      <td>146.060287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>chance_to</td>\n",
       "      <td>[-0.076346055, -0.068050414, 0.045101043, -0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>91.152027</td>\n",
       "      <td>91.152027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>pull_them</td>\n",
       "      <td>[-0.07578059, -0.068899095, 0.046062343, -0.05...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>68.229612</td>\n",
       "      <td>68.229612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>even</td>\n",
       "      <td>[-0.07531281, -0.06878092, 0.045848608, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>263.750417</td>\n",
       "      <td>263.750417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>during</td>\n",
       "      <td>[-0.075270906, -0.06829051, 0.04525918, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>202.653603</td>\n",
       "      <td>202.653603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>moments</td>\n",
       "      <td>[-0.07510789, -0.06854443, 0.045356125, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123.823240</td>\n",
       "      <td>123.823240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>stop</td>\n",
       "      <td>[-0.07562193, -0.068521276, 0.045468047, -0.05...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>335.764058</td>\n",
       "      <td>335.764058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>love</td>\n",
       "      <td>[-0.074964926, -0.06873153, 0.04533715, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>171.701173</td>\n",
       "      <td>171.701173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>has</td>\n",
       "      <td>[-0.075834334, -0.06862439, 0.045339786, -0.05...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>232.665264</td>\n",
       "      <td>232.665264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>right</td>\n",
       "      <td>[-0.075154096, -0.068243, 0.04551233, -0.05729...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>286.119438</td>\n",
       "      <td>286.119438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>no_one</td>\n",
       "      <td>[-0.07522499, -0.068301424, 0.045388978, -0.05...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>255.914638</td>\n",
       "      <td>255.914638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>than</td>\n",
       "      <td>[-0.075249776, -0.068925135, 0.045816753, -0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>226.425592</td>\n",
       "      <td>226.425592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>can</td>\n",
       "      <td>[-0.07584191, -0.06892602, 0.045856472, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>211.756811</td>\n",
       "      <td>211.756811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>repeat</td>\n",
       "      <td>[-0.075267605, -0.06831687, 0.045277916, -0.05...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>153.899689</td>\n",
       "      <td>153.899689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>one_day</td>\n",
       "      <td>[-0.075461216, -0.06832586, 0.04576195, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>223.769482</td>\n",
       "      <td>223.769482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>god</td>\n",
       "      <td>[-0.075464316, -0.06890705, 0.045751546, -0.05...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>248.790657</td>\n",
       "      <td>248.790657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>him</td>\n",
       "      <td>[-0.07575419, -0.06860798, 0.04598673, -0.0580...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>222.708428</td>\n",
       "      <td>222.708428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>ever</td>\n",
       "      <td>[-0.07527636, -0.068829626, 0.04530787, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>296.517146</td>\n",
       "      <td>296.517146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>does</td>\n",
       "      <td>[-0.07520275, -0.06888588, 0.0452552, -0.05726...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>169.553651</td>\n",
       "      <td>169.553651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>the_same</td>\n",
       "      <td>[-0.0756261, -0.06868572, 0.04570137, -0.05770...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>214.722508</td>\n",
       "      <td>214.722508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>will</td>\n",
       "      <td>[-0.07565645, -0.068009086, 0.04542585, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>181.831444</td>\n",
       "      <td>181.831444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>kill</td>\n",
       "      <td>[-0.0753042, -0.06850659, 0.04460184, -0.05644...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>73.116806</td>\n",
       "      <td>73.116806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>his_eyes</td>\n",
       "      <td>[-0.07513953, -0.06749151, 0.04478044, -0.0562...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.424175</td>\n",
       "      <td>62.424175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>stopped</td>\n",
       "      <td>[-0.07573685, -0.06841429, 0.045444697, -0.057...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>181.978938</td>\n",
       "      <td>181.978938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>forever</td>\n",
       "      <td>[-0.07499343, -0.06841891, 0.04514829, -0.0581...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>98.082291</td>\n",
       "      <td>98.082291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               words                                            vectors  \\\n",
       "150              who  [-0.07556464, -0.06873704, 0.04588552, -0.0578...   \n",
       "151             once  [-0.0757213, -0.06869393, 0.045420565, -0.0579...   \n",
       "152       very_close  [-0.074609675, -0.06896993, 0.045199975, -0.05...   \n",
       "153  younger_brother  [-0.07555058, -0.06919016, 0.04644517, -0.0583...   \n",
       "154          of_mine  [-0.07719445, -0.06819945, 0.04667286, -0.0576...   \n",
       "155            point  [-0.075444974, -0.06839889, 0.046304647, -0.05...   \n",
       "156               he  [-0.075445764, -0.06874796, 0.045975536, -0.05...   \n",
       "157          to_pull  [-0.075072676, -0.068783544, 0.04531798, -0.05...   \n",
       "158        my_cheeks  [-0.074997894, -0.06811643, 0.045605626, -0.05...   \n",
       "159         at_first  [-0.07547845, -0.06847387, 0.046001263, -0.057...   \n",
       "160               it  [-0.07498242, -0.06873257, 0.045585696, -0.057...   \n",
       "161             only  [-0.07572719, -0.06819402, 0.04599388, -0.0580...   \n",
       "162            cheek  [-0.07485219, -0.069285296, 0.046353035, -0.05...   \n",
       "163       after_some  [-0.07596878, -0.06758069, 0.046058685, -0.058...   \n",
       "164             days  [-0.075735316, -0.068033405, 0.045753643, -0.0...   \n",
       "165          pulling  [-0.075733826, -0.06843491, 0.04559923, -0.057...   \n",
       "166            can_t  [-0.07528685, -0.06852053, 0.04526976, -0.0576...   \n",
       "167              how  [-0.07514472, -0.068411484, 0.045160286, -0.05...   \n",
       "168         annoying  [-0.076168545, -0.068006374, 0.045185804, -0.0...   \n",
       "169         this_guy  [-0.07588773, -0.06843015, 0.045889072, -0.057...   \n",
       "170         becoming  [-0.075679556, -0.06768281, 0.04614217, -0.056...   \n",
       "171              day  [-0.07551178, -0.0684234, 0.04556286, -0.05768...   \n",
       "172               as  [-0.07559392, -0.068816364, 0.045891427, -0.05...   \n",
       "173              are  [-0.07528629, -0.06864059, 0.045936957, -0.057...   \n",
       "174            never  [-0.07515622, -0.06816913, 0.045547828, -0.058...   \n",
       "175           missed  [-0.07482586, -0.06816755, 0.04595259, -0.0581...   \n",
       "176        chance_to  [-0.076346055, -0.068050414, 0.045101043, -0.0...   \n",
       "177        pull_them  [-0.07578059, -0.068899095, 0.046062343, -0.05...   \n",
       "178             even  [-0.07531281, -0.06878092, 0.045848608, -0.057...   \n",
       "179           during  [-0.075270906, -0.06829051, 0.04525918, -0.057...   \n",
       "180          moments  [-0.07510789, -0.06854443, 0.045356125, -0.057...   \n",
       "181             stop  [-0.07562193, -0.068521276, 0.045468047, -0.05...   \n",
       "182             love  [-0.074964926, -0.06873153, 0.04533715, -0.057...   \n",
       "183              has  [-0.075834334, -0.06862439, 0.045339786, -0.05...   \n",
       "184            right  [-0.075154096, -0.068243, 0.04551233, -0.05729...   \n",
       "185           no_one  [-0.07522499, -0.068301424, 0.045388978, -0.05...   \n",
       "186             than  [-0.075249776, -0.068925135, 0.045816753, -0.0...   \n",
       "187              can  [-0.07584191, -0.06892602, 0.045856472, -0.057...   \n",
       "188           repeat  [-0.075267605, -0.06831687, 0.045277916, -0.05...   \n",
       "189          one_day  [-0.075461216, -0.06832586, 0.04576195, -0.057...   \n",
       "190              god  [-0.075464316, -0.06890705, 0.045751546, -0.05...   \n",
       "191              him  [-0.07575419, -0.06860798, 0.04598673, -0.0580...   \n",
       "192             ever  [-0.07527636, -0.068829626, 0.04530787, -0.057...   \n",
       "193             does  [-0.07520275, -0.06888588, 0.0452552, -0.05726...   \n",
       "194         the_same  [-0.0756261, -0.06868572, 0.04570137, -0.05770...   \n",
       "195             will  [-0.07565645, -0.068009086, 0.04542585, -0.057...   \n",
       "196             kill  [-0.0753042, -0.06850659, 0.04460184, -0.05644...   \n",
       "197         his_eyes  [-0.07513953, -0.06749151, 0.04478044, -0.0562...   \n",
       "198          stopped  [-0.07573685, -0.06841429, 0.045444697, -0.057...   \n",
       "199          forever  [-0.07499343, -0.06841891, 0.04514829, -0.0581...   \n",
       "\n",
       "     cluster  cluster_value  closeness_score  sentiment_coeff  \n",
       "150        1              1       238.925753       238.925753  \n",
       "151        1              1       218.337019       218.337019  \n",
       "152        1              1       103.731178       103.731178  \n",
       "153        1              1       107.517364       107.517364  \n",
       "154        1              1        53.626643        53.626643  \n",
       "155        1              1       123.013986       123.013986  \n",
       "156        1              1       301.967154       301.967154  \n",
       "157        1              1       146.548512       146.548512  \n",
       "158        1              1       218.510706       218.510706  \n",
       "159        1              1       215.475240       215.475240  \n",
       "160        1              1       247.152952       247.152952  \n",
       "161        1              1       227.114586       227.114586  \n",
       "162        1              1       114.413792       114.413792  \n",
       "163        1              1        95.943739        95.943739  \n",
       "164        1              1       165.588745       165.588745  \n",
       "165        1              1       217.371347       217.371347  \n",
       "166        1              1       244.181967       244.181967  \n",
       "167        1              1       213.784133       213.784133  \n",
       "168        1              1       136.120686       136.120686  \n",
       "169        1              1       147.114621       147.114621  \n",
       "170        1              1       116.810744       116.810744  \n",
       "171        1              1       221.990193       221.990193  \n",
       "172        1              1       316.283748       316.283748  \n",
       "173        1              1       214.362485       214.362485  \n",
       "174        1              1       263.891818       263.891818  \n",
       "175        1              1       146.060287       146.060287  \n",
       "176        1              1        91.152027        91.152027  \n",
       "177        1              1        68.229612        68.229612  \n",
       "178        1              1       263.750417       263.750417  \n",
       "179        1              1       202.653603       202.653603  \n",
       "180        1              1       123.823240       123.823240  \n",
       "181        1              1       335.764058       335.764058  \n",
       "182        1              1       171.701173       171.701173  \n",
       "183        1              1       232.665264       232.665264  \n",
       "184        1              1       286.119438       286.119438  \n",
       "185        1              1       255.914638       255.914638  \n",
       "186        1              1       226.425592       226.425592  \n",
       "187        1              1       211.756811       211.756811  \n",
       "188        1              1       153.899689       153.899689  \n",
       "189        1              1       223.769482       223.769482  \n",
       "190        1              1       248.790657       248.790657  \n",
       "191        1              1       222.708428       222.708428  \n",
       "192        1              1       296.517146       296.517146  \n",
       "193        1              1       169.553651       169.553651  \n",
       "194        1              1       214.722508       214.722508  \n",
       "195        1              1       181.831444       181.831444  \n",
       "196        1              1        73.116806        73.116806  \n",
       "197        1              1        62.424175        62.424175  \n",
       "198        1              1       181.978938       181.978938  \n",
       "199        1              1        98.082291        98.082291  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[150:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[['words', 'sentiment_coeff']].to_csv('sentiment_dictionary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
