{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "mp.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load('word2vec.model').wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.Word2VecKeyedVectors"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Word2VecKeyedVectors in module gensim.models.keyedvectors object:\n",
      "\n",
      "class Word2VecKeyedVectors(WordEmbeddingsKeyedVectors)\n",
      " |  Mapping between words and vectors for the :class:`~gensim.models.Word2Vec` model.\n",
      " |  Used to perform operations on the vectors such as vector lookup, distance, similarity etc.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2VecKeyedVectors\n",
      " |      WordEmbeddingsKeyedVectors\n",
      " |      BaseKeyedVectors\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  get_keras_embedding(self, train_embeddings=False)\n",
      " |      Get a Keras 'Embedding' layer with weights set as the Word2Vec model's learned word embeddings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      train_embeddings : bool\n",
      " |          If False, the weights are frozen and stopped from being updated.\n",
      " |          If True, the weights can/will be further trained/updated.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `keras.layers.Embedding`\n",
      " |          Embedding layer.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `Keras <https://pypi.org/project/Keras/>`_ not installed.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      Current method work only if `Keras <https://pypi.org/project/Keras/>`_ installed.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None)\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path used to save the vectors in\n",
      " |      fvocab : str, optional\n",
      " |          Optional file path used to save the vocabulary\n",
      " |      binary : bool, optional\n",
      " |          If True, the data will be saved in binary word2vec format, else it will be saved in plain text.\n",
      " |      total_vec : int, optional\n",
      " |          Optional parameter to explicitly specify total no. of vectors\n",
      " |          (in case word vectors are appended with document vectors afterwards).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(fname_or_handle, **kwargs) from builtins.type\n",
      " |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to file that contains needed object.\n",
      " |      mmap : str, optional\n",
      " |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      " |          via mmap (shared memory) using `mmap='r'.\n",
      " |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.save`\n",
      " |          Save object to file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object\n",
      " |          Object loaded from `fname`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          When called on an object instance instead of class (this is a class method).\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
      " |      Load the input-hidden weight matrix from the original C word2vec-tool format.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      The information stored in the file is incomplete (the binary tree is missing),\n",
      " |      so while you can query for word similarity etc., you cannot continue training\n",
      " |      with a model loaded this way.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to the saved word2vec-format file.\n",
      " |      fvocab : str, optional\n",
      " |          File path to the vocabulary.Word counts are read from `fvocab` filename, if set\n",
      " |          (this is the file generated by `-save-vocab` flag of the original C tool).\n",
      " |      binary : bool, optional\n",
      " |          If True, indicates whether the data is in binary word2vec format.\n",
      " |      encoding : str, optional\n",
      " |          If you trained the C model using non-utf8 encoding for words, specify that encoding in `encoding`.\n",
      " |      unicode_errors : str, optional\n",
      " |          default 'strict', is a string suitable to be passed as the `errors`\n",
      " |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      " |          file may include word tokens truncated in the middle of a multibyte unicode character\n",
      " |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      " |      limit : int, optional\n",
      " |          Sets a maximum number of word-vectors to read from the file. The default,\n",
      " |          None, means read all.\n",
      " |      datatype : type, optional\n",
      " |          (Experimental) Can coerce dimensions to a non-default float type (such as `np.float16`) to save memory.\n",
      " |          Such types may result in much slower bulk operations or incompatibility with optimized routines.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |  \n",
      " |  __init__(self, vector_size)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=<function WordEmbeddingsKeyedVectors.most_similar at 0x7f5f8b1a8048>, case_insensitive=True)\n",
      " |      Compute accuracy of the model.\n",
      " |      \n",
      " |      The accuracy is reported (=printed to log and returned as a list) for each\n",
      " |      section separately, plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      questions : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      most_similar : function, optional\n",
      " |          Function used for similarity calculation.\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of dict of (str, (str, str, str)\n",
      " |          Full lists of correct and incorrect predictions divided by sections.\n",
      " |  \n",
      " |  distance(self, w1, w2)\n",
      " |      Compute cosine distance between two words.\n",
      " |      Calculate 1 - :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Distance between `w1` and `w2`.\n",
      " |  \n",
      " |  distances(self, word_or_vector, other_words=())\n",
      " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
      " |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_or_vector : {str, numpy.ndarray}\n",
      " |          Word or vector from which distances are to be computed.\n",
      " |      other_words : iterable of str\n",
      " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
      " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Array containing distances to all words in `other_words` from input `word_or_vector`.\n",
      " |      \n",
      " |      Raises\n",
      " |      -----\n",
      " |      KeyError\n",
      " |          If either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Which word from the given list doesn't go with the others?\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of words.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          The word further away from the mean of all words.\n",
      " |  \n",
      " |  evaluate_word_analogies(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute performance of the model on an analogy test set.\n",
      " |      \n",
      " |      This is modern variant of :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.accuracy`, see\n",
      " |      `discussion on GitHub #1935 <https://github.com/RaRe-Technologies/gensim/pull/1935>`_.\n",
      " |      \n",
      " |      The accuracy is reported (printed to log and returned as a score) for each section separately,\n",
      " |      plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      " |      See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      analogies : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          The overall evaluation score on the entire evaluation set\n",
      " |      sections : list of dict of {str : str or list of tuple of (str, str, str, str)}\n",
      " |          Results broken down by each section of the evaluation set. Each dict contains the name of the section\n",
      " |          under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the\n",
      " |          keys 'correct' and 'incorrect'.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute correlation of the model with human similarity judgments.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      More datasets can be found at\n",
      " |      * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html\n",
      " |      * https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pairs : str\n",
      " |          Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.\n",
      " |          See `test/test_data/wordsim353.tsv` as example.\n",
      " |      delimiter : str, optional\n",
      " |          Separator in `pairs` file.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pearson : tuple of (float, float)\n",
      " |          Pearson correlation coefficient with 2-tailed p-value.\n",
      " |      spearman : tuple of (float, float)\n",
      " |          Spearman rank-order correlation coefficient between the similarities from the dataset and the\n",
      " |          similarities produced by the model itself, with 2-tailed p-value.\n",
      " |      oov_ratio : float\n",
      " |          The ratio of pairs with unknown words.\n",
      " |  \n",
      " |  get_vector(self, word)\n",
      " |      Get the entity's representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entity : str\n",
      " |          Identifier of the entity to return the vector for.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector for the specified entity.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If the given entity identifier doesn't exist.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool, optional\n",
      " |          If True - forget the original vectors and only keep the normalized ones = saves lots of memory!\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      You **cannot continue training** after doing a replace.\n",
      " |      The model becomes effectively read-only: you can call\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`,\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`, etc., but not train.\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Find the top-N most similar words.\n",
      " |      Positive words contribute positively towards the similarity, negative words negatively.\n",
      " |      \n",
      " |      This method computes cosine similarity between a simple mean of the projection\n",
      " |      weight vectors of the given words and the vectors for each word in the model.\n",
      " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      " |      word2vec implementation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Find the top-N most similar words, using the multiplicative combination objective,\n",
      " |      proposed by `Omer Levy and Yoav Goldberg \"Linguistic Regularities in Sparse and Explicit Word Representations\"\n",
      " |      <http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,\n",
      " |      negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n",
      " |      In the common analogy-solving case, of two positive and one negative examples,\n",
      " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
      " |      \n",
      " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
      " |      respectively - a potentially sensible but untested extension of the method.\n",
      " |      With a single positive example, rankings will be the same as in the default\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Compute cosine similarity between two sets of words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ws1 : list of str\n",
      " |          Sequence of words.\n",
      " |      ws2: list of str\n",
      " |          Sequence of words.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Similarities between `ws1` and `ws2`.\n",
      " |  \n",
      " |  relative_cosine_similarity(self, wa, wb, topn=10)\n",
      " |      Compute the relative cosine similarity between two words given top-n similar words,\n",
      " |      by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc \"A Minimally Supervised Approach\n",
      " |      for Synonym Extraction with Word Embeddings\" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.\n",
      " |      \n",
      " |      To calculate relative cosine similarity between two words, equation (1) of the paper is used.\n",
      " |      For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than\n",
      " |      any arbitrary word pairs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      wa: str\n",
      " |          Word for which we have to look top-n similar word.\n",
      " |      wb: str\n",
      " |          Word for which we evaluating relative cosine similarity with wa.\n",
      " |      topn: int, optional\n",
      " |          Number of top-n similar words to look with respect to wa.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.float64\n",
      " |          Relative cosine similarity between wa and wb.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save KeyedVectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the output file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.load`\n",
      " |          Load saved model.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words by vector.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector : numpy.array\n",
      " |          Vector from which similarities are to be computed.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Word\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return. If topn is None, similar_by_word returns\n",
      " |          the vector of similarity scores.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Compute cosine similarity between two words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Cosine similarity between `w1` and `w2`.\n",
      " |  \n",
      " |  similarity_matrix(self, dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100, dtype=<class 'numpy.float32'>)\n",
      " |      Construct a term similarity matrix for computing Soft Cosine Measure.\n",
      " |      \n",
      " |      This creates a sparse term similarity matrix in the :class:`scipy.sparse.csc_matrix` format for computing\n",
      " |      Soft Cosine Measure between documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dictionary : :class:`~gensim.corpora.dictionary.Dictionary`\n",
      " |          A dictionary that specifies the considered terms.\n",
      " |      tfidf : :class:`gensim.models.tfidfmodel.TfidfModel` or None, optional\n",
      " |          A model that specifies the relative importance of the terms in the dictionary. The\n",
      " |          columns of the term similarity matrix will be build in a decreasing order of importance\n",
      " |          of terms, or in the order of term identifiers if None.\n",
      " |      threshold : float, optional\n",
      " |          Only embeddings more similar than `threshold` are considered when retrieving word\n",
      " |          embeddings closest to a given word embedding.\n",
      " |      exponent : float, optional\n",
      " |          Take the word embedding similarities larger than `threshold` to the power of `exponent`.\n",
      " |      nonzero_limit : int, optional\n",
      " |          The maximum number of non-zero elements outside the diagonal in a single column of the\n",
      " |          sparse term similarity matrix.\n",
      " |      dtype : numpy.dtype, optional\n",
      " |          Data-type of the sparse term similarity matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`scipy.sparse.csc_matrix`\n",
      " |          Term similarity matrix.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :func:`gensim.matutils.softcossim`\n",
      " |          The Soft Cosine Measure.\n",
      " |      :class:`~gensim.similarities.docsim.SoftCosineSimilarity`\n",
      " |          A class for performing corpus-based similarity queries with Soft Cosine Measure.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The constructed matrix corresponds to the matrix Mrel defined in section 2.1 of\n",
      " |      `Delphine Charlet and Geraldine Damnati, \"SimBow at SemEval-2017 Task 3: Soft-Cosine Semantic Similarity\n",
      " |      between Questions for Community Question Answering\", 2017\n",
      " |      <http://www.aclweb.org/anthology/S/S17/S17-2051.pdf>`_.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Compute the Word Mover's Distance between two documents.\n",
      " |      \n",
      " |      When using this code, please consider citing the following papers:\n",
      " |      \n",
      " |      * `Ofir Pele and Michael Werman \"A linear time histogram metric for improved SIFT matching\"\n",
      " |        <http://www.cs.huji.ac.il/~werman/Papers/ECCV2008.pdf>`_\n",
      " |      * `Ofir Pele and Michael Werman \"Fast and robust earth mover's distances\"\n",
      " |        <https://ieeexplore.ieee.org/document/5459199/>`_\n",
      " |      * `Matt Kusner et al. \"From Word Embeddings To Document Distances\"\n",
      " |        <http://proceedings.mlr.press/v37/kusnerb15.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document1 : list of str\n",
      " |          Input document.\n",
      " |      document2 : list of str\n",
      " |          Input document.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Word Mover's distance between `document1` and `document2`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method only works if `pyemd <https://pypi.org/project/pyemd/>`_ is installed.\n",
      " |      \n",
      " |      If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)\n",
      " |      will be returned.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `pyemd <https://pypi.org/project/pyemd/>`_  isn't installed.\n",
      " |  \n",
      " |  word_vec(self, word, use_norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word\n",
      " |      use_norm : bool, optional\n",
      " |          If True - resulting vector will be L2-normalized (unit euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word not in vocabulary.\n",
      " |  \n",
      " |  words_closer_than(self, w1, w2)\n",
      " |      Get all words that are closer to `w1` than `w2` is to `w1`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list (str)\n",
      " |          List of words that are closer to `w1` than `w2` is to `w1`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  cosine_similarities(vector_1, vectors_all)\n",
      " |      Compute cosine similarities between one vector and a set of other vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_1 : numpy.ndarray\n",
      " |          Vector from which similarities are to be computed, expected shape (dim,).\n",
      " |      vectors_all : numpy.ndarray\n",
      " |          For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  index2entity\n",
      " |  \n",
      " |  syn0\n",
      " |  \n",
      " |  syn0norm\n",
      " |  \n",
      " |  wv\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseKeyedVectors:\n",
      " |  \n",
      " |  __getitem__(self, entities)\n",
      " |      Get vector representation of `entities`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : {str, list of str}\n",
      " |          Input entity/entities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation for `entities` (1D if `entities` is string, otherwise - 2D).\n",
      " |  \n",
      " |  __setitem__(self, entities, weights)\n",
      " |      Add entities and theirs vectors in a manual way.\n",
      " |      If some entity is already in the vocabulary, old vector is replaced with the new one.\n",
      " |      This method is alias for :meth:`~gensim.models.keyedvectors.BaseKeyedVectors.add` with `replace=True`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : {str, list of str}\n",
      " |          Entities specified by their string ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or 2D np.array of vectors.\n",
      " |  \n",
      " |  add(self, entities, weights, replace=False)\n",
      " |      Append entities and theirs vectors in a manual way.\n",
      " |      If some entity is already in the vocabulary, the old vector is kept unless `replace` flag is True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : list of str\n",
      " |          Entities specified by string ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or a 2D np.array of vectors.\n",
      " |      replace: bool, optional\n",
      " |          Flag indicating whether to replace vectors for entities which already exist in the vocabulary,\n",
      " |          if True - replace vectors, otherwise - keep old vectors.\n",
      " |  \n",
      " |  closer_than(self, entity1, entity2)\n",
      " |      Get all entities that are closer to `entity1` than `entity2` is to `entity1`.\n",
      " |  \n",
      " |  most_similar_to_given(self, entity1, entities_list)\n",
      " |      Get the `entity` from `entities_list` most similar to `entity1`.\n",
      " |  \n",
      " |  rank(self, entity1, entity2)\n",
      " |      Rank of the distance of `entity2` from `entity1`, in relation to distances of all entities from `entity1`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history):\n",
    "    \"\"\"\n",
    "    Plot Accuracy and Loss curves given the model_history\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    fig.savefig('kmeans.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=1000,\n",
       "       n_clusters=2, n_init=50, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=True, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X = w2v.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 0.996428370475769),\n",
       " ('me', 0.99398273229599),\n",
       " ('in', 0.9932867288589478),\n",
       " ('the', 0.9924371242523193),\n",
       " ('and', 0.9920103549957275),\n",
       " ('my', 0.9919503927230835),\n",
       " ('i', 0.9903295636177063),\n",
       " ('asked', 0.989032506942749),\n",
       " ('i_was', 0.9860483407974243),\n",
       " ('for', 0.984266996383667)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.similar_by_vector(model.cluster_centers_[0], topn=10, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cluster_center = model.cluster_centers_[0]\n",
    "negative_cluster_center = model.cluster_centers_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>any</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>things</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>rakesh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>disease</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0       just\n",
       "1       like\n",
       "2        any\n",
       "3      other\n",
       "4        day\n",
       "..       ...\n",
       "201      say\n",
       "202   things\n",
       "203    bully\n",
       "204   rakesh\n",
       "205  disease\n",
       "\n",
       "[206 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = pd.DataFrame(w2v.vocab.keys())\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.columns = ['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>any</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>things</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>rakesh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>disease</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       words\n",
       "0       just\n",
       "1       like\n",
       "2        any\n",
       "3      other\n",
       "4        day\n",
       "..       ...\n",
       "201      say\n",
       "202   things\n",
       "203    bully\n",
       "204   rakesh\n",
       "205  disease\n",
       "\n",
       "[206 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arfan/.conda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "words['vectors'] = words.words.apply(lambda x: w2v.wv[f'{x}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just</td>\n",
       "      <td>[0.06401228, 0.10244543, -0.12459137, 0.049309...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like</td>\n",
       "      <td>[0.041160908, 0.076062895, -0.16023546, 0.0556...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>any</td>\n",
       "      <td>[-0.004705759, 0.012123186, -0.056069445, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other</td>\n",
       "      <td>[0.036368866, 0.110916555, -0.108262785, 0.092...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day</td>\n",
       "      <td>[0.07634358, 0.10929913, -0.15942724, 0.026457...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>say</td>\n",
       "      <td>[0.06571426, 0.08929079, -0.1438337, 0.0303185...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>things</td>\n",
       "      <td>[0.043621585, 0.08232436, -0.16201611, 0.04404...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>bully</td>\n",
       "      <td>[0.035272025, 0.112041526, -0.13689706, 0.0334...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>rakesh</td>\n",
       "      <td>[0.06330622, 0.098207384, -0.15660104, 0.03151...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>disease</td>\n",
       "      <td>[0.03049219, 0.097921014, -0.151512, 0.0495529...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       words                                            vectors\n",
       "0       just  [0.06401228, 0.10244543, -0.12459137, 0.049309...\n",
       "1       like  [0.041160908, 0.076062895, -0.16023546, 0.0556...\n",
       "2        any  [-0.004705759, 0.012123186, -0.056069445, -0.0...\n",
       "3      other  [0.036368866, 0.110916555, -0.108262785, 0.092...\n",
       "4        day  [0.07634358, 0.10929913, -0.15942724, 0.026457...\n",
       "..       ...                                                ...\n",
       "201      say  [0.06571426, 0.08929079, -0.1438337, 0.0303185...\n",
       "202   things  [0.043621585, 0.08232436, -0.16201611, 0.04404...\n",
       "203    bully  [0.035272025, 0.112041526, -0.13689706, 0.0334...\n",
       "204   rakesh  [0.06330622, 0.098207384, -0.15660104, 0.03151...\n",
       "205  disease  [0.03049219, 0.097921014, -0.151512, 0.0495529...\n",
       "\n",
       "[206 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['cluster'] = words.vectors.apply(lambda x: model.predict([np.array(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just</td>\n",
       "      <td>[0.06401228, 0.10244543, -0.12459137, 0.049309...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like</td>\n",
       "      <td>[0.041160908, 0.076062895, -0.16023546, 0.0556...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>any</td>\n",
       "      <td>[-0.004705759, 0.012123186, -0.056069445, -0.0...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other</td>\n",
       "      <td>[0.036368866, 0.110916555, -0.108262785, 0.092...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day</td>\n",
       "      <td>[0.07634358, 0.10929913, -0.15942724, 0.026457...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>say</td>\n",
       "      <td>[0.06571426, 0.08929079, -0.1438337, 0.0303185...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>things</td>\n",
       "      <td>[0.043621585, 0.08232436, -0.16201611, 0.04404...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>bully</td>\n",
       "      <td>[0.035272025, 0.112041526, -0.13689706, 0.0334...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>rakesh</td>\n",
       "      <td>[0.06330622, 0.098207384, -0.15660104, 0.03151...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>disease</td>\n",
       "      <td>[0.03049219, 0.097921014, -0.151512, 0.0495529...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       words                                            vectors cluster\n",
       "0       just  [0.06401228, 0.10244543, -0.12459137, 0.049309...     [0]\n",
       "1       like  [0.041160908, 0.076062895, -0.16023546, 0.0556...     [0]\n",
       "2        any  [-0.004705759, 0.012123186, -0.056069445, -0.0...     [0]\n",
       "3      other  [0.036368866, 0.110916555, -0.108262785, 0.092...     [0]\n",
       "4        day  [0.07634358, 0.10929913, -0.15942724, 0.026457...     [0]\n",
       "..       ...                                                ...     ...\n",
       "201      say  [0.06571426, 0.08929079, -0.1438337, 0.0303185...     [0]\n",
       "202   things  [0.043621585, 0.08232436, -0.16201611, 0.04404...     [0]\n",
       "203    bully  [0.035272025, 0.112041526, -0.13689706, 0.0334...     [0]\n",
       "204   rakesh  [0.06330622, 0.098207384, -0.15660104, 0.03151...     [0]\n",
       "205  disease  [0.03049219, 0.097921014, -0.151512, 0.0495529...     [0]\n",
       "\n",
       "[206 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.cluster = words.cluster.apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just</td>\n",
       "      <td>[0.06401228, 0.10244543, -0.12459137, 0.049309...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like</td>\n",
       "      <td>[0.041160908, 0.076062895, -0.16023546, 0.0556...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>any</td>\n",
       "      <td>[-0.004705759, 0.012123186, -0.056069445, -0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other</td>\n",
       "      <td>[0.036368866, 0.110916555, -0.108262785, 0.092...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day</td>\n",
       "      <td>[0.07634358, 0.10929913, -0.15942724, 0.026457...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>say</td>\n",
       "      <td>[0.06571426, 0.08929079, -0.1438337, 0.0303185...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>things</td>\n",
       "      <td>[0.043621585, 0.08232436, -0.16201611, 0.04404...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>bully</td>\n",
       "      <td>[0.035272025, 0.112041526, -0.13689706, 0.0334...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>rakesh</td>\n",
       "      <td>[0.06330622, 0.098207384, -0.15660104, 0.03151...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>disease</td>\n",
       "      <td>[0.03049219, 0.097921014, -0.151512, 0.0495529...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       words                                            vectors  cluster\n",
       "0       just  [0.06401228, 0.10244543, -0.12459137, 0.049309...        0\n",
       "1       like  [0.041160908, 0.076062895, -0.16023546, 0.0556...        0\n",
       "2        any  [-0.004705759, 0.012123186, -0.056069445, -0.0...        0\n",
       "3      other  [0.036368866, 0.110916555, -0.108262785, 0.092...        0\n",
       "4        day  [0.07634358, 0.10929913, -0.15942724, 0.026457...        0\n",
       "..       ...                                                ...      ...\n",
       "201      say  [0.06571426, 0.08929079, -0.1438337, 0.0303185...        0\n",
       "202   things  [0.043621585, 0.08232436, -0.16201611, 0.04404...        0\n",
       "203    bully  [0.035272025, 0.112041526, -0.13689706, 0.0334...        0\n",
       "204   rakesh  [0.06330622, 0.098207384, -0.15660104, 0.03151...        0\n",
       "205  disease  [0.03049219, 0.097921014, -0.151512, 0.0495529...        0\n",
       "\n",
       "[206 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words['cluster_value'] = [1 if i==0 1 elif i == 1 else 2 for i in words.cluster]\n",
    "for w in words.cluster:\n",
    "    if w == 0:\n",
    "        words['cluster_value'] = 0\n",
    "    elif w == 1:\n",
    "        words['cluster_value'] = 1\n",
    "    else:\n",
    "        words['cluster_value'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just</td>\n",
       "      <td>[0.06401228, 0.10244543, -0.12459137, 0.049309...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like</td>\n",
       "      <td>[0.041160908, 0.076062895, -0.16023546, 0.0556...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>any</td>\n",
       "      <td>[-0.004705759, 0.012123186, -0.056069445, -0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other</td>\n",
       "      <td>[0.036368866, 0.110916555, -0.108262785, 0.092...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day</td>\n",
       "      <td>[0.07634358, 0.10929913, -0.15942724, 0.026457...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>say</td>\n",
       "      <td>[0.06571426, 0.08929079, -0.1438337, 0.0303185...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>things</td>\n",
       "      <td>[0.043621585, 0.08232436, -0.16201611, 0.04404...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>bully</td>\n",
       "      <td>[0.035272025, 0.112041526, -0.13689706, 0.0334...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>rakesh</td>\n",
       "      <td>[0.06330622, 0.098207384, -0.15660104, 0.03151...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>disease</td>\n",
       "      <td>[0.03049219, 0.097921014, -0.151512, 0.0495529...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       words                                            vectors  cluster  \\\n",
       "0       just  [0.06401228, 0.10244543, -0.12459137, 0.049309...        0   \n",
       "1       like  [0.041160908, 0.076062895, -0.16023546, 0.0556...        0   \n",
       "2        any  [-0.004705759, 0.012123186, -0.056069445, -0.0...        0   \n",
       "3      other  [0.036368866, 0.110916555, -0.108262785, 0.092...        0   \n",
       "4        day  [0.07634358, 0.10929913, -0.15942724, 0.026457...        0   \n",
       "..       ...                                                ...      ...   \n",
       "201      say  [0.06571426, 0.08929079, -0.1438337, 0.0303185...        0   \n",
       "202   things  [0.043621585, 0.08232436, -0.16201611, 0.04404...        0   \n",
       "203    bully  [0.035272025, 0.112041526, -0.13689706, 0.0334...        0   \n",
       "204   rakesh  [0.06330622, 0.098207384, -0.15660104, 0.03151...        0   \n",
       "205  disease  [0.03049219, 0.097921014, -0.151512, 0.0495529...        0   \n",
       "\n",
       "     cluster_value  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  \n",
       "..             ...  \n",
       "201              0  \n",
       "202              0  \n",
       "203              0  \n",
       "204              0  \n",
       "205              0  \n",
       "\n",
       "[206 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_value</th>\n",
       "      <th>closeness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just</td>\n",
       "      <td>[0.06401228, 0.10244543, -0.12459137, 0.049309...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.027561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like</td>\n",
       "      <td>[0.041160908, 0.076062895, -0.16023546, 0.0556...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.220271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>any</td>\n",
       "      <td>[-0.004705759, 0.012123186, -0.056069445, -0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.401266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other</td>\n",
       "      <td>[0.036368866, 0.110916555, -0.108262785, 0.092...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.359943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day</td>\n",
       "      <td>[0.07634358, 0.10929913, -0.15942724, 0.026457...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.490357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>say</td>\n",
       "      <td>[0.06571426, 0.08929079, -0.1438337, 0.0303185...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.410727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>things</td>\n",
       "      <td>[0.043621585, 0.08232436, -0.16201611, 0.04404...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.759315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>bully</td>\n",
       "      <td>[0.035272025, 0.112041526, -0.13689706, 0.0334...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.678192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>rakesh</td>\n",
       "      <td>[0.06330622, 0.098207384, -0.15660104, 0.03151...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.602103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>disease</td>\n",
       "      <td>[0.03049219, 0.097921014, -0.151512, 0.0495529...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.097911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       words                                            vectors  cluster  \\\n",
       "0       just  [0.06401228, 0.10244543, -0.12459137, 0.049309...        0   \n",
       "1       like  [0.041160908, 0.076062895, -0.16023546, 0.0556...        0   \n",
       "2        any  [-0.004705759, 0.012123186, -0.056069445, -0.0...        0   \n",
       "3      other  [0.036368866, 0.110916555, -0.108262785, 0.092...        0   \n",
       "4        day  [0.07634358, 0.10929913, -0.15942724, 0.026457...        0   \n",
       "..       ...                                                ...      ...   \n",
       "201      say  [0.06571426, 0.08929079, -0.1438337, 0.0303185...        0   \n",
       "202   things  [0.043621585, 0.08232436, -0.16201611, 0.04404...        0   \n",
       "203    bully  [0.035272025, 0.112041526, -0.13689706, 0.0334...        0   \n",
       "204   rakesh  [0.06330622, 0.098207384, -0.15660104, 0.03151...        0   \n",
       "205  disease  [0.03049219, 0.097921014, -0.151512, 0.0495529...        0   \n",
       "\n",
       "     cluster_value  closeness_score  \n",
       "0                0         3.027561  \n",
       "1                0         2.220271  \n",
       "2                0         1.401266  \n",
       "3                0         1.359943  \n",
       "4                0         2.490357  \n",
       "..             ...              ...  \n",
       "201              0         4.410727  \n",
       "202              0         2.759315  \n",
       "203              0         4.678192  \n",
       "204              0         4.602103  \n",
       "205              0         3.097911  \n",
       "\n",
       "[206 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['sentiment_coeff'] = words.closeness_score * words.cluster_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_value</th>\n",
       "      <th>closeness_score</th>\n",
       "      <th>sentiment_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>wasn_t</td>\n",
       "      <td>[0.077366784, 0.10052142, -0.07932672, 0.06703...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.950220</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>home</td>\n",
       "      <td>[-0.0022268514, 0.1320761, -0.10145202, 0.0726...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.055336</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>always</td>\n",
       "      <td>[0.010727334, 0.10077481, -0.13231076, 0.06112...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.791127</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>who</td>\n",
       "      <td>[0.041929875, 0.06628057, -0.0635532, 0.032363...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.556763</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>said</td>\n",
       "      <td>[0.073347755, 0.08684462, -0.11043333, 0.01335...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.807974</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>or</td>\n",
       "      <td>[0.09029762, 0.11544328, -0.0903675, 0.0237198...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.973365</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>some</td>\n",
       "      <td>[0.06544155, 0.08807347, -0.111523494, 0.04467...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.464964</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>noticed</td>\n",
       "      <td>[0.0069835912, 0.10363454, -0.14654888, 0.0522...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.110968</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>where</td>\n",
       "      <td>[0.013180582, 0.040102858, -0.01023495, -0.055...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.159982</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>did</td>\n",
       "      <td>[0.058951147, 0.12304556, -0.15273815, 0.01902...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.892291</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>weird</td>\n",
       "      <td>[0.060342334, 0.05503284, -0.09524252, 0.03339...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.452633</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>close</td>\n",
       "      <td>[0.028209023, 0.10367024, -0.13795954, 0.05265...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.777033</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>asked</td>\n",
       "      <td>[0.040985093, 0.0870999, -0.15866901, 0.039822...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.789738</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>because</td>\n",
       "      <td>[0.026819792, 0.10956048, -0.16380951, 0.03697...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.533310</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>t_have</td>\n",
       "      <td>[0.04718983, 0.05452005, -0.074673064, -0.0222...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.412324</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>told</td>\n",
       "      <td>[0.046398334, 0.116562754, -0.16370094, 0.0517...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.110518</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>out</td>\n",
       "      <td>[0.06044977, 0.09710592, -0.13321766, 0.042567...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.596244</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>beat</td>\n",
       "      <td>[0.06487117, 0.08845928, -0.11587944, 0.048797...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.011828</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>your</td>\n",
       "      <td>[0.084559724, 0.07086326, -0.09732823, 0.03486...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.284449</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>go</td>\n",
       "      <td>[0.07789978, 0.059355702, -0.12908569, 0.01887...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.730077</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>thing</td>\n",
       "      <td>[0.036836628, 0.07842893, -0.11226163, 0.06549...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.288906</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>s</td>\n",
       "      <td>[0.038622584, 0.10879778, -0.13081372, 0.06046...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.649110</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>never</td>\n",
       "      <td>[0.069072604, 0.068447806, -0.15002675, 0.0345...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.112595</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>everyone</td>\n",
       "      <td>[0.010085005, 0.020066313, -0.062995285, -0.03...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.449513</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>fat</td>\n",
       "      <td>[0.055769652, 0.0678707, -0.15594542, 0.030770...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.739899</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>even</td>\n",
       "      <td>[0.026819909, 0.09234932, -0.12029709, 0.05883...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.941256</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>high_school</td>\n",
       "      <td>[0.0495706, 0.09740633, -0.14637525, 0.0124422...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.058419</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>girls</td>\n",
       "      <td>[0.008823015, 0.01955772, -0.1417351, -0.00159...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.386159</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>name</td>\n",
       "      <td>[0.006965787, 0.08794537, -0.16881044, 0.06244...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.021178</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>pretty</td>\n",
       "      <td>[0.058723785, 0.062196314, -0.1570344, 0.02858...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.871746</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>bullying</td>\n",
       "      <td>[0.0039847954, 0.059772078, -0.15487488, 0.038...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.402651</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>bullied</td>\n",
       "      <td>[0.08529985, 0.055052157, -0.114659645, 0.0366...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.440752</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>about_it</td>\n",
       "      <td>[0.052249905, 0.13277811, -0.16550201, -0.0163...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.727195</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>my_best</td>\n",
       "      <td>[0.06210439, 0.11778579, -0.06944129, 0.075448...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.255333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>mean_girls</td>\n",
       "      <td>[-0.0412226, 0.048035163, -0.08706963, -0.0355...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.197887</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>my_friends</td>\n",
       "      <td>[0.01280857, 0.11863217, -0.13743517, 0.026601...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.594825</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>still</td>\n",
       "      <td>[0.0016612421, 0.06699986, -0.13410479, 0.0021...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.968430</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>year_in</td>\n",
       "      <td>[0.02415606, 0.13417998, -0.17077604, -0.00097...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.998927</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>knew</td>\n",
       "      <td>[0.045925792, 0.125555, -0.11345351, 0.0595440...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.553781</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>class</td>\n",
       "      <td>[0.011481658, 0.11176742, -0.14259523, 0.06232...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.703285</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>while</td>\n",
       "      <td>[0.06507579, 0.07872616, -0.119987294, 0.02853...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.904274</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>boys</td>\n",
       "      <td>[0.04898546, -0.049998425, 0.036221102, -0.053...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.962533</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>start</td>\n",
       "      <td>[0.04280632, 0.09447646, -0.12015669, 0.052337...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.802161</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>then</td>\n",
       "      <td>[-0.017736409, -0.04889355, -0.0628197, -0.011...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.132048</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>the_girl</td>\n",
       "      <td>[-0.005665213, 0.09541083, -0.17429787, 0.0246...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.866123</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>ever</td>\n",
       "      <td>[0.031045998, 0.10226928, -0.14378275, 0.02830...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.755050</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>flat</td>\n",
       "      <td>[0.0590803, 0.07470881, -0.11655432, 0.0648619...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.529826</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>something</td>\n",
       "      <td>[0.08940876, 0.033794887, -0.105081536, 0.0439...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.487331</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>grade</td>\n",
       "      <td>[0.09335579, 0.005145905, -0.16178578, 0.01982...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.412076</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>my_mother</td>\n",
       "      <td>[-0.014904412, 0.08968838, -0.07453629, -0.020...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.818222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           words                                            vectors  cluster  \\\n",
       "150       wasn_t  [0.077366784, 0.10052142, -0.07932672, 0.06703...        0   \n",
       "151         home  [-0.0022268514, 0.1320761, -0.10145202, 0.0726...        0   \n",
       "152       always  [0.010727334, 0.10077481, -0.13231076, 0.06112...        0   \n",
       "153          who  [0.041929875, 0.06628057, -0.0635532, 0.032363...        0   \n",
       "154         said  [0.073347755, 0.08684462, -0.11043333, 0.01335...        0   \n",
       "155           or  [0.09029762, 0.11544328, -0.0903675, 0.0237198...        0   \n",
       "156         some  [0.06544155, 0.08807347, -0.111523494, 0.04467...        0   \n",
       "157      noticed  [0.0069835912, 0.10363454, -0.14654888, 0.0522...        0   \n",
       "158        where  [0.013180582, 0.040102858, -0.01023495, -0.055...        1   \n",
       "159          did  [0.058951147, 0.12304556, -0.15273815, 0.01902...        0   \n",
       "160        weird  [0.060342334, 0.05503284, -0.09524252, 0.03339...        0   \n",
       "161        close  [0.028209023, 0.10367024, -0.13795954, 0.05265...        0   \n",
       "162        asked  [0.040985093, 0.0870999, -0.15866901, 0.039822...        0   \n",
       "163      because  [0.026819792, 0.10956048, -0.16380951, 0.03697...        0   \n",
       "164       t_have  [0.04718983, 0.05452005, -0.074673064, -0.0222...        0   \n",
       "165         told  [0.046398334, 0.116562754, -0.16370094, 0.0517...        0   \n",
       "166          out  [0.06044977, 0.09710592, -0.13321766, 0.042567...        0   \n",
       "167         beat  [0.06487117, 0.08845928, -0.11587944, 0.048797...        0   \n",
       "168         your  [0.084559724, 0.07086326, -0.09732823, 0.03486...        0   \n",
       "169           go  [0.07789978, 0.059355702, -0.12908569, 0.01887...        0   \n",
       "170        thing  [0.036836628, 0.07842893, -0.11226163, 0.06549...        0   \n",
       "171            s  [0.038622584, 0.10879778, -0.13081372, 0.06046...        0   \n",
       "172        never  [0.069072604, 0.068447806, -0.15002675, 0.0345...        0   \n",
       "173     everyone  [0.010085005, 0.020066313, -0.062995285, -0.03...        0   \n",
       "174          fat  [0.055769652, 0.0678707, -0.15594542, 0.030770...        0   \n",
       "175         even  [0.026819909, 0.09234932, -0.12029709, 0.05883...        0   \n",
       "176  high_school  [0.0495706, 0.09740633, -0.14637525, 0.0124422...        0   \n",
       "177        girls  [0.008823015, 0.01955772, -0.1417351, -0.00159...        0   \n",
       "178         name  [0.006965787, 0.08794537, -0.16881044, 0.06244...        0   \n",
       "179       pretty  [0.058723785, 0.062196314, -0.1570344, 0.02858...        0   \n",
       "180     bullying  [0.0039847954, 0.059772078, -0.15487488, 0.038...        0   \n",
       "181      bullied  [0.08529985, 0.055052157, -0.114659645, 0.0366...        0   \n",
       "182     about_it  [0.052249905, 0.13277811, -0.16550201, -0.0163...        0   \n",
       "183      my_best  [0.06210439, 0.11778579, -0.06944129, 0.075448...        1   \n",
       "184   mean_girls  [-0.0412226, 0.048035163, -0.08706963, -0.0355...        1   \n",
       "185   my_friends  [0.01280857, 0.11863217, -0.13743517, 0.026601...        0   \n",
       "186        still  [0.0016612421, 0.06699986, -0.13410479, 0.0021...        0   \n",
       "187      year_in  [0.02415606, 0.13417998, -0.17077604, -0.00097...        0   \n",
       "188         knew  [0.045925792, 0.125555, -0.11345351, 0.0595440...        0   \n",
       "189        class  [0.011481658, 0.11176742, -0.14259523, 0.06232...        0   \n",
       "190        while  [0.06507579, 0.07872616, -0.119987294, 0.02853...        0   \n",
       "191         boys  [0.04898546, -0.049998425, 0.036221102, -0.053...        1   \n",
       "192        start  [0.04280632, 0.09447646, -0.12015669, 0.052337...        0   \n",
       "193         then  [-0.017736409, -0.04889355, -0.0628197, -0.011...        1   \n",
       "194     the_girl  [-0.005665213, 0.09541083, -0.17429787, 0.0246...        0   \n",
       "195         ever  [0.031045998, 0.10226928, -0.14378275, 0.02830...        0   \n",
       "196         flat  [0.0590803, 0.07470881, -0.11655432, 0.0648619...        0   \n",
       "197    something  [0.08940876, 0.033794887, -0.105081536, 0.0439...        0   \n",
       "198        grade  [0.09335579, 0.005145905, -0.16178578, 0.01982...        0   \n",
       "199    my_mother  [-0.014904412, 0.08968838, -0.07453629, -0.020...        0   \n",
       "\n",
       "     cluster_value  closeness_score  sentiment_coeff  \n",
       "150              0         1.950220              0.0  \n",
       "151              0         2.055336              0.0  \n",
       "152              0         2.791127              0.0  \n",
       "153              0         1.556763              0.0  \n",
       "154              0         1.807974              0.0  \n",
       "155              0         1.973365              0.0  \n",
       "156              0         3.464964              0.0  \n",
       "157              0         2.110968              0.0  \n",
       "158              0         1.159982              0.0  \n",
       "159              0         1.892291              0.0  \n",
       "160              0         2.452633              0.0  \n",
       "161              0         3.777033              0.0  \n",
       "162              0         5.789738              0.0  \n",
       "163              0         2.533310              0.0  \n",
       "164              0         1.412324              0.0  \n",
       "165              0         3.110518              0.0  \n",
       "166              0         3.596244              0.0  \n",
       "167              0         3.011828              0.0  \n",
       "168              0         2.284449              0.0  \n",
       "169              0         2.730077              0.0  \n",
       "170              0         2.288906              0.0  \n",
       "171              0         2.649110              0.0  \n",
       "172              0         3.112595              0.0  \n",
       "173              0         1.449513              0.0  \n",
       "174              0         3.739899              0.0  \n",
       "175              0         2.941256              0.0  \n",
       "176              0         4.058419              0.0  \n",
       "177              0         1.386159              0.0  \n",
       "178              0         2.021178              0.0  \n",
       "179              0         2.871746              0.0  \n",
       "180              0         2.402651              0.0  \n",
       "181              0         2.440752              0.0  \n",
       "182              0         1.727195              0.0  \n",
       "183              0         1.255333              0.0  \n",
       "184              0         1.197887              0.0  \n",
       "185              0         2.594825              0.0  \n",
       "186              0         1.968430              0.0  \n",
       "187              0         1.998927              0.0  \n",
       "188              0         2.553781              0.0  \n",
       "189              0         2.703285              0.0  \n",
       "190              0         3.904274              0.0  \n",
       "191              0         0.962533              0.0  \n",
       "192              0         2.802161              0.0  \n",
       "193              0         1.132048              0.0  \n",
       "194              0         1.866123              0.0  \n",
       "195              0         2.755050              0.0  \n",
       "196              0         2.529826              0.0  \n",
       "197              0         1.487331              0.0  \n",
       "198              0         1.412076              0.0  \n",
       "199              0         1.818222              0.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[150:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[['words', 'sentiment_coeff']].to_csv('sentiment_dictionary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
