{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['opencv-text-detection.zip',\n",
       " '.ipynb_checkpoints',\n",
       " 'accepted_projects',\n",
       " 'Andrew W. Trask - Grokking Deep Learning-Manning Publications (2019).pdf',\n",
       " 'Collective_Dataset',\n",
       " 'corpus',\n",
       " 'corpus.zip',\n",
       " 'doc2vec.ipynb',\n",
       " 'doc2vec_with_kfold.html',\n",
       " 'doc2vec_with_kfold.ipynb',\n",
       " 'Final Slides and Books',\n",
       " 'google-play-store-apps',\n",
       " 'google-play-store-apps.zip',\n",
       " 'helpline-of-all-sorts',\n",
       " 'helpline-of-all-sorts.zip',\n",
       " 'kmeans_clustering.html',\n",
       " 'kmeans_clustering.ipynb',\n",
       " 'labels.txt',\n",
       " 'logistic_regression.ipynb',\n",
       " 'MURA-v1.1',\n",
       " 'MURA-v1.1.zip',\n",
       " 'nltk',\n",
       " 'opencv-text-detection',\n",
       " 'Papers',\n",
       " 'Papers.zip',\n",
       " 'Papers_Association_Rule',\n",
       " 'processed_dataset.csv',\n",
       " 'Results',\n",
       " 'reviews.txt',\n",
       " 'sentiment_analysis_from_story_preprocessing.html',\n",
       " 'sentiment_analysis_from_story_preprocessing.ipynb',\n",
       " 'sentiment_dictionary.csv',\n",
       " 'sklearn_test.ipynb',\n",
       " 'Slides',\n",
       " 'Story-categorization-using-NLP',\n",
       " 'Tan.pdf',\n",
       " 'Testing',\n",
       " 'Udacity',\n",
       " 'videodata.csv',\n",
       " 'word2vec.model',\n",
       " 'word2vector_test.ipynb']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import multiprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "cores = multiprocessing.cpu_count()\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.getcwd() + \"/Collective_Dataset/419_data - Sheet1.csv\", usecols=[0,1])#header=None, , names=['story', 'category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just like any other day, employees arrived in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My so-called ‘friends’ in middle school used t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i have been called hurtful names and i have be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at my old school kids would hit me and call me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I had debilitating migraines for three years b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I love my work, but hate going each day becaus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I have a chronic illness which was doing well ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The other part is that sense of worthlessness....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I feel my whole body hurting. My mental health...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>As a librarian, I've been threatened with stal...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               story  category\n",
       "0  Just like any other day, employees arrived in ...         0\n",
       "1  My so-called ‘friends’ in middle school used t...         1\n",
       "2  i have been called hurtful names and i have be...         1\n",
       "3  at my old school kids would hit me and call me...         1\n",
       "4  I had debilitating migraines for three years b...         0\n",
       "5  I love my work, but hate going each day becaus...         0\n",
       "6  I have a chronic illness which was doing well ...         0\n",
       "7  The other part is that sense of worthlessness....         0\n",
       "8  I feel my whole body hurting. My mental health...         0\n",
       "9  As a librarian, I've been threatened with stal...         2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Just like any other day, employees arrived in ...\n",
       "1      My so-called ‘friends’ in middle school used t...\n",
       "2      i have been called hurtful names and i have be...\n",
       "3      at my old school kids would hit me and call me...\n",
       "4      I had debilitating migraines for three years b...\n",
       "                             ...                        \n",
       "250    Yes, software jobs are stressful. They are pre...\n",
       "251    I was doing what I thought was my dream job. I...\n",
       "252    All of my work experience has been in the Los ...\n",
       "253    I remember it very vividly: me, lying on my be...\n",
       "254    My stress strikes when I find out that I am th...\n",
       "Name: story, Length: 255, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      0\n",
       "      ..\n",
       "250    0\n",
       "251    0\n",
       "252    0\n",
       "253    0\n",
       "254    0\n",
       "Name: category, Length: 255, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = range(255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47669"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.story.apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtYAAAEGCAYAAABBxtJ0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAYtklEQVR4nO3debRlZX3m8e8jSMugjUCBCFUUCirEOHVBcFhOOKCA2B1xWFrQSCQmjnEC062kbaNobIyJiSsoaAkoZQCFgK0hKItGW7FwHkARC6qghAIpDU5Mv/7j7GqP17p1t3X2vvdu7vez1ln3nHefc96HWutSD5t3vztVhSRJkqTJ3GuuA0iSJEn3BBZrSZIkqQMWa0mSJKkDFmtJkiSpAxZrSZIkqQMWa0mSJKkDW8/GJElOAw4DbqqqhzdjOwErgaXAauD5VXVrkgDvA54N/AL4r1X11Znm2GWXXWrp0qW95JckSZI2uuKKK26uqkVTx2elWAMfAd4PfHRs7ATg4qo6KckJzevjgWcB+zaPPwI+0PzcrKVLl7Jq1aqOY0uSJEm/Lcm1mxqflaUgVXUp8JMpw0cAK5rnK4Dnjo1/tEa+BOyYZPfZyClJkiRtqblcY71bVa0DaH7u2ozvAawZe9/aZkySJEmat+bjxYvZxNgm77ue5Lgkq5KsWr9+fc+xJEmSpOnNZbG+ceMSj+bnTc34WmDx2Pv2BG7Y1BdU1SlVtayqli1a9DvrxyVJkqRZM5fF+nzg6Ob50cB5Y+NHZeQg4Kcbl4xIkiRJ89Vsbbf3ceDJwC5J1gInAicBn0hyLHAdcGTz9k8z2mrvakbb7R0zGxklSZKkScxKsa6qF01z6OBNvLeAV/SbSJIkSerWfLx4UZIkSRqc2bpBjCT93n78wRfMdQRpYg942cq5jiBplnjGWpIkSeqAxVqSJEnqgMVakiRJ6oDFWpIkSeqAxVqSJEnqgMVakiRJ6oDb7UmSpN/y0pUvnesI0sROe8Fpsz6nZ6wlSZKkDlisJUmSpA5YrCVJkqQOWKwlSZKkDlisJUmSpA5YrCVJkqQOWKwlSZKkDlisJUmSpA5YrCVJkqQOWKwlSZKkDlisJUmSpA5YrCVJkqQOWKwlSZKkDlisJUmSpA5YrCVJkqQOWKwlSZKkDlisJUmSpA5YrCVJkqQOWKwlSZKkDlisJUmSpA5sUbFOsm2SbboOI0mSJA1Vq2Kd5D1JDmyeHwr8BNiQ5PA+w0mSJElD0faM9YuBbzfP3wq8BHgO8I4+QkmSJElDs3XL921XVb9IsjPwoKo6ByDJXv1FkyRJkoajbbH+fpIXA/sAFwEk2QX4ZV/BJEmSpCFpuxTkz4FXAE8F3tKMPRP410kDJPmLJN9J8u0kH09ynyR7J/lykh8kWemFkpIkSZrvWhXrqvpKVT2uqp5UVT9sxs6squWTTJ5kD+DVwLKqejiwFfBC4F3Ae6tqX+BW4NhJ5pEkSZL61nq7vSRPT3Jqkn9pXi9L8tQOMmwNbJtka2A7YB2jM+NnN8dXAM/tYB5JkiSpN22323sV8AHgB8ATm+FfAm+fZPKquh54D3Ado0L9U+AKYENV3dm8bS2wxyTzSJIkSX1re8b6tcDTquok4O5m7ErgoZNMnuT+wBHA3sADge2BZ23irTXN549LsirJqvXr108SRZIkSZpI22J9X2BN83xjyb03cPuE8z8N+FFVra+qO4BzgccBOzZLQwD2BG7Y1Ier6pSqWlZVyxYtWjRhFEmSJGnLtS3WlwInTBl7NfD5Cee/DjgoyXZJAhwMfLf53uc17zkaOG/CeSRJkqRetS3WrwL+c5LVwH2TXAUcCbxuksmr6suMLlL8KvCtJs8pwPHA65JcDewMnDrJPJIkSVLfWt0gpqrWJTkAOADYi9GykMur6u7Nf7LVd58InDhl+BrgwEm/W5IkSZotrYp1kkcBt1TV5cDlzdjiJDtV1Tf6DChJkiQNQdulIGcwulhx3DbA6d3GkSRJkoapbbFeUlXXjA80d2Bc2nkiSZIkaYDaFuu1SR4zPtC83uQ2eJIkSdJC02qNNfBe4Lwk7wZ+CDwYeAPw130FkyRJkoak7a4gH0yyATgWWMxoV5DXV9XZfYaTJEmShqLtGWuq6p+Bf+4xiyRJkjRYrYt1kmcAjwJ2GB+vqrd2HUqSJEkamrb7WL8feD6jW43/YuxQ9RFKkiRJGpq2Z6xfBDyqqtb0GUaSJEkaqrbb7d0CbOgziCRJkjRkbc9Y/y/gzCTvBG4cPzD1xjGSJEnSQtS2WH+g+XnYlPECtuoujiRJkjRMbfexbrtkRJIkSVqQfq/CnGRxkoP6CiNJkiQNVatinWRJki8AVwL/1ow9L8mH+gwnSZIkDUXbM9b/BFwI3Be4oxm7CHh6H6EkSZKkoWl78eKBwKFVdXeSAqiqnyb5j/1FkyRJkoaj7RnrG4F9xgeS7A9c13kiSZIkaYDaFuv3ABckOQbYOsmLgJXAu3pLJkmSJA1I2+32TkvyE+A4YA1wFPCWqvpUn+EkSZKkoZixWCfZCjgR+GuLtCRJkrRpMy4Fqaq7gFfwm91AJEmSJE3Rdo31CuDlfQaRJEmShuz32W7vVUnexGiNdW08UFVP7COYJEmSNCRti/UHm4ckSZKkTWh78eKDGV28+Ov+I0mSJEnD48WLkiRJUge8eFGSJEnqgBcvzuAl77twriNInTjjNYfOdQRJku7RvHhRkiRJ6kDbW5qv6DuIJEmSNGStinWSl053rKpO6y6OJEmSNExtl4Isn/L6AYy24PsCYLGWJEnSgtd2KchTpo41Z7H3mzRAkh2BDwEPZ3RR5EuBq4CVwFJgNfD8qrp10rkkSZKkvrTdbm9TPgIc20GG9wGfqaqHAY8EvgecAFxcVfsCFzevJUmSpHmrVbFOcq8pjx2A44ANk0ye5H7AE4FTAarq9qraABzBaO9smp/PnWQeSZIkqW9t11jfydje1Y3rGZXrSTwIWA98OMkjgSuA1wC7VdU6gKpal2TXTX04yXEbMyxZsmTCKJIkSdKWa7sUZG9GJXjjY7eqWlJVn5lw/q2BxwAfqKpHAz/n91j2UVWnVNWyqlq2aNGiCaNIkiRJW65tsb4T+FlVXds8bk5y/yQPnHD+tcDaqvpy8/psRkX7xiS7AzQ/b5pwHkmSJKlXbYv1p4A9p4ztCXxyksmr6sfAmiQPbYYOBr4LnA8c3YwdDZw3yTySJElS39qusX5oVX1rfKCqvpXkYR1keBVwZpJtgGuAYxgV/k8kORa4Djiyg3kkSZKk3rQt1jcl2aeqrt44kGQf4JZJA1TV14Flmzh08KTfLUmSJM2WtktBTgPOSXJYkv2THM5oPfSH+osmSZIkDUfbM9YnAXcA7wEWM1qecSpwck+5JEmSpEFpe0vzu4G/aR6SJEmSpmh758UTkhwwZezAJG/qJ5YkSZI0LG3XWL+G0TZ4474LvLbbOJIkSdIwtS3W2zBaYz3uduA+3caRJEmShqltsb4C+PMpYy8HvtptHEmSJGmY2u4K8hfARUmWAz8E9gF2A57eVzBJkiRpSNruCvKdJA8BDmO03d65wAVVdVuf4SRJkqShaHvGGmB34Frgiqr6QU95JEmSpEGacY11kv+SZDVwFfAF4Mokq5M8r+9wkiRJ0lBstlgnORT4MPCPwIOAbYEHAx8APpTksN4TSpIkSQMw01KQtwB/WlVnjY2tBt6V5Lrm+AU9ZZMkSZIGY6alIH8AfHKaY+cC+3cbR5IkSRqmmYr1r4H7TXNsR0Y3iZEkSZIWvJmK9WeAd05z7B3AZ7uNI0mSJA3TTGusjwcuS/JN4BxgHaNt9/6Y0ZnsJ/QbT5IkSRqGzRbrqro+yWOA1wGHALsANwPnAe+tqp/0H1GSJEma/2a8QUxV3cpo94+39B9HkiRJGqYZbxAjSZIkaWYWa0mSJKkDFmtJkiSpA9MW6yRfGnt+4uzEkSRJkoZpc2esH5LkPs3z189GGEmSJGmoNrcryHnA95OsBrZNcumm3lRVT+wjmCRJkjQk0xbrqjomyROApcABwKmzFUqSJEkampluEHMZozsvblNVK2YpkyRJkjQ4M94gBqCqTkvyFGA5sAdwPXBGVX2uz3CSJEnSULTabi/JnwArgR8D5wLrgI8leVmP2SRJkqTBaHXGGngT8PSq+sbGgSQrgXOAD/YRTJIkSRqStjeI2Rn47pSxq4Cduo0jSZIkDVPbYn0ZcHKS7QCSbA/8DfDFvoJJkiRJQ9K2WL8ceATw0yQ3AhuARwJ/2lcwSZIkaUja7gqyDnhSkj2BBwI3VNXarkIk2QpYBVxfVYcl2Rs4i9FSk68Cy6vq9q7mkyRJkrrW9ow1AFW1tqou77JUN14DfG/s9buA91bVvsCtwLEdzydJkiR16vcq1n1ozoIfCnyoeR3gqcDZzVtWAM+dm3SSJElSO3NerIG/ZbSd393N652BDVV1Z/N6LaOb0kiSJEnz1ozFOsm9kjw1yTZdT57kMOCmqrpifHgTb61pPn9cklVJVq1fv77reJIkSVJrMxbrqrobOK+niwcfDzwnyWpGFys+ldEZ7B2TbLywck/ghmmynVJVy6pq2aJFi3qIJ0mSJLXTdinIpUkO6nryqnpzVe1ZVUuBFwKfq6oXA58Hnte87WjgvK7nliRJkrrU9pbm1wL/O8l5wBrGlmZU1Vt7yHU8cFaStwNfA07tYQ5JkiSpM22L9bbAp5rne/YRpKouAS5pnl8DHNjHPJIkSVIf2t4g5pi+g0iSJElD1vaMNUn2Y7TuebeqemWShwL/oaq+2Vs6SZIkaSBaXbyY5EjgUkb7SR/VDN8XOLmnXJIkSdKgtN0V5G3A06vq5cBdzdg3gEf2kkqSJEkamLbFeldGRRp+syNIMc2NWyRJkqSFpm2xvgJYPmXshcDl3caRJEmShqntxYuvBv41ybHA9kk+CzwEeEZvySRJkqQBabvd3pVJHgYcBlzA6CYxF1TVbX2GkyRJkoai9XZ7VfWLJF8AfgTcYKmWJEmSfqPtdntLkvwfYDVwIbA6yWVJ9uoznCRJkjQUbS9eXMHoAsYdq2pX4P7AV5pxSZIkacFruxTkPwHPqKo7AKrqtiTHA7f0lkySJEkakLZnrL8EHDhlbBnwf7uNI0mSJA3TtGesk7xt7OUPgU8nuZDRjiCLgWcDH+s3niRJkjQMm1sKsnjK63Obn7sCvwY+Cdynj1CSJEnS0ExbrKvqmNkMIkmSJA1Z632sk2wH7APsMD5eVV/sOpQkSZI0NK2KdZKjgPcDtwO/HDtUwJIeckmSJEmD0vaM9buBP66qi/oMI0mSJA1V2+32bgcu6TGHJEmSNGhti/VbgJOT7NJnGEmSJGmo2hbr7wPPAW5MclfzuDvJXT1mkyRJkgaj7Rrr04GPAiv57YsXJUmSJNG+WO8MvLWqqs8wkiRJ0lC1XQryYWB5n0EkSZKkIWt7xvpA4JVJ/htw4/iBqnpi56kkSZKkgWlbrD/YPCRJkiRtQqtiXVUr+g4iSZIkDVnbW5q/dLpjVXVad3EkSZKkYWq7FGTqhYsPAB4MfAGwWEuSJGnBa7sU5ClTx5qz2Pt1nkiSJEkaoLbb7W3KR4BjO8ohSZIkDVrbNdZTC/h2wEuADZ0nkiRJkgao7RrrO4Gpd128HnhZt3EkSZKkYWpbrPee8vrnVXXzpJMnWQx8lNHFkHcDp1TV+5LsBKwElgKrgedX1a2TzidJkiT1pdUa66q6dspj4lLduBN4fVXtBxwEvCLJ/sAJwMVVtS9wcfNakiRJmrc2e8Y6yef53SUg46qqDt7SyatqHbCuef7vSb4H7AEcATy5edsK4BLg+C2dR5IkSerbTEtBzphmfA/g1YwuYuxEkqXAo4EvA7s1pZuqWpdk167mkSRJkvqw2WJdVaeOv06yM/BmRhctrgTe1kWIJDsA5wCvraqfJWn7ueOA4wCWLFnSRRRJkiRpi7RaY53kfkn+J3A1sBvwmKo6rqrWThogyb0Zleozq+rcZvjGJLs3x3cHbtrUZ6vqlKpaVlXLFi1aNGkUSZIkaYtttlgn2TbJm4FrGN1l8QlVtbyqftjF5Bmdmj4V+F5VnTx26Hzg6Ob50cB5XcwnSZIk9WWmNdY/ArYC3g2sAnZLstv4G6rqcxPM/3hgOfCtJF9vxv4SOAn4RJJjgeuAIyeYQ5IkSerdTMX6V4x2BfmzaY4X8KAtnbyqLgOmW1C9xbuNSJIkSbNtposXl85SDkmSJGnQWl28KEmSJGnzLNaSJElSByzWkiRJUgcs1pIkSVIHLNaSJElSByzWkiRJUgcs1pIkSVIHLNaSJElSByzWkiRJUgcs1pIkSVIHLNaSJElSByzWkiRJUgcs1pIkSVIHLNaSJElSByzWkiRJUgcs1pIkSVIHLNaSJElSByzWkiRJUgcs1pIkSVIHLNaSJElSByzWkiRJUgcs1pIkSVIHLNaSJElSByzWkiRJUgcs1pIkSVIHLNaSJElSByzWkiRJUgcs1pIkSVIHLNaSJElSByzWkiRJUgcs1pIkSVIHLNaSJElSByzWkiRJUgfmbbFOckiSq5JcneSEuc4jSZIkbc68LNZJtgL+AXgWsD/woiT7z20qSZIkaXrzslgDBwJXV9U1VXU7cBZwxBxnkiRJkqaVqprrDL8jyfOAQ6rqT5rXy4E/qqpXTnnfccBxzcuHAlfNalB1aRfg5rkOIS1A/u5Jc8PfvWHbq6oWTR3cei6StJBNjP3OfwFU1SnAKf3HUd+SrKqqZXOdQ1po/N2T5oa/e/dM83UpyFpg8djrPYEb5iiLJEmSNKP5Wqy/AuybZO8k2wAvBM6f40ySJEnStOblUpCqujPJK4HPAlsBp1XVd+Y4lvrlkh5pbvi7J80Nf/fugeblxYuSJEnS0MzXpSCSJEnSoFisJUmSpA5YrCVJkqQOzMuLF3XPluRhjO6kuQej/clvAM6vqu/NaTBJknrS/N23B/DlqrptbPyQqvrM3CVTlzxjrVmV5HhGt6gPcDmjrRUDfDzJCXOZTVrIkhwz1xmke6okrwbOA14FfDvJEWOH3zE3qdQHdwXRrEryfeAPquqOKePbAN+pqn3nJpm0sCW5rqqWzHUO6Z4oybeAx1bVbUmWAmcDp1fV+5J8raoePacB1RmXgmi23Q08ELh2yvjuzTFJPUnyzekOAbvNZhZpgdlq4/KPqlqd5MnA2Un2YvT7p3sIi7Vm22uBi5P8AFjTjC0B9gFeOWeppIVhN+CZwK1TxgN8cfbjSAvGj5M8qqq+DtCcuT4MOA34w7mNpi5ZrDWrquozSR4CHMjoIo4Aa4GvVNVdcxpOuue7ANhh41/u45JcMvtxpAXjKODO8YGquhM4Ksk/zU0k9cE11pIkSVIH3BVEkiRJ6oDFWpIkSeqAxVqSJEnqgMVaku5BkqxO8rS5ziFJC5HFWpI0sSTuMiVpwbNYS9I8lWRxknOTrE9yS5L3J3lwks81r29OcmaSHZv3n85oX/h/SXJbkjc14wcl+WKSDUm+0dycYuMceye5NMm/J/m3JP+Q5Iyx489J8p3ms5ck2W/s2Ookxzc3nvl5kjcmOWfKP8PfJ/nbfv+kJGl+sFhL0jyUZCtG+05fCyxltO/7WYz2fn8nozuY7gcsBv4KoKqWA9cBh1fVDlX17iR7ABcCbwd2At4AnJNkUTPVx4DLgZ2b71k+luEhwMcZ3dhpEfBpRqV9m7GoLwIOBXYEzgAOGSv6WwMvAE7v5k9FkuY3i7UkzU8HMirPb6yqn1fVr6rqsqq6uqouqqpfV9V64GTgSZv5npcAn66qT1fV3VV1EbAKeHaSJcABwFur6vaqugw4f+yzLwAubOa7A3gPsC3wuLH3/F1VramqX1bVOuBS4Mjm2CHAzVV1xcR/GpI0ABZrSZqfFgPXNndn+/+S7JrkrCTXJ/kZo7PEu2zme/YCjmyWcmxIsgF4ArA7o+L+k6r6xdj714w9fyCjM+YAVNXdzfE9pnk/wApGZZ7mp2erJS0YFmtJmp/WAEs2cVHgO4ECHlFV92NUXjN2fOrtdNcAp1fVjmOP7avqJGAdsFOS7cbev3js+Q2MijkASdIcv34z830KeESShwOHAWe2+GeVpHsEi7UkzU+XMyq+JyXZPsl9kjweuC9wG7ChWT/9ximfuxF40NjrM4DDkzwzyVbN9zw5yZ5VdS2jZSF/lWSbJI8FDh/77CeAQ5McnOTewOuBXwNfnC50Vf0KOJtm7XZVXTfBn4EkDYrFWpLmoaq6i1HJ3YfRBYlrGa15/h/AY4CfMroo8dwpH30n8N+bZR9vqKo1wBHAXwLrGZ3BfiO/+ff/i4HHArcwusBxJaPyTFVdxeiM+N8DNzd5Dq+q22eIvwL4Q1wGImmBSdXU/4snSVqokqwErqyqEyf4jiXAlcADqupnnYWTpHnOM9aStIAlOaDZG/teSQ5hdHb7UxN8372A1wFnWaolLTTeKUuSFrYHMFpOsjOj5SZ/VlVf25IvSrI9ozXe1zLaak+SFhSXgkiSJEkdcCmIJEmS1AGLtSRJktQBi7UkSZLUAYu1JEmS1AGLtSRJktQBi7UkSZLUgf8H5M8KN7gm78wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnt_pro = df['category'].value_counts()\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('category', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Work Stress Total Data Count:  64\n",
      "Bullying Total Data Count:  97\n",
      "Sexual Harassment Total Data Count:  94\n"
     ]
    }
   ],
   "source": [
    "work_stress_counter, bullying_counter, sexual_harassment_counter = 0, 0, 0\n",
    "for c in df['category']:\n",
    "    if c==0:\n",
    "        work_stress_counter+=1\n",
    "    elif c == 1:\n",
    "        bullying_counter+=1\n",
    "    else:\n",
    "        sexual_harassment_counter+=1\n",
    "print(\"Work Stress Total Data Count: \", work_stress_counter)\n",
    "print(\"Bullying Total Data Count: \", bullying_counter)\n",
    "print(\"Sexual Harassment Total Data Count: \", sexual_harassment_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have been called hurtful names and i have been called black bitch and people are making fun of me for being black by my suppost to be friend t and she has posted rumors about me and i dont know what to do.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(df.story[2])\n",
    "print(df.category[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just like any other day, employees arrived in the workplace sparingly, filling the cubicles and getting their coffees ready. Once more, the manager was already sitting at his desk, grumbling and shouting - You’re way too slow, again! How am I supposed to get my work done with you slowing me down every day? The other employees were staring at each other. They were embarrassed by his outburst but deep inside they knew he was right. Satisfied by the nods in the assembly, the manager calmed down. As usual, he quickly got absorbed by his screen and numerous emails. He was not the moody type, just a normal guy. His team liked him very much and his performance record was exemplary. He was often described as someone caring and trustworthy with a genuine interest in people. Yet once again, his fist hit the desk loudly as he started screaming. That’s it, I’m done! Slow and unreliable old crap! You made me lose five hours of my life, again. World will be better off without you and I’m going to let the board know about that. He stood up suddenly and clenched his fists in anger. He then walked rapidly to the stairs, climbed and disappeared. Nobody saw him again for the day, but they all knew he was going for a confrontation, if not a fight. It was going to get really ugly up there.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df.story[0])\n",
    "print(df.category[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I worked as an office manager, and the only woman, for an industrial insulation company. I had just come back from maternity leave and I was worried about my milk supply. I went into the bathroom to pump for about 15 minutes every two hours, and all of the men in the office would stand in the break area (right in front of the bathroom door) and make baby crying noises to make fun of me. Eventually it progressed to the point that they would make crying noises every time they passed my desk in hopes that I would leak through my shirt. They would also make comments about how much larger my breasts were since having a baby. I felt so harassed and unsafe that I would dread going to work every day, and I even had more than a few nervous breakdowns. My husband was furious and I had to convince him not to take any drastic action so that I could be sure to have a good reference if I needed to find another job. We had a long conversation and looked at our finances and decided the extra money wasn't worth the emotional distress. I ended up quitting my job and staying home with our kids.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(df.story[10])\n",
    "print(df.category[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('x', '')\n",
    "    return text\n",
    "df['story'] = df['story'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH TRAIN_TEST_SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size = .20, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged = train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "test_tagged = test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>when i joined the force 20 years ago, we were ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>my bullying started, or when i first noticed i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>and im 14 years old. all my life, i been bulli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>i am currently 17 years old and a junior in hi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>my eperiences with seual harassment at my reta...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hi, i'm rahul. i was bullied all through schoo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>when i was in 7th grade, i had a semester of m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>being a doctor is one of the hardest jobs main...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i got made fun of for being fat in elementary,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>let me tell you about a few of my eperiences. ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 story  category\n",
       "243  when i joined the force 20 years ago, we were ...         0\n",
       "25   my bullying started, or when i first noticed i...         1\n",
       "115  and im 14 years old. all my life, i been bulli...         1\n",
       "120  i am currently 17 years old and a junior in hi...         1\n",
       "182  my eperiences with seual harassment at my reta...         2\n",
       "..                                                 ...       ...\n",
       "22   hi, i'm rahul. i was bullied all through schoo...         1\n",
       "72   when i was in 7th grade, i had a semester of m...         1\n",
       "237  being a doctor is one of the hardest jobs main...         0\n",
       "15   i got made fun of for being fat in elementary,...         1\n",
       "168  let me tell you about a few of my eperiences. ...         2\n",
       "\n",
       "[204 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>i’ve always been super flat-chested and i’m st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i work in law enforcement. i started out as a ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>i felt my whole body hurting. my mental health...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>i'm 13 years old and live in nsw, australia wi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>it was the late ‘60s, and apparently one of my...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>i was around 12-13 years old and at this “meet...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>my story might be short because i only remembe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>when i got admitted in a well reputed college,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at my old school kids would hit me and call me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>when i was around 10/11do to a little family i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>the summer after i graduated from college, i w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>ever since first grade, my three best friends ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>i am 14 years old and in the 8th grade. i have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>i am a teenage girl now, i’m in the middle sch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>i was most definitely bullied in my high schoo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i worked as an office manager, and the only wo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>the final straw came when i worked 17 days in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>i was 14 years old at that time and went to vi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>i am 15 now and i have been bullied since grad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>one of our rotations as second year family pra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>i find myself ready to leave my 6-figure job t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>a male customer stalked me for several weeks l...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>i have a weird name. i don't even know why i h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>i met a girl in 7th grade, let's call her a, a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>as i was studying in paris, i also worked nigh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>getting on the last, packed, train home from a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>the first time was the day after my 20th birth...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>it all began in the 2nd grade, i was about 3 y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>high school and junior high were hell for me. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>a girl jumped on me on my friend's house! she ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>i had this one particular kid who would pick o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>i work in a pharmacy, and my manager at the ti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>eleven years ago i was seually harassed by a t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>yesterday during lunchbreak thoe girls called ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>i was just starting middle school, and i had t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>this one time, a man came in with his son. whe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>my boyfriend and i were watching a movie in hi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>i have been bullied since i was young. on my f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>during my sophomore year at wake forest, i was...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>hello! first of all, i'm sorry if there are a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>i was in a managerial job with a lot of sites,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>i wouldn't say high-high paying, i had a job t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>i was born with a craniofacial disease called ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>unemployed. i hated my old job (cps). i saw so...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>my skin is black . for this reason i have to t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>i’m a waitress, and i get harassed daily by cu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>i was in class 11. i was coming back from my t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>i work at subway. one time, a man told my cowo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>one time, my freshman year of high school, i w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>when i attended a parochial school, a group of...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>i had been seeing this guy for just over three...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 story  category\n",
       "53   i’ve always been super flat-chested and i’m st...         1\n",
       "13   i work in law enforcement. i started out as a ...         2\n",
       "144  i felt my whole body hurting. my mental health...         0\n",
       "94   i'm 13 years old and live in nsw, australia wi...         1\n",
       "159  it was the late ‘60s, and apparently one of my...         2\n",
       "71   i was around 12-13 years old and at this “meet...         1\n",
       "85   my story might be short because i only remembe...         1\n",
       "35   when i got admitted in a well reputed college,...         1\n",
       "3    at my old school kids would hit me and call me...         1\n",
       "118  when i was around 10/11do to a little family i...         1\n",
       "190  the summer after i graduated from college, i w...         2\n",
       "54   ever since first grade, my three best friends ...         1\n",
       "91   i am 14 years old and in the 8th grade. i have...         1\n",
       "77   i am a teenage girl now, i’m in the middle sch...         1\n",
       "65   i was most definitely bullied in my high schoo...         1\n",
       "10   i worked as an office manager, and the only wo...         2\n",
       "246  the final straw came when i worked 17 days in ...         0\n",
       "41   i was 14 years old at that time and went to vi...         2\n",
       "74   i am 15 now and i have been bullied since grad...         1\n",
       "239  one of our rotations as second year family pra...         0\n",
       "228  i find myself ready to leave my 6-figure job t...         0\n",
       "183  a male customer stalked me for several weeks l...         2\n",
       "93   i have a weird name. i don't even know why i h...         1\n",
       "109  i met a girl in 7th grade, let's call her a, a...         1\n",
       "229  as i was studying in paris, i also worked nigh...         0\n",
       "212  getting on the last, packed, train home from a...         2\n",
       "128  the first time was the day after my 20th birth...         2\n",
       "84   it all began in the 2nd grade, i was about 3 y...         1\n",
       "24   high school and junior high were hell for me. ...         1\n",
       "30   a girl jumped on me on my friend's house! she ...         2\n",
       "64   i had this one particular kid who would pick o...         1\n",
       "214  i work in a pharmacy, and my manager at the ti...         0\n",
       "127  eleven years ago i was seually harassed by a t...         2\n",
       "20   yesterday during lunchbreak thoe girls called ...         1\n",
       "28   i was just starting middle school, and i had t...         1\n",
       "181  this one time, a man came in with his son. whe...         2\n",
       "131  my boyfriend and i were watching a movie in hi...         2\n",
       "90   i have been bullied since i was young. on my f...         1\n",
       "126  during my sophomore year at wake forest, i was...         2\n",
       "101  hello! first of all, i'm sorry if there are a ...         1\n",
       "223  i was in a managerial job with a lot of sites,...         0\n",
       "222  i wouldn't say high-high paying, i had a job t...         0\n",
       "23   i was born with a craniofacial disease called ...         1\n",
       "224  unemployed. i hated my old job (cps). i saw so...         0\n",
       "230  my skin is black . for this reason i have to t...         1\n",
       "14   i’m a waitress, and i get harassed daily by cu...         2\n",
       "29   i was in class 11. i was coming back from my t...         1\n",
       "171  i work at subway. one time, a man told my cowo...         2\n",
       "18   one time, my freshman year of high school, i w...         1\n",
       "169  when i attended a parochial school, a group of...         2\n",
       "197  i had been seeing this guy for just over three...         2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243    ([when, joined, the, force, 20, years, ago, we...\n",
       "25     ([my, bullying, started, or, when, first, noti...\n",
       "115    ([and, im, 14, years, old, all, my, life, been...\n",
       "120    ([am, currently, 17, years, old, and, junior, ...\n",
       "182    ([my, eperiences, with, seual, harassment, at,...\n",
       "                             ...                        \n",
       "22     ([hi, 'm, rahul, was, bullied, all, through, s...\n",
       "72     ([when, was, in, 7th, grade, had, semester, of...\n",
       "237    ([being, doctor, is, one, of, the, hardest, jo...\n",
       "15     ([got, made, fun, of, for, being, fat, in, ele...\n",
       "168    ([let, me, tell, you, about, few, of, my, eper...\n",
       "Length: 204, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['and', 'im', '14', 'years', 'old', 'all', 'my', 'life', 'been', 'bullied', 'non-stop', '1st', 'grade', 'to', '9th', 'grade', 'on', 'saturday', 'was', 'sleeping', 'in', 'got', 'call', 'from', 'blocked', 'number', 'calling', 'me', 'really', 'awful', 'names', 'telling', 'me', 'that', 'nobody', 'wants', 'me', 'here', 'and', 'for', 'me', 'to', 'kill', 'myself', 'just', 'let', 'it', 'go', 'fell', 'back', 'to', 'sleep..', 'got', 'another', 'call', 'from', 'restricted', 'tellin', 'me', 'the', 'same', 'thing', 'felt', 'devastated', 'everybody', 'in', 'that', 'school', 'thought', 'was', 'the', 'edited', 'felt', 'trashy', 'embarrassed', 'ran', 'down', 'the', 'hall', 'out', 'the', 'door', 'went', 'back', 'home', 'missed', 'weeks', 'of', 'school', 'the', 'bullying', 'was', 'bad', 'checked', 'my', 'facebook', 'everybody', 'was', 'talking', 'about', 'me', 'felt', 'worthless', 'lonely', 'tried', 'to', 'kill', 'myself', 'cut', 'took', '15', 'pills', 'nothing', 'happens', 'when', 'went', 'back', 'to', 'school', 'nothing', 'changed', 'walked', 'the', 'halls', 'alone', 'had', 'no', 'friends', 'well', 'at', 'lunch', 'that', 'day', 'this', 'really', 'rude', 'girl', 'threw', 'my', 'food', 'in', 'my', 'face', 'was', 'fed', 'up', 'with', 'it', 'finally', 'stood', 'up', 'for', 'myself', 'told', 'her', 'to', 'back', 'off', 'she', \"'s\", 'no', 'better', 'than', 'me', 'after', 'that', 'nobody', 'messed', 'with', 'me', 'just', 'had', 'to', 'stand', 'up', 'forself', 'then', 'everybody', 'left', 'me', 'alone', \"'m\", 'glad', 'did', \"n't\", 'die', 'because', 'if', 'did', 'in', 'would', \"n't\", 'of', 'got', 'the', 'chance', 'to', 'show', 'everybody', 'who', 'really', 'am'], tags=[1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged.values[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['work', 'in', 'law', 'enforcement', 'started', 'out', 'as', 'correctional', 'officer', 'in', 'an', 'all-male', 'prison', 'when', 'was', 'walking', 'to', 'my', 'post', 'one', 'day', 'an', 'inmate', 'screamed', 'at', 'me', \"'bitch\", \"'ll\", 'beat', 'your', 'pussy', 'so', 'hard', 'it', \"'ll\", 'put', 'you', 'in', 'the', 'hospital', 'called', 'unit', 'code', 'because', 'to', 'me', 'that', 'is', 'harassment', 'violent', 'seual', 'harassment', 'but', 'because', 'worked', 'in', 'male', 'prison', 'was', 'supposed', 'to', 'let', 'it', 'go', 'in', 'one', 'ear', 'and', 'out', 'the', 'other'], tags=[2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tagged.values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PV-DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 839684.02it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1275168.43it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 836400.80it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1711276.03it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 770151.23it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1151598.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 381 ms, sys: 16.3 ms, total: 397 ms\n",
      "Wall time: 154 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(5):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.8235294117647058\n",
      "Testing F1 score: 0.8209623371895177\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  1  0]\n",
      " [ 1 23  1]\n",
      " [ 3  3 11]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.89      0.76         9\n",
      "           1       0.85      0.92      0.88        25\n",
      "           2       0.92      0.65      0.76        17\n",
      "\n",
      "    accuracy                           0.82        51\n",
      "   macro avg       0.81      0.82      0.80        51\n",
      "weighted avg       0.84      0.82      0.82        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "print('\\nTesting Confusion Matrix: ')\n",
    "print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "print('Testing Classification Report: ')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PV-DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1495870.66it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 616453.90it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 782834.42it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 949653.74it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 954953.14it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 997247.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 455 ms, sys: 23.6 ms, total: 478 ms\n",
      "Wall time: 187 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(5):\n",
    "    model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dmm.alpha -= 0.002\n",
    "    model_dmm.min_alpha = model_dmm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dmm, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dmm, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.8823529411764706\n",
      "Testing F1 score: 0.883290571130413\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  0  2]\n",
      " [ 0 23  2]\n",
      " [ 1  1 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.78      0.82         9\n",
      "           1       0.96      0.92      0.94        25\n",
      "           2       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.88        51\n",
      "   macro avg       0.87      0.86      0.87        51\n",
      "weighted avg       0.89      0.88      0.88        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "print('\\nTesting Confusion Matrix: ')\n",
    "print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "print('Testing Classification Report: ')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = get_vectors(new_model, train_tagged)\n",
    "y_test, X_test = get_vectors(new_model, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.803921568627451\n",
      "Testing F1 score: 0.808761927067704\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  0  3]\n",
      " [ 2 21  2]\n",
      " [ 2  1 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.67      0.63         9\n",
      "           1       0.95      0.84      0.89        25\n",
      "           2       0.74      0.82      0.78        17\n",
      "\n",
      "    accuracy                           0.80        51\n",
      "   macro avg       0.76      0.78      0.77        51\n",
      "weighted avg       0.82      0.80      0.81        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "print('\\nTesting Confusion Matrix: ')\n",
    "print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "print('Testing Classification Report: ')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH K-FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  255\n",
      "Type:  <class 'pandas.core.series.Series'>\n",
      "First Ten Values:\n",
      " 0    just like any other day, employees arrived in ...\n",
      "1    my so-called ‘friends’ in middle school used t...\n",
      "2    i have been called hurtful names and i have be...\n",
      "3    at my old school kids would hit me and call me...\n",
      "4    i had debilitating migraines for three years b...\n",
      "5    i love my work, but hate going each day becaus...\n",
      "6    i have a chronic illness which was doing well ...\n",
      "7    the other part is that sense of worthlessness....\n",
      "8    i feel my whole body hurting. my mental health...\n",
      "9    as a librarian, i've been threatened with stal...\n",
      "Name: story, dtype: object\n",
      "just like any other day, employees arrived in the workplace sparingly, filling the cubicles and getting their coffees ready. once more, the manager was already sitting at his desk, grumbling and shouting - you’re way too slow, again! how am i supposed to get my work done with you slowing me down every day? the other employees were staring at each other. they were embarrassed by his outburst but deep inside they knew he was right. satisfied by the nods in the assembly, the manager calmed down. as usual, he quickly got absorbed by his screen and numerous emails. he was not the moody type, just a normal guy. his team liked him very much and his performance record was eemplary. he was often described as someone caring and trustworthy with a genuine interest in people. yet once again, his fist hit the desk loudly as he started screaming. that’s it, i’m done! slow and unreliable old crap! you made me lose five hours of my life, again. world will be better off without you and i’m going to let the board know about that. he stood up suddenly and clenched his fists in anger. he then walked rapidly to the stairs, climbed and disappeared. nobody saw him again for the day, but they all knew he was going for a confrontation, if not a fight. it was going to get really ugly up there.\n"
     ]
    }
   ],
   "source": [
    "X = df['story']\n",
    "print(\"Length: \", len(X))\n",
    "print(\"Type: \", type(X))\n",
    "print(\"First Ten Values:\\n\", X[:10])\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  255\n",
      "Type:  <class 'pandas.core.series.Series'>\n",
      "First Ten Values:  0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "8    0\n",
      "9    2\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y = df['category']\n",
    "print(\"Length: \", len(y))\n",
    "print(\"Type: \", type(y))\n",
    "print(\"First Ten Values: \", y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=5, random_state=42, shuffle=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten_fold = KFold(n_splits=5, shuffle = True, random_state=42)\n",
    "ten_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold:  5\n",
      "X_train:  (204,) X_test:  (51,)\n",
      "X_train:  (204,) X_test:  (51,)\n",
      "X_train:  (204,) X_test:  (51,)\n",
      "X_train:  (204,) X_test:  (51,)\n",
      "X_train:  (204,) X_test:  (51,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Fold: \", ten_fold.get_n_splits(X))\n",
    "fold_no = 1\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"X_train: \", X_train.shape, \"X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "total_fold = ten_fold.get_n_splits(X)\n",
    "print(total_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-FOLD PV-DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold:  5\n",
      "Train Fold No.:  [  0   1   2   3   4   5   7   8  11  12  13  14  17  20  21  22  23  26\n",
      "  27  28  29  31  32  34  35  36  37  38  39  40  41  42  43  44  46  47\n",
      "  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65\n",
      "  69  70  71  72  73  74  76  77  78  80  81  82  83  84  85  87  88  89\n",
      "  90  91  92  93  94  95  98  99 100 101 102 103 104 105 106 107 108 109\n",
      " 110 111 112 113 114 115 116 118 119 121 122 123 125 126 127 128 129 130\n",
      " 131 132 133 134 135 138 139 140 141 142 143 144 145 146 147 149 150 151\n",
      " 153 154 155 156 157 158 159 160 161 162 163 165 166 168 169 170 171 172\n",
      " 173 174 175 178 179 182 183 186 187 188 189 191 192 193 194 196 197 199\n",
      " 202 203 204 207 208 210 211 212 213 214 215 217 218 219 220 223 224 225\n",
      " 226 227 228 230 231 232 233 235 236 237 239 240 241 242 244 245 246 247\n",
      " 248 249 250 251 253 254]  Test Fold No.:  [  6   9  10  15  16  18  19  24  25  30  33  45  66  67  68  75  79  86\n",
      "  96  97 117 120 124 136 137 148 152 164 167 176 177 180 181 184 185 190\n",
      " 195 198 200 201 205 206 209 216 221 222 229 234 238 243 252]\n",
      "No. of Training Dataset in Fold:  1  204\n",
      "No. of Testing Dataset in Fold:  1  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1423690.54it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 143370.98it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 702494.27it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 729444.17it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1164133.36it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1252764.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  1\n",
      "Testing accuracy 0.9019607843137255\n",
      "Testing F1 score: 0.9014005602240897\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  1  1]\n",
      " [ 0 16  1]\n",
      " [ 1  1 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86        11\n",
      "           1       0.89      0.94      0.91        17\n",
      "           2       0.91      0.91      0.91        23\n",
      "\n",
      "    accuracy                           0.90        51\n",
      "   macro avg       0.90      0.89      0.89        51\n",
      "weighted avg       0.90      0.90      0.90        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Fold No.:  [  0   1   2   3   4   6   7   8   9  10  11  13  14  15  16  17  18  19\n",
      "  20  21  23  24  25  26  27  30  32  33  34  36  37  39  40  41  43  44\n",
      "  45  46  47  48  49  50  51  52  53  54  57  58  59  61  62  63  64  66\n",
      "  67  68  70  71  72  74  75  76  77  78  79  80  81  83  86  87  88  89\n",
      "  91  92  94  95  96  97  98  99 100 102 103 105 106 107 110 116 117 118\n",
      " 119 120 121 122 123 124 128 129 130 131 133 134 135 136 137 138 140 142\n",
      " 145 146 147 148 149 150 151 152 153 155 156 157 159 160 161 163 164 165\n",
      " 166 167 168 169 171 173 174 175 176 177 179 180 181 182 183 184 185 187\n",
      " 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205\n",
      " 206 207 208 209 210 211 212 213 214 215 216 218 220 221 222 223 225 226\n",
      " 227 228 229 230 231 232 233 234 235 237 238 239 240 241 243 244 246 247\n",
      " 249 250 251 252 253 254]  Test Fold No.:  [  5  12  22  28  29  31  35  38  42  55  56  60  65  69  73  82  84  85\n",
      "  90  93 101 104 108 109 111 112 113 114 115 125 126 127 132 139 141 143\n",
      " 144 154 158 162 170 172 178 186 217 219 224 236 242 245 248]\n",
      "No. of Training Dataset in Fold:  2  204\n",
      "No. of Testing Dataset in Fold:  2  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1151598.94it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1485482.67it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 574639.37it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1290555.08it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1269492.61it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 729444.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  2\n",
      "Testing accuracy 0.8431372549019608\n",
      "Testing F1 score: 0.8443080732636227\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  1  1]\n",
      " [ 0 21  3]\n",
      " [ 2  1 12]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83        12\n",
      "           1       0.91      0.88      0.89        24\n",
      "           2       0.75      0.80      0.77        15\n",
      "\n",
      "    accuracy                           0.84        51\n",
      "   macro avg       0.83      0.84      0.83        51\n",
      "weighted avg       0.85      0.84      0.84        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Fold No.:  [  1   3   5   6   7   8   9  10  12  13  14  15  16  17  18  19  20  21\n",
      "  22  23  24  25  28  29  30  31  33  34  35  37  38  39  40  42  43  44\n",
      "  45  47  48  49  50  52  53  54  55  56  57  58  59  60  63  64  65  66\n",
      "  67  68  69  71  72  73  74  75  79  80  81  82  83  84  85  86  87  88\n",
      "  89  90  91  92  93  94  96  97  99 101 102 103 104 105 106 107 108 109\n",
      " 110 111 112 113 114 115 116 117 120 121 123 124 125 126 127 129 130 131\n",
      " 132 133 134 136 137 139 141 143 144 145 148 149 151 152 153 154 156 157\n",
      " 158 160 161 162 163 164 165 166 167 169 170 171 172 174 175 176 177 178\n",
      " 179 180 181 184 185 186 187 188 189 190 191 194 195 197 198 200 201 202\n",
      " 203 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221\n",
      " 222 224 225 226 227 228 229 232 233 234 235 236 237 238 240 242 243 244\n",
      " 245 247 248 251 252 254]  Test Fold No.:  [  0   2   4  11  26  27  32  36  41  46  51  61  62  70  76  77  78  95\n",
      "  98 100 118 119 122 128 135 138 140 142 146 147 150 155 159 168 173 182\n",
      " 183 192 193 196 199 204 223 230 231 239 241 246 249 250 253]\n",
      "No. of Training Dataset in Fold:  3  204\n",
      "No. of Testing Dataset in Fold:  3  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1122884.54it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1017405.49it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1247285.74it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 925014.07it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 825108.98it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 789333.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  3\n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.8631211857018308\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  0  2]\n",
      " [ 1 16  1]\n",
      " [ 0  3 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90        16\n",
      "           1       0.84      0.89      0.86        18\n",
      "           2       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.87      0.86      0.86        51\n",
      "weighted avg       0.86      0.86      0.86        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Fold No.:  [  0   1   2   4   5   6   9  10  11  12  14  15  16  18  19  20  21  22\n",
      "  24  25  26  27  28  29  30  31  32  33  35  36  37  38  41  42  45  46\n",
      "  48  50  51  52  54  55  56  57  58  60  61  62  63  65  66  67  68  69\n",
      "  70  71  73  74  75  76  77  78  79  82  84  85  86  87  88  90  92  93\n",
      "  95  96  97  98  99 100 101 102 103 104 106 107 108 109 111 112 113 114\n",
      " 115 116 117 118 119 120 121 122 124 125 126 127 128 129 130 132 135 136\n",
      " 137 138 139 140 141 142 143 144 146 147 148 149 150 151 152 154 155 157\n",
      " 158 159 160 162 164 167 168 169 170 172 173 174 176 177 178 179 180 181\n",
      " 182 183 184 185 186 187 188 189 190 191 192 193 195 196 198 199 200 201\n",
      " 202 203 204 205 206 207 209 210 211 212 214 216 217 218 219 221 222 223\n",
      " 224 226 229 230 231 234 235 236 238 239 240 241 242 243 245 246 247 248\n",
      " 249 250 251 252 253 254]  Test Fold No.:  [  3   7   8  13  17  23  34  39  40  43  44  47  49  53  59  64  72  80\n",
      "  81  83  89  91  94 105 110 123 131 133 134 145 153 156 161 163 165 166\n",
      " 171 175 194 197 208 213 215 220 225 227 228 232 233 237 244]\n",
      "No. of Training Dataset in Fold:  4  204\n",
      "No. of Testing Dataset in Fold:  4  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1341125.42it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1341125.42it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 178443.80it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 945456.37it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1288611.47it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1358155.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  4\n",
      "Testing accuracy 0.9215686274509803\n",
      "Testing F1 score: 0.9198978926382082\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[12  2  1]\n",
      " [ 1 15  0]\n",
      " [ 0  0 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.80      0.86        15\n",
      "           1       0.88      0.94      0.91        16\n",
      "           2       0.95      1.00      0.98        20\n",
      "\n",
      "    accuracy                           0.92        51\n",
      "   macro avg       0.92      0.91      0.91        51\n",
      "weighted avg       0.92      0.92      0.92        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Fold No.:  [  0   2   3   4   5   6   7   8   9  10  11  12  13  15  16  17  18  19\n",
      "  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  38  39  40\n",
      "  41  42  43  44  45  46  47  49  51  53  55  56  59  60  61  62  64  65\n",
      "  66  67  68  69  70  72  73  75  76  77  78  79  80  81  82  83  84  85\n",
      "  86  89  90  91  93  94  95  96  97  98 100 101 104 105 108 109 110 111\n",
      " 112 113 114 115 117 118 119 120 122 123 124 125 126 127 128 131 132 133\n",
      " 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 150 152 153\n",
      " 154 155 156 158 159 161 162 163 164 165 166 167 168 170 171 172 173 175\n",
      " 176 177 178 180 181 182 183 184 185 186 190 192 193 194 195 196 197 198\n",
      " 199 200 201 204 205 206 208 209 213 215 216 217 219 220 221 222 223 224\n",
      " 225 227 228 229 230 231 232 233 234 236 237 238 239 241 242 243 244 245\n",
      " 246 248 249 250 252 253]  Test Fold No.:  [  1  14  20  21  37  48  50  52  54  57  58  63  71  74  87  88  92  99\n",
      " 102 103 106 107 116 121 129 130 149 151 157 160 169 174 179 187 188 189\n",
      " 191 202 203 207 210 211 212 214 218 226 235 240 247 251 254]\n",
      "No. of Training Dataset in Fold:  5  204\n",
      "No. of Testing Dataset in Fold:  5  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1215394.91it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 555248.55it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 729444.17it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 900671.60it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1472698.82it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1034628.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  5\n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.8573270006311309\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  2  2]\n",
      " [ 0 21  1]\n",
      " [ 0  2 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75        10\n",
      "           1       0.84      0.95      0.89        22\n",
      "           2       0.85      0.89      0.87        19\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.90      0.82      0.84        51\n",
      "weighted avg       0.88      0.86      0.86        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Fold: \", total_fold)\n",
    "fold_no = 1\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "    print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold: \", fold_no, \"\", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold: \", fold_no, \"\", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg.fit(X_trained, y_trained)\n",
    "    y_pred = logreg.predict(X_tested)\n",
    "    \n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-FOLD PV-DM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold:  5\n",
      "No. of Training Dataset in Fold  1 :  204\n",
      "No. of Testing Dataset in Fold  1 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 951766.42it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 702494.27it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1130301.21it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1053741.40it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1284741.77it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 695640.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  1\n",
      "Testing accuracy 0.9019607843137255\n",
      "Testing F1 score: 0.901883306514095\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  0  2]\n",
      " [ 0 16  1]\n",
      " [ 0  2 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.82      0.90        11\n",
      "           1       0.89      0.94      0.91        17\n",
      "           2       0.88      0.91      0.89        23\n",
      "\n",
      "    accuracy                           0.90        51\n",
      "   macro avg       0.92      0.89      0.90        51\n",
      "weighted avg       0.91      0.90      0.90        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  204\n",
      "No. of Testing Dataset in Fold  2 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1118481.07it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1148507.40it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 913167.57it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 585251.72it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1108339.40it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1085835.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  2\n",
      "Testing accuracy 0.9215686274509803\n",
      "Testing F1 score: 0.9207545087000317\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  1  1]\n",
      " [ 0 24  0]\n",
      " [ 2  0 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83        12\n",
      "           1       0.96      1.00      0.98        24\n",
      "           2       0.93      0.87      0.90        15\n",
      "\n",
      "    accuracy                           0.92        51\n",
      "   macro avg       0.91      0.90      0.90        51\n",
      "weighted avg       0.92      0.92      0.92        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  204\n",
      "No. of Testing Dataset in Fold  3 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1019830.77it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 827502.92it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 793727.29it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1265736.71it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 842163.40it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1269492.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  3\n",
      "Testing accuracy 0.8431372549019608\n",
      "Testing F1 score: 0.844424256188962\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[12  1  3]\n",
      " [ 0 16  2]\n",
      " [ 0  2 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86        16\n",
      "           1       0.84      0.89      0.86        18\n",
      "           2       0.75      0.88      0.81        17\n",
      "\n",
      "    accuracy                           0.84        51\n",
      "   macro avg       0.86      0.84      0.84        51\n",
      "weighted avg       0.86      0.84      0.84        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  204\n",
      "No. of Testing Dataset in Fold  4 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 680698.50it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 561810.91it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 825905.42it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 775034.43it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 530463.74it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1495870.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  4\n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.8580065625209263\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  1  4]\n",
      " [ 1 15  0]\n",
      " [ 0  1 19]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.67      0.77        15\n",
      "           1       0.88      0.94      0.91        16\n",
      "           2       0.83      0.95      0.88        20\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.87      0.85      0.85        51\n",
      "weighted avg       0.87      0.86      0.86        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  204\n",
      "No. of Testing Dataset in Fold  5 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1402685.27it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1134798.43it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 707138.86it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1245470.18it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1193358.46it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 879381.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  5\n",
      "Testing accuracy 0.9411764705882353\n",
      "Testing F1 score: 0.938138239513883\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  0  3]\n",
      " [ 0 22  0]\n",
      " [ 0  0 19]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82        10\n",
      "           1       1.00      1.00      1.00        22\n",
      "           2       0.86      1.00      0.93        19\n",
      "\n",
      "    accuracy                           0.94        51\n",
      "   macro avg       0.95      0.90      0.92        51\n",
      "weighted avg       0.95      0.94      0.94        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold: \", total_fold)\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg.fit(X_trained, y_trained)\n",
    "    y_pred = logreg.predict(X_tested)\n",
    "    \n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n\\n\")\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAIRED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 5\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  1 :  204\n",
      "No. of Testing Dataset in Fold  1 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1561383.24it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1450233.93it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 614682.48it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 798915.05it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 587663.47it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 659197.24it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 696206.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.8626453038217745\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  1  1]\n",
      " [ 0 16  1]\n",
      " [ 1  3 19]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86        11\n",
      "           1       0.80      0.94      0.86        17\n",
      "           2       0.90      0.83      0.86        23\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.87      0.86      0.86        51\n",
      "weighted avg       0.87      0.86      0.86        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 704228.82it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 701342.64it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 795202.62it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 71149.01it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 51162.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8235294117647058\n",
      "Testing F1 score: 0.8159041394335511\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  1  4]\n",
      " [ 0 16  1]\n",
      " [ 1  2 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.55      0.67        11\n",
      "           1       0.84      0.94      0.89        17\n",
      "           2       0.80      0.87      0.83        23\n",
      "\n",
      "    accuracy                           0.82        51\n",
      "   macro avg       0.83      0.79      0.80        51\n",
      "weighted avg       0.83      0.82      0.82        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model: \n",
      "Testing accuracy 0.8431372549019608\n",
      "Testing F1 score: 0.841665194606371\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  1  2]\n",
      " [ 0 16  1]\n",
      " [ 1  3 19]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.73      0.80        11\n",
      "           1       0.80      0.94      0.86        17\n",
      "           2       0.86      0.83      0.84        23\n",
      "\n",
      "    accuracy                           0.84        51\n",
      "   macro avg       0.85      0.83      0.84        51\n",
      "weighted avg       0.85      0.84      0.84        51\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  204\n",
      "No. of Testing Dataset in Fold  2 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 365970.07it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1488066.11it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1164133.36it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1495870.66it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1220596.31it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1667910.36it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1186737.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.803921568627451\n",
      "Testing F1 score: 0.8080247814593166\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  1  1]\n",
      " [ 0 20  4]\n",
      " [ 4  0 11]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77        12\n",
      "           1       0.95      0.83      0.89        24\n",
      "           2       0.69      0.73      0.71        15\n",
      "\n",
      "    accuracy                           0.80        51\n",
      "   macro avg       0.78      0.80      0.79        51\n",
      "weighted avg       0.82      0.80      0.81        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 1343230.79it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1222340.02it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 715416.40it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1349586.78it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 683963.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8431372549019608\n",
      "Testing F1 score: 0.8455065359477125\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  1  1]\n",
      " [ 1 20  3]\n",
      " [ 2  0 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.80        12\n",
      "           1       0.95      0.83      0.89        24\n",
      "           2       0.76      0.87      0.81        15\n",
      "\n",
      "    accuracy                           0.84        51\n",
      "   macro avg       0.83      0.84      0.83        51\n",
      "weighted avg       0.85      0.84      0.85        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model: \n",
      "Testing accuracy 0.8823529411764706\n",
      "Testing F1 score: 0.8845170175093449\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  0  1]\n",
      " [ 0 22  2]\n",
      " [ 3  0 12]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.92      0.85        12\n",
      "           1       1.00      0.92      0.96        24\n",
      "           2       0.80      0.80      0.80        15\n",
      "\n",
      "    accuracy                           0.88        51\n",
      "   macro avg       0.86      0.88      0.87        51\n",
      "weighted avg       0.89      0.88      0.88        51\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  204\n",
      "No. of Testing Dataset in Fold  3 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1430832.80it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1433229.51it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1314344.11it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 818792.36it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1320429.04it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 943371.57it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 827502.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8431372549019608\n",
      "Testing F1 score: 0.8436974789915965\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  0  2]\n",
      " [ 1 15  2]\n",
      " [ 1  2 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88        16\n",
      "           1       0.88      0.83      0.86        18\n",
      "           2       0.78      0.82      0.80        17\n",
      "\n",
      "    accuracy                           0.84        51\n",
      "   macro avg       0.85      0.84      0.84        51\n",
      "weighted avg       0.85      0.84      0.84        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 1186737.89it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1275168.43it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 804927.58it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1104049.05it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1028411.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.803921568627451\n",
      "Testing F1 score: 0.8047368051163117\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  1  2]\n",
      " [ 1 14  3]\n",
      " [ 1  2 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84        16\n",
      "           1       0.82      0.78      0.80        18\n",
      "           2       0.74      0.82      0.78        17\n",
      "\n",
      "    accuracy                           0.80        51\n",
      "   macro avg       0.81      0.80      0.81        51\n",
      "weighted avg       0.81      0.80      0.80        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model: \n",
      "Testing accuracy 0.803921568627451\n",
      "Testing F1 score: 0.8041950869275348\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  1  2]\n",
      " [ 1 15  2]\n",
      " [ 1  3 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84        16\n",
      "           1       0.79      0.83      0.81        18\n",
      "           2       0.76      0.76      0.76        17\n",
      "\n",
      "    accuracy                           0.80        51\n",
      "   macro avg       0.81      0.80      0.80        51\n",
      "weighted avg       0.81      0.80      0.80        51\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  204\n",
      "No. of Testing Dataset in Fold  4 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 466796.52it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1326570.57it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 834768.80it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1104049.05it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 647719.92it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 747935.33it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 744680.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.8823529411764706\n",
      "Testing F1 score: 0.8796649973120562\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  1  3]\n",
      " [ 1 15  0]\n",
      " [ 0  1 19]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.73      0.81        15\n",
      "           1       0.88      0.94      0.91        16\n",
      "           2       0.86      0.95      0.90        20\n",
      "\n",
      "    accuracy                           0.88        51\n",
      "   macro avg       0.89      0.87      0.88        51\n",
      "weighted avg       0.89      0.88      0.88        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 575799.47it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1278980.59it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 872210.01it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 788606.47it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1292504.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.8615001556178027\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  2  2]\n",
      " [ 1 15  0]\n",
      " [ 0  2 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.73      0.81        15\n",
      "           1       0.79      0.94      0.86        16\n",
      "           2       0.90      0.90      0.90        20\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.87      0.86      0.86        51\n",
      "weighted avg       0.87      0.86      0.86        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model: \n",
      "Testing accuracy 0.9019607843137255\n",
      "Testing F1 score: 0.8985661448041748\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  1  3]\n",
      " [ 1 15  0]\n",
      " [ 0  0 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.73      0.81        15\n",
      "           1       0.94      0.94      0.94        16\n",
      "           2       0.87      1.00      0.93        20\n",
      "\n",
      "    accuracy                           0.90        51\n",
      "   macro avg       0.91      0.89      0.89        51\n",
      "weighted avg       0.90      0.90      0.90        51\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  204\n",
      "No. of Testing Dataset in Fold  5 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 517314.40it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 870435.42it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1292504.56it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 825905.42it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 951766.42it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 707723.75it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 829106.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8431372549019608\n",
      "Testing F1 score: 0.8417211328976034\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  2  1]\n",
      " [ 0 19  3]\n",
      " [ 1  1 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.70      0.78        10\n",
      "           1       0.86      0.86      0.86        22\n",
      "           2       0.81      0.89      0.85        19\n",
      "\n",
      "    accuracy                           0.84        51\n",
      "   macro avg       0.85      0.82      0.83        51\n",
      "weighted avg       0.85      0.84      0.84        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 166726.04it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 807967.91it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 852229.10it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 168831.49it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 571186.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.9019607843137255\n",
      "Testing F1 score: 0.8998213056312356\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  1  2]\n",
      " [ 0 20  2]\n",
      " [ 0  0 19]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82        10\n",
      "           1       0.95      0.91      0.93        22\n",
      "           2       0.83      1.00      0.90        19\n",
      "\n",
      "    accuracy                           0.90        51\n",
      "   macro avg       0.93      0.87      0.89        51\n",
      "weighted avg       0.91      0.90      0.90        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model: \n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.8620170366368161\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  1  2]\n",
      " [ 0 19  3]\n",
      " [ 0  1 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82        10\n",
      "           1       0.90      0.86      0.88        22\n",
      "           2       0.78      0.95      0.86        19\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.90      0.84      0.85        51\n",
      "weighted avg       0.88      0.86      0.86        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: {}\\n\\n\" .format(total_fold))\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    logreg_dbow = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg_dbow.fit(X_trained, y_trained)\n",
    "    y_pred = logreg_dbow.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    logreg_dm = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg_dm.fit(X_trained, y_trained)\n",
    "    y_pred = logreg_dm.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    logreg_paired = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg_paired.fit(X_train, y_train)\n",
    "    y_pred = logreg_paired.predict(X_test)\n",
    "    \n",
    "    print(\"FOR Paired Model: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Using Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 5\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  1 :  204\n",
      "No. of Testing Dataset in Fold  1 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1353857.62it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1530658.35it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 634746.30it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1322469.89it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 695075.56it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 952826.30it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1304326.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Entropy: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8823529411764706\n",
      "Testing F1 score: 0.882103952692188\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  1  1]\n",
      " [ 0 16  1]\n",
      " [ 2  1 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82        11\n",
      "           1       0.89      0.94      0.91        17\n",
      "           2       0.91      0.87      0.89        23\n",
      "\n",
      "    accuracy                           0.88        51\n",
      "   macro avg       0.87      0.88      0.87        51\n",
      "weighted avg       0.88      0.88      0.88        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 1386771.50it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 883011.37it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 734453.23it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1222340.02it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 804171.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Entropy:  \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.9215686274509803\n",
      "Testing F1 score: 0.9204481792717086\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  0  2]\n",
      " [ 0 17  0]\n",
      " [ 1  1 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86        11\n",
      "           1       0.94      1.00      0.97        17\n",
      "           2       0.91      0.91      0.91        23\n",
      "\n",
      "    accuracy                           0.92        51\n",
      "   macro avg       0.92      0.91      0.91        51\n",
      "weighted avg       0.92      0.92      0.92        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Entropy: \n",
      "Testing accuracy 0.9215686274509803\n",
      "Testing F1 score: 0.9204481792717086\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  0  2]\n",
      " [ 0 17  0]\n",
      " [ 1  1 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86        11\n",
      "           1       0.94      1.00      0.97        17\n",
      "           2       0.91      0.91      0.91        23\n",
      "\n",
      "    accuracy                           0.92        51\n",
      "   macro avg       0.92      0.91      0.91        51\n",
      "weighted avg       0.92      0.92      0.92        51\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  204\n",
      "No. of Testing Dataset in Fold  2 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1467646.68it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1334848.70it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1498490.40it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1312328.25it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1198372.57it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 825108.98it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 781404.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Entropy: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8431372549019608\n",
      "Testing F1 score: 0.8436394678130271\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  2  1]\n",
      " [ 0 21  3]\n",
      " [ 2  0 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78        12\n",
      "           1       0.91      0.88      0.89        24\n",
      "           2       0.76      0.87      0.81        15\n",
      "\n",
      "    accuracy                           0.84        51\n",
      "   macro avg       0.83      0.83      0.83        51\n",
      "weighted avg       0.85      0.84      0.84        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 1386771.50it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 754530.88it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 980112.27it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 860802.83it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 853930.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Entropy:  \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8235294117647058\n",
      "Testing F1 score: 0.8288755053213431\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  0  1]\n",
      " [ 2 22  0]\n",
      " [ 6  0  9]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.92      0.71        12\n",
      "           1       1.00      0.92      0.96        24\n",
      "           2       0.90      0.60      0.72        15\n",
      "\n",
      "    accuracy                           0.82        51\n",
      "   macro avg       0.83      0.81      0.80        51\n",
      "weighted avg       0.87      0.82      0.83        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Entropy: \n",
      "Testing accuracy 0.8235294117647058\n",
      "Testing F1 score: 0.8288755053213431\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  0  1]\n",
      " [ 2 22  0]\n",
      " [ 6  0  9]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.92      0.71        12\n",
      "           1       1.00      0.92      0.96        24\n",
      "           2       0.90      0.60      0.72        15\n",
      "\n",
      "    accuracy                           0.82        51\n",
      "   macro avg       0.83      0.81      0.80        51\n",
      "weighted avg       0.87      0.82      0.83        51\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  204\n",
      "No. of Testing Dataset in Fold  3 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1398101.33it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1042190.03it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 656164.12it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 818792.36it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 526870.70it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 177887.32it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 754530.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Entropy: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8431372549019608\n",
      "Testing F1 score: 0.8425951025571519\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  0  1]\n",
      " [ 3 15  0]\n",
      " [ 1  3 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86        16\n",
      "           1       0.83      0.83      0.83        18\n",
      "           2       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.84        51\n",
      "   macro avg       0.85      0.85      0.84        51\n",
      "weighted avg       0.85      0.84      0.84        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 1400389.55it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1231133.84it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 716015.08it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1258291.20it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 673730.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Entropy:  \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.803921568627451\n",
      "Testing F1 score: 0.8067538126361656\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  0  3]\n",
      " [ 0 15  3]\n",
      " [ 1  3 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.87        16\n",
      "           1       0.83      0.83      0.83        18\n",
      "           2       0.68      0.76      0.72        17\n",
      "\n",
      "    accuracy                           0.80        51\n",
      "   macro avg       0.82      0.80      0.81        51\n",
      "weighted avg       0.81      0.80      0.81        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Entropy: \n",
      "Testing accuracy 0.803921568627451\n",
      "Testing F1 score: 0.8067538126361656\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  0  3]\n",
      " [ 0 15  3]\n",
      " [ 1  3 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.87        16\n",
      "           1       0.83      0.83      0.83        18\n",
      "           2       0.68      0.76      0.72        17\n",
      "\n",
      "    accuracy                           0.80        51\n",
      "   macro avg       0.82      0.80      0.81        51\n",
      "weighted avg       0.81      0.80      0.81        51\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  204\n",
      "No. of Testing Dataset in Fold  4 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 731940.13it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1240055.10it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1092768.86it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 763281.01it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 555248.55it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1081716.83it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 799661.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Entropy: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.8823529411764706\n",
      "Testing F1 score: 0.8812549776099488\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  0  4]\n",
      " [ 1 15  0]\n",
      " [ 1  0 19]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.73      0.79        15\n",
      "           1       1.00      0.94      0.97        16\n",
      "           2       0.83      0.95      0.88        20\n",
      "\n",
      "    accuracy                           0.88        51\n",
      "   macro avg       0.89      0.87      0.88        51\n",
      "weighted avg       0.89      0.88      0.88        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 978990.86it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 852229.10it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 540175.52it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 711253.55it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 797425.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Entropy:  \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.8533783965022398\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  3  3]\n",
      " [ 1 15  0]\n",
      " [ 0  0 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.60      0.72        15\n",
      "           1       0.83      0.94      0.88        16\n",
      "           2       0.87      1.00      0.93        20\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.87      0.85      0.84        51\n",
      "weighted avg       0.87      0.86      0.85        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Entropy: \n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.8533783965022398\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  3  3]\n",
      " [ 1 15  0]\n",
      " [ 0  0 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.60      0.72        15\n",
      "           1       0.83      0.94      0.88        16\n",
      "           2       0.87      1.00      0.93        20\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.87      0.85      0.84        51\n",
      "weighted avg       0.87      0.86      0.85        51\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  204\n",
      "No. of Testing Dataset in Fold  5 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1019830.77it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 895018.85it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1371214.77it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1326570.57it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 713031.68it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 847166.35it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 615124.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Entropy: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.8613692443991826\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  2  1]\n",
      " [ 0 20  2]\n",
      " [ 0  2 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82        10\n",
      "           1       0.83      0.91      0.87        22\n",
      "           2       0.85      0.89      0.87        19\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.89      0.83      0.85        51\n",
      "weighted avg       0.87      0.86      0.86        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 852229.10it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1164133.36it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 773632.93it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1298388.49it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 936146.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Entropy:  \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.8619893764209741\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  1  2]\n",
      " [ 0 18  4]\n",
      " [ 0  0 19]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82        10\n",
      "           1       0.95      0.82      0.88        22\n",
      "           2       0.76      1.00      0.86        19\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.90      0.84      0.86        51\n",
      "weighted avg       0.89      0.86      0.86        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Entropy: \n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.8619893764209741\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  1  2]\n",
      " [ 0 18  4]\n",
      " [ 0  0 19]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82        10\n",
      "           1       0.95      0.82      0.88        22\n",
      "           2       0.76      1.00      0.86        19\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.90      0.84      0.86        51\n",
      "weighted avg       0.89      0.86      0.86        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: {}\\n\\n\" .format(total_fold))\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"entropy\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using Decision Tree with Entropy: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"entropy\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using Decision Tree with Entropy:  \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"entropy\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR Paired Model Using Decision Tree with Entropy: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Using Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 5\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  1 :  204\n",
      "No. of Testing Dataset in Fold  1 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 658689.77it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 735084.21it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1070886.13it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1181820.46it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1343230.79it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 565898.16it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1069547.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Gini: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.8615001556178027\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  2  1]\n",
      " [ 0 16  1]\n",
      " [ 2  1 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.73      0.76        11\n",
      "           1       0.84      0.94      0.89        17\n",
      "           2       0.91      0.87      0.89        23\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.85      0.85      0.85        51\n",
      "weighted avg       0.86      0.86      0.86        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 1525201.45it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 811031.29it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 862538.32it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 674793.39it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 786432.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Gini:  \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.9019607843137255\n",
      "Testing F1 score: 0.9021475256769375\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  1  1]\n",
      " [ 1 16  0]\n",
      " [ 1  1 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82        11\n",
      "           1       0.89      0.94      0.91        17\n",
      "           2       0.95      0.91      0.93        23\n",
      "\n",
      "    accuracy                           0.90        51\n",
      "   macro avg       0.89      0.89      0.89        51\n",
      "weighted avg       0.90      0.90      0.90        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Gini: \n",
      "Testing accuracy 0.9019607843137255\n",
      "Testing F1 score: 0.9021475256769375\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  1  1]\n",
      " [ 1 16  0]\n",
      " [ 1  1 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82        11\n",
      "           1       0.89      0.94      0.91        17\n",
      "           2       0.95      0.91      0.93        23\n",
      "\n",
      "    accuracy                           0.90        51\n",
      "   macro avg       0.89      0.89      0.89        51\n",
      "weighted avg       0.90      0.90      0.90        51\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  204\n",
      "No. of Testing Dataset in Fold  2 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1249106.59it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1258291.20it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 648210.62it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 295863.77it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1089984.73it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 827502.92it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 747935.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Gini: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.7843137254901961\n",
      "Testing F1 score: 0.78758345048447\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  1  0]\n",
      " [ 1 18  5]\n",
      " [ 4  0 11]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.92      0.79        12\n",
      "           1       0.95      0.75      0.84        24\n",
      "           2       0.69      0.73      0.71        15\n",
      "\n",
      "    accuracy                           0.78        51\n",
      "   macro avg       0.77      0.80      0.78        51\n",
      "weighted avg       0.81      0.78      0.79        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 1402685.27it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 577353.59it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1133295.39it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 680698.50it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1154707.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Gini:  \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8235294117647058\n",
      "Testing F1 score: 0.8282572722800426\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  0  1]\n",
      " [ 0 20  4]\n",
      " [ 4  0 11]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.92      0.81        12\n",
      "           1       1.00      0.83      0.91        24\n",
      "           2       0.69      0.73      0.71        15\n",
      "\n",
      "    accuracy                           0.82        51\n",
      "   macro avg       0.81      0.83      0.81        51\n",
      "weighted avg       0.85      0.82      0.83        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Gini: \n",
      "Testing accuracy 0.8235294117647058\n",
      "Testing F1 score: 0.8282572722800426\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  0  1]\n",
      " [ 0 20  4]\n",
      " [ 4  0 11]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.92      0.81        12\n",
      "           1       1.00      0.83      0.91        24\n",
      "           2       0.69      0.73      0.71        15\n",
      "\n",
      "    accuracy                           0.82        51\n",
      "   macro avg       0.81      0.83      0.81        51\n",
      "weighted avg       0.85      0.82      0.83        51\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  204\n",
      "No. of Testing Dataset in Fold  3 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1472698.82it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 707138.86it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 549190.00it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 944412.82it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 820362.43it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1211951.86it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1356003.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Gini: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.861284747314159\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  0  1]\n",
      " [ 1 16  1]\n",
      " [ 1  3 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91        16\n",
      "           1       0.84      0.89      0.86        18\n",
      "           2       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.86      0.86      0.86        51\n",
      "weighted avg       0.86      0.86      0.86        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 573099.81it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 489775.62it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 818009.58it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1430832.80it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 648210.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Gini:  \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.803921568627451\n",
      "Testing F1 score: 0.8002285296402943\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 1 16  1]\n",
      " [ 2  4 11]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85        16\n",
      "           1       0.76      0.89      0.82        18\n",
      "           2       0.85      0.65      0.73        17\n",
      "\n",
      "    accuracy                           0.80        51\n",
      "   macro avg       0.81      0.80      0.80        51\n",
      "weighted avg       0.81      0.80      0.80        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Gini: \n",
      "Testing accuracy 0.803921568627451\n",
      "Testing F1 score: 0.8002285296402943\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 1 16  1]\n",
      " [ 2  4 11]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85        16\n",
      "           1       0.76      0.89      0.82        18\n",
      "           2       0.85      0.65      0.73        17\n",
      "\n",
      "    accuracy                           0.80        51\n",
      "   macro avg       0.81      0.80      0.80        51\n",
      "weighted avg       0.81      0.80      0.80        51\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  204\n",
      "No. of Testing Dataset in Fold  4 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 651170.48it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 772236.48it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 953888.54it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 757872.47it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1290555.08it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1175326.95it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1258291.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Gini: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9019607843137255\n",
      "Testing F1 score: 0.8997629680741634\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  0  4]\n",
      " [ 1 15  0]\n",
      " [ 0  0 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.73      0.81        15\n",
      "           1       1.00      0.94      0.97        16\n",
      "           2       0.83      1.00      0.91        20\n",
      "\n",
      "    accuracy                           0.90        51\n",
      "   macro avg       0.92      0.89      0.90        51\n",
      "weighted avg       0.91      0.90      0.90        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 798915.05it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 522367.53it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1081716.83it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1156267.59it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1165719.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Gini:  \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.8235294117647058\n",
      "Testing F1 score: 0.799083269671505\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  1  8]\n",
      " [ 0 16  0]\n",
      " [ 0  0 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.40      0.57        15\n",
      "           1       0.94      1.00      0.97        16\n",
      "           2       0.71      1.00      0.83        20\n",
      "\n",
      "    accuracy                           0.82        51\n",
      "   macro avg       0.89      0.80      0.79        51\n",
      "weighted avg       0.87      0.82      0.80        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Gini: \n",
      "Testing accuracy 0.8235294117647058\n",
      "Testing F1 score: 0.799083269671505\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  1  8]\n",
      " [ 0 16  0]\n",
      " [ 0  0 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.40      0.57        15\n",
      "           1       0.94      1.00      0.97        16\n",
      "           2       0.71      1.00      0.83        20\n",
      "\n",
      "    accuracy                           0.82        51\n",
      "   macro avg       0.89      0.80      0.79        51\n",
      "weighted avg       0.87      0.82      0.80        51\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  204\n",
      "No. of Testing Dataset in Fold  5 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 532775.85it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1277071.67it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 685058.46it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1217123.78it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 728202.57it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 542229.41it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1159401.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Gini: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8431372549019608\n",
      "Testing F1 score: 0.8388320545609548\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  2  2]\n",
      " [ 0 20  2]\n",
      " [ 0  2 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75        10\n",
      "           1       0.83      0.91      0.87        22\n",
      "           2       0.81      0.89      0.85        19\n",
      "\n",
      "    accuracy                           0.84        51\n",
      "   macro avg       0.88      0.80      0.82        51\n",
      "weighted avg       0.86      0.84      0.84        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 1164133.36it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 977872.02it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 472467.15it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 681240.46it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 913167.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Gini:  \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8431372549019608\n",
      "Testing F1 score: 0.8413429556675046\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  1  2]\n",
      " [ 2 18  2]\n",
      " [ 0  1 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.70      0.74        10\n",
      "           1       0.90      0.82      0.86        22\n",
      "           2       0.82      0.95      0.88        19\n",
      "\n",
      "    accuracy                           0.84        51\n",
      "   macro avg       0.83      0.82      0.82        51\n",
      "weighted avg       0.85      0.84      0.84        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Gini: \n",
      "Testing accuracy 0.8431372549019608\n",
      "Testing F1 score: 0.8413429556675046\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  1  2]\n",
      " [ 2 18  2]\n",
      " [ 0  1 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.70      0.74        10\n",
      "           1       0.90      0.82      0.86        22\n",
      "           2       0.82      0.95      0.88        19\n",
      "\n",
      "    accuracy                           0.84        51\n",
      "   macro avg       0.83      0.82      0.82        51\n",
      "weighted avg       0.85      0.84      0.84        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: {}\\n\\n\" .format(total_fold))\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using Decision Tree with Gini: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using Decision Tree with Gini:  \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR Paired Model Using Decision Tree with Gini: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbour with Minkowski Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 5\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  1 :  204\n",
      "No. of Testing Dataset in Fold  1 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1162551.65it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1168904.39it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1181820.46it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 472206.41it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1260144.35it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1111218.20it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 571186.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Minkowski Distance: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.9019607843137255\n",
      "Testing F1 score: 0.9014005602240897\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  1  1]\n",
      " [ 0 16  1]\n",
      " [ 1  1 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86        11\n",
      "           1       0.89      0.94      0.91        17\n",
      "           2       0.91      0.91      0.91        23\n",
      "\n",
      "    accuracy                           0.90        51\n",
      "   macro avg       0.90      0.89      0.89        51\n",
      "weighted avg       0.90      0.90      0.90        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 1092768.86it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 768077.21it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1083086.10it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1275168.43it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1183455.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Minkowski Distance: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.9019607843137255\n",
      "Testing F1 score: 0.9012675777381659\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  1  1]\n",
      " [ 0 17  0]\n",
      " [ 2  1 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82        11\n",
      "           1       0.89      1.00      0.94        17\n",
      "           2       0.95      0.87      0.91        23\n",
      "\n",
      "    accuracy                           0.90        51\n",
      "   macro avg       0.89      0.90      0.89        51\n",
      "weighted avg       0.90      0.90      0.90        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Minkowski Distance: \n",
      "Testing accuracy 0.9019607843137255\n",
      "Testing F1 score: 0.9012675777381659\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  1  1]\n",
      " [ 0 17  0]\n",
      " [ 2  1 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82        11\n",
      "           1       0.89      1.00      0.94        17\n",
      "           2       0.95      0.87      0.91        23\n",
      "\n",
      "    accuracy                           0.90        51\n",
      "   macro avg       0.89      0.90      0.89        51\n",
      "weighted avg       0.90      0.90      0.90        51\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  204\n",
      "No. of Testing Dataset in Fold  2 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 665866.16it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 616010.09it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 463760.44it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 520144.69it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1108339.40it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 821938.54it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 650675.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Minkowski Distance: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8823529411764706\n",
      "Testing F1 score: 0.8852450980392156\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  0  1]\n",
      " [ 0 21  3]\n",
      " [ 2  0 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88        12\n",
      "           1       1.00      0.88      0.93        24\n",
      "           2       0.76      0.87      0.81        15\n",
      "\n",
      "    accuracy                           0.88        51\n",
      "   macro avg       0.87      0.89      0.88        51\n",
      "weighted avg       0.89      0.88      0.89        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 1092768.86it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 779269.60it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 844657.47it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 798169.79it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 804927.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Minkowski Distance: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.8649538713843232\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  1  1]\n",
      " [ 1 22  1]\n",
      " [ 3  0 12]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77        12\n",
      "           1       0.96      0.92      0.94        24\n",
      "           2       0.86      0.80      0.83        15\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.84      0.85      0.84        51\n",
      "weighted avg       0.87      0.86      0.86        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Minkowski Distance: \n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.8649538713843232\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  1  1]\n",
      " [ 1 22  1]\n",
      " [ 3  0 12]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77        12\n",
      "           1       0.96      0.92      0.94        24\n",
      "           2       0.86      0.80      0.83        15\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.84      0.85      0.84        51\n",
      "weighted avg       0.87      0.86      0.86        51\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  204\n",
      "No. of Testing Dataset in Fold  3 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1428444.10it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1089984.73it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 175227.94it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 909285.88it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 870435.42it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 614241.22it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 833955.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Minkowski Distance: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8431372549019608\n",
      "Testing F1 score: 0.8425567595459237\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 1 16  1]\n",
      " [ 1  3 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88        16\n",
      "           1       0.80      0.89      0.84        18\n",
      "           2       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.84        51\n",
      "   macro avg       0.85      0.84      0.84        51\n",
      "weighted avg       0.85      0.84      0.84        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 1240055.10it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 704808.91it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 747282.11it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1404988.53it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 820362.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Minkowski Distance: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8235294117647058\n",
      "Testing F1 score: 0.8221670802315963\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 1 16  1]\n",
      " [ 1  4 12]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88        16\n",
      "           1       0.76      0.89      0.82        18\n",
      "           2       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.82        51\n",
      "   macro avg       0.83      0.82      0.82        51\n",
      "weighted avg       0.83      0.82      0.82        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Minkowski Distance: \n",
      "Testing accuracy 0.8235294117647058\n",
      "Testing F1 score: 0.8221670802315963\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 1 16  1]\n",
      " [ 1  4 12]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88        16\n",
      "           1       0.76      0.89      0.82        18\n",
      "           2       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.82        51\n",
      "   macro avg       0.83      0.82      0.82        51\n",
      "weighted avg       0.83      0.82      0.82        51\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  204\n",
      "No. of Testing Dataset in Fold  4 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 609428.79it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 612921.21it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1292504.56it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 848005.96it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 807967.91it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 722667.24it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 651170.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Minkowski Distance: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9803921568627451\n",
      "Testing F1 score: 0.9803511791341407\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  0]\n",
      " [ 0 16  0]\n",
      " [ 0  0 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.97        15\n",
      "           1       0.94      1.00      0.97        16\n",
      "           2       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           0.98        51\n",
      "   macro avg       0.98      0.98      0.98        51\n",
      "weighted avg       0.98      0.98      0.98        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 816448.49it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1358155.58it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1330696.76it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1115564.56it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 824314.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Minkowski Distance: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9019607843137255\n",
      "Testing F1 score: 0.8967532812583027\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  4  1]\n",
      " [ 0 16  0]\n",
      " [ 0  0 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80        15\n",
      "           1       0.80      1.00      0.89        16\n",
      "           2       0.95      1.00      0.98        20\n",
      "\n",
      "    accuracy                           0.90        51\n",
      "   macro avg       0.92      0.89      0.89        51\n",
      "weighted avg       0.92      0.90      0.90        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Minkowski Distance: \n",
      "Testing accuracy 0.9019607843137255\n",
      "Testing F1 score: 0.8967532812583027\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  4  1]\n",
      " [ 0 16  0]\n",
      " [ 0  0 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80        15\n",
      "           1       0.80      1.00      0.89        16\n",
      "           2       0.95      1.00      0.98        20\n",
      "\n",
      "    accuracy                           0.90        51\n",
      "   macro avg       0.92      0.89      0.89        51\n",
      "weighted avg       0.92      0.90      0.90        51\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  204\n",
      "No. of Testing Dataset in Fold  5 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 492028.76it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1488066.11it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 677464.78it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 792257.42it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 638535.83it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 777852.74it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1017405.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Minkowski Distance: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.8615852877098553\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  1  2]\n",
      " [ 0 20  2]\n",
      " [ 0  2 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82        10\n",
      "           1       0.87      0.91      0.89        22\n",
      "           2       0.81      0.89      0.85        19\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.89      0.83      0.85        51\n",
      "weighted avg       0.87      0.86      0.86        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 663285.28it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1114112.00it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 793727.29it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 192105.53it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 802662.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Minkowski Distance: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.9411764705882353\n",
      "Testing F1 score: 0.9399698340874811\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  0  2]\n",
      " [ 0 22  0]\n",
      " [ 0  1 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89        10\n",
      "           1       0.96      1.00      0.98        22\n",
      "           2       0.90      0.95      0.92        19\n",
      "\n",
      "    accuracy                           0.94        51\n",
      "   macro avg       0.95      0.92      0.93        51\n",
      "weighted avg       0.94      0.94      0.94        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Minkowski Distance: \n",
      "Testing accuracy 0.9411764705882353\n",
      "Testing F1 score: 0.9399698340874811\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  0  2]\n",
      " [ 0 22  0]\n",
      " [ 0  1 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89        10\n",
      "           1       0.96      1.00      0.98        22\n",
      "           2       0.90      0.95      0.92        19\n",
      "\n",
      "    accuracy                           0.94        51\n",
      "   macro avg       0.95      0.92      0.93        51\n",
      "weighted avg       0.94      0.94      0.94        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: {}\\n\\n\" .format(total_fold))\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "        \n",
    "    knn_dbow = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski')\n",
    "    knn_dbow.fit(X_trained, y_trained)\n",
    "    y_pred = knn_dbow.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using KNN with Minkowski Distance: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    knn_dm = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski')\n",
    "    knn_dm.fit(X_trained, y_trained)\n",
    "    y_pred = knn_dm.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using KNN with Minkowski Distance: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    knn_mixed = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski')\n",
    "    knn_mixed.fit(X_trained, y_trained)\n",
    "    y_pred = knn_mixed.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PAIRED Model Using KNN with Minkowski Distance: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbour with Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 5\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  1 :  204\n",
      "No. of Testing Dataset in Fold  1 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 681240.46it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1467646.68it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 639012.71it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 563661.41it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 701342.64it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 641888.98it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 818792.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Euclidean Distance: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8823529411764706\n",
      "Testing F1 score: 0.8820417055711174\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  1  1]\n",
      " [ 0 16  1]\n",
      " [ 1  2 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86        11\n",
      "           1       0.84      0.94      0.89        17\n",
      "           2       0.91      0.87      0.89        23\n",
      "\n",
      "    accuracy                           0.88        51\n",
      "   macro avg       0.88      0.88      0.88        51\n",
      "weighted avg       0.88      0.88      0.88        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 1300361.73it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1332769.50it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1393547.26it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1438047.09it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1304326.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Euclidean Distance: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.9411764705882353\n",
      "Testing F1 score: 0.9400560224089636\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  1  1]\n",
      " [ 0 17  0]\n",
      " [ 1  0 22]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86        11\n",
      "           1       0.94      1.00      0.97        17\n",
      "           2       0.96      0.96      0.96        23\n",
      "\n",
      "    accuracy                           0.94        51\n",
      "   macro avg       0.93      0.92      0.93        51\n",
      "weighted avg       0.94      0.94      0.94        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Euclidean Distance: \n",
      "Testing accuracy 0.9411764705882353\n",
      "Testing F1 score: 0.9400560224089636\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  1  1]\n",
      " [ 0 17  0]\n",
      " [ 1  0 22]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86        11\n",
      "           1       0.94      1.00      0.97        17\n",
      "           2       0.96      0.96      0.96        23\n",
      "\n",
      "    accuracy                           0.94        51\n",
      "   macro avg       0.93      0.92      0.93        51\n",
      "weighted avg       0.94      0.94      0.94        51\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  204\n",
      "No. of Testing Dataset in Fold  2 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1343230.79it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1371214.77it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 684510.41it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 867787.03it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 495160.89it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1351718.82it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 787155.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Euclidean Distance: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8823529411764706\n",
      "Testing F1 score: 0.8852450980392156\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  0  1]\n",
      " [ 0 21  3]\n",
      " [ 2  0 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88        12\n",
      "           1       1.00      0.88      0.93        24\n",
      "           2       0.76      0.87      0.81        15\n",
      "\n",
      "    accuracy                           0.88        51\n",
      "   macro avg       0.87      0.89      0.88        51\n",
      "weighted avg       0.89      0.88      0.89        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 1117020.91it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 853078.78it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1320429.04it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 726348.06it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 753202.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Euclidean Distance: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8235294117647058\n",
      "Testing F1 score: 0.8293911970382558\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  0  2]\n",
      " [ 1 20  3]\n",
      " [ 3  0 12]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77        12\n",
      "           1       1.00      0.83      0.91        24\n",
      "           2       0.71      0.80      0.75        15\n",
      "\n",
      "    accuracy                           0.82        51\n",
      "   macro avg       0.81      0.82      0.81        51\n",
      "weighted avg       0.85      0.82      0.83        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Euclidean Distance: \n",
      "Testing accuracy 0.8235294117647058\n",
      "Testing F1 score: 0.8293911970382558\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  0  2]\n",
      " [ 1 20  3]\n",
      " [ 3  0 12]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77        12\n",
      "           1       1.00      0.83      0.91        24\n",
      "           2       0.71      0.80      0.75        15\n",
      "\n",
      "    accuracy                           0.82        51\n",
      "   macro avg       0.81      0.82      0.81        51\n",
      "weighted avg       0.85      0.82      0.83        51\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  204\n",
      "No. of Testing Dataset in Fold  3 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 711253.55it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1428444.10it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 801159.19it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 710073.04it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 989176.90it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 720840.79it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 645277.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Euclidean Distance: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8823529411764706\n",
      "Testing F1 score: 0.8807404540763675\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  0  1]\n",
      " [ 0 17  1]\n",
      " [ 1  3 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        16\n",
      "           1       0.85      0.94      0.89        18\n",
      "           2       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.88        51\n",
      "   macro avg       0.88      0.88      0.88        51\n",
      "weighted avg       0.88      0.88      0.88        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 692824.30it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1398101.33it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 840508.86it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1593366.88it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 182283.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Euclidean Distance: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.7843137254901961\n",
      "Testing F1 score: 0.7816571790006325\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 1 15  2]\n",
      " [ 4  2 11]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.88      0.80        16\n",
      "           1       0.83      0.83      0.83        18\n",
      "           2       0.79      0.65      0.71        17\n",
      "\n",
      "    accuracy                           0.78        51\n",
      "   macro avg       0.79      0.79      0.78        51\n",
      "weighted avg       0.79      0.78      0.78        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Euclidean Distance: \n",
      "Testing accuracy 0.7843137254901961\n",
      "Testing F1 score: 0.7816571790006325\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 1 15  2]\n",
      " [ 4  2 11]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.88      0.80        16\n",
      "           1       0.83      0.83      0.83        18\n",
      "           2       0.79      0.65      0.71        17\n",
      "\n",
      "    accuracy                           0.78        51\n",
      "   macro avg       0.79      0.79      0.78        51\n",
      "weighted avg       0.79      0.78      0.78        51\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  204\n",
      "No. of Testing Dataset in Fold  4 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1006632.96it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1290555.08it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 842163.40it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1343230.79it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 751218.63it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1369020.83it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 672671.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Euclidean Distance: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9411764705882353\n",
      "Testing F1 score: 0.9418163535810595\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  0]\n",
      " [ 1 15  0]\n",
      " [ 0  1 19]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        15\n",
      "           1       0.88      0.94      0.91        16\n",
      "           2       1.00      0.95      0.97        20\n",
      "\n",
      "    accuracy                           0.94        51\n",
      "   macro avg       0.94      0.94      0.94        51\n",
      "weighted avg       0.94      0.94      0.94        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 1040922.16it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 717215.44it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 657677.18it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 925014.07it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 515755.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Euclidean Distance: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9215686274509803\n",
      "Testing F1 score: 0.9197012138188607\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[12  1  2]\n",
      " [ 1 15  0]\n",
      " [ 0  0 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.80      0.86        15\n",
      "           1       0.94      0.94      0.94        16\n",
      "           2       0.91      1.00      0.95        20\n",
      "\n",
      "    accuracy                           0.92        51\n",
      "   macro avg       0.92      0.91      0.92        51\n",
      "weighted avg       0.92      0.92      0.92        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Euclidean Distance: \n",
      "Testing accuracy 0.9215686274509803\n",
      "Testing F1 score: 0.9197012138188607\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[12  1  2]\n",
      " [ 1 15  0]\n",
      " [ 0  0 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.80      0.86        15\n",
      "           1       0.94      0.94      0.94        16\n",
      "           2       0.91      1.00      0.95        20\n",
      "\n",
      "    accuracy                           0.92        51\n",
      "   macro avg       0.92      0.91      0.92        51\n",
      "weighted avg       0.92      0.92      0.92        51\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  204\n",
      "No. of Testing Dataset in Fold  5 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1018616.69it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 619578.58it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1001918.05it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 529806.82it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 573099.81it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 866907.82it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 650180.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Euclidean Distance: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.8600928294022923\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  1  2]\n",
      " [ 0 21  1]\n",
      " [ 1  2 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.70      0.78        10\n",
      "           1       0.88      0.95      0.91        22\n",
      "           2       0.84      0.84      0.84        19\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.86      0.83      0.84        51\n",
      "weighted avg       0.86      0.86      0.86        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 1145432.42it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1210237.65it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1324517.05it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1256443.49it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 920040.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Euclidean Distance: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.862286435041853\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  1  1]\n",
      " [ 0 20  2]\n",
      " [ 1  2 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.84        10\n",
      "           1       0.87      0.91      0.89        22\n",
      "           2       0.84      0.84      0.84        19\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.87      0.85      0.86        51\n",
      "weighted avg       0.86      0.86      0.86        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Euclidean Distance: \n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.862286435041853\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  1  1]\n",
      " [ 0 20  2]\n",
      " [ 1  2 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.84        10\n",
      "           1       0.87      0.91      0.89        22\n",
      "           2       0.84      0.84      0.84        19\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.87      0.85      0.86        51\n",
      "weighted avg       0.86      0.86      0.86        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: {}\\n\\n\" .format(total_fold))\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "        \n",
    "    knn_dbow = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\n",
    "    knn_dbow.fit(X_trained, y_trained)\n",
    "    y_pred = knn_dbow.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using KNN with Euclidean Distance: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    knn_dm = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\n",
    "    knn_dm.fit(X_trained, y_trained)\n",
    "    y_pred = knn_dm.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using KNN with Euclidean Distance: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    knn_mixed = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\n",
    "    knn_mixed.fit(X_trained, y_trained)\n",
    "    y_pred = knn_mixed.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PAIRED Model Using KNN with Euclidean Distance: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold With Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 5\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  1 :  204\n",
      "No. of Testing Dataset in Fold  1 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1108339.40it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1480342.59it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 950708.91it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1267611.88it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1286673.71it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 570425.34it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 464768.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Gaussian Naive Bayes: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.9019607843137255\n",
      "Testing F1 score: 0.9023569023569024\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  1  0]\n",
      " [ 0 16  1]\n",
      " [ 1  2 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91        11\n",
      "           1       0.84      0.94      0.89        17\n",
      "           2       0.95      0.87      0.91        23\n",
      "\n",
      "    accuracy                           0.90        51\n",
      "   macro avg       0.90      0.91      0.90        51\n",
      "weighted avg       0.91      0.90      0.90        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 1196696.53it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 920040.88it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 630072.18it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 515444.59it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 600869.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Gaussian Naive Bayes: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.9019607843137255\n",
      "Testing F1 score: 0.9023569023569024\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  1  0]\n",
      " [ 0 16  1]\n",
      " [ 1  2 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91        11\n",
      "           1       0.84      0.94      0.89        17\n",
      "           2       0.95      0.87      0.91        23\n",
      "\n",
      "    accuracy                           0.90        51\n",
      "   macro avg       0.90      0.91      0.90        51\n",
      "weighted avg       0.91      0.90      0.90        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using Gaussian Naive Bayes: \n",
      "Testing accuracy 0.8823529411764706\n",
      "Testing F1 score: 0.8807768083309879\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  2  1]\n",
      " [ 0 17  0]\n",
      " [ 1  2 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.73      0.80        11\n",
      "           1       0.81      1.00      0.89        17\n",
      "           2       0.95      0.87      0.91        23\n",
      "\n",
      "    accuracy                           0.88        51\n",
      "   macro avg       0.88      0.87      0.87        51\n",
      "weighted avg       0.89      0.88      0.88        51\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  204\n",
      "No. of Testing Dataset in Fold  2 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1085835.05it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 984623.72it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1467646.68it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1300361.73it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1168904.39it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1180190.37it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1457645.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Gaussian Naive Bayes: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8823529411764706\n",
      "Testing F1 score: 0.8852450980392156\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  0  1]\n",
      " [ 0 21  3]\n",
      " [ 2  0 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88        12\n",
      "           1       1.00      0.88      0.93        24\n",
      "           2       0.76      0.87      0.81        15\n",
      "\n",
      "    accuracy                           0.88        51\n",
      "   macro avg       0.87      0.89      0.88        51\n",
      "weighted avg       0.89      0.88      0.89        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 725116.96it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1275168.43it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 841335.32it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1225842.43it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1488066.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Gaussian Naive Bayes: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8823529411764706\n",
      "Testing F1 score: 0.8852450980392156\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  0  1]\n",
      " [ 0 21  3]\n",
      " [ 2  0 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88        12\n",
      "           1       1.00      0.88      0.93        24\n",
      "           2       0.76      0.87      0.81        15\n",
      "\n",
      "    accuracy                           0.88        51\n",
      "   macro avg       0.87      0.89      0.88        51\n",
      "weighted avg       0.89      0.88      0.89        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using Gaussian Naive Bayes: \n",
      "Testing accuracy 0.8823529411764706\n",
      "Testing F1 score: 0.8845170175093449\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  0  1]\n",
      " [ 0 22  2]\n",
      " [ 3  0 12]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.92      0.85        12\n",
      "           1       1.00      0.92      0.96        24\n",
      "           2       0.80      0.80      0.80        15\n",
      "\n",
      "    accuracy                           0.88        51\n",
      "   macro avg       0.86      0.88      0.87        51\n",
      "weighted avg       0.89      0.88      0.88        51\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  204\n",
      "No. of Testing Dataset in Fold  3 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1185094.20it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1430832.80it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1227601.17it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 860802.83it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 831523.83it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 789333.96it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1173714.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Gaussian Naive Bayes: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.863095238095238\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  0  1]\n",
      " [ 3 15  0]\n",
      " [ 1  2 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86        16\n",
      "           1       0.88      0.83      0.86        18\n",
      "           2       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.87      0.86      0.86        51\n",
      "weighted avg       0.87      0.86      0.86        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 689474.63it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 834768.80it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 766700.73it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1210237.65it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 889436.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Gaussian Naive Bayes: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.863095238095238\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  0  1]\n",
      " [ 3 15  0]\n",
      " [ 1  2 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86        16\n",
      "           1       0.88      0.83      0.86        18\n",
      "           2       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.87      0.86      0.86        51\n",
      "weighted avg       0.87      0.86      0.86        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using Gaussian Naive Bayes: \n",
      "Testing accuracy 0.8235294117647058\n",
      "Testing F1 score: 0.8235433924015239\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 3 14  1]\n",
      " [ 1  2 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.82        16\n",
      "           1       0.82      0.78      0.80        18\n",
      "           2       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.82        51\n",
      "   macro avg       0.83      0.83      0.82        51\n",
      "weighted avg       0.83      0.82      0.82        51\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  204\n",
      "No. of Testing Dataset in Fold  4 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1250932.77it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1208528.27it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 854783.23it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 623188.65it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1005450.08it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1038395.65it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 654658.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Gaussian Naive Bayes: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9607843137254902\n",
      "Testing F1 score: 0.9603174603174605\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  0  2]\n",
      " [ 0 16  0]\n",
      " [ 0  0 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93        15\n",
      "           1       1.00      1.00      1.00        16\n",
      "           2       0.91      1.00      0.95        20\n",
      "\n",
      "    accuracy                           0.96        51\n",
      "   macro avg       0.97      0.96      0.96        51\n",
      "weighted avg       0.96      0.96      0.96        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 1312328.25it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 728202.57it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1127322.81it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1349586.78it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1485482.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Gaussian Naive Bayes: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9607843137254902\n",
      "Testing F1 score: 0.9603174603174605\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  0  2]\n",
      " [ 0 16  0]\n",
      " [ 0  0 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93        15\n",
      "           1       1.00      1.00      1.00        16\n",
      "           2       0.91      1.00      0.95        20\n",
      "\n",
      "    accuracy                           0.96        51\n",
      "   macro avg       0.97      0.96      0.96        51\n",
      "weighted avg       0.96      0.96      0.96        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using Gaussian Naive Bayes: \n",
      "Testing accuracy 0.9215686274509803\n",
      "Testing F1 score: 0.9197012138188607\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[12  1  2]\n",
      " [ 1 15  0]\n",
      " [ 0  0 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.80      0.86        15\n",
      "           1       0.94      0.94      0.94        16\n",
      "           2       0.91      1.00      0.95        20\n",
      "\n",
      "    accuracy                           0.92        51\n",
      "   macro avg       0.92      0.91      0.92        51\n",
      "weighted avg       0.92      0.92      0.92        51\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  204\n",
      "No. of Testing Dataset in Fold  5 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1435634.26it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 902571.75it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1151598.94it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1229364.97it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1164133.36it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1267611.88it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 804171.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Gaussian Naive Bayes: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.8615852877098553\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  1  2]\n",
      " [ 0 20  2]\n",
      " [ 0  2 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82        10\n",
      "           1       0.87      0.91      0.89        22\n",
      "           2       0.81      0.89      0.85        19\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.89      0.83      0.85        51\n",
      "weighted avg       0.87      0.86      0.86        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 204/204 [00:00<00:00, 1205123.97it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 747935.33it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 872210.01it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 665348.38it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 818792.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Gaussian Naive Bayes: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.8615852877098553\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  1  2]\n",
      " [ 0 20  2]\n",
      " [ 0  2 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82        10\n",
      "           1       0.87      0.91      0.89        22\n",
      "           2       0.81      0.89      0.85        19\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.89      0.83      0.85        51\n",
      "weighted avg       0.87      0.86      0.86        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using Gaussian Naive Bayes: \n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.862286435041853\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  1  1]\n",
      " [ 0 20  2]\n",
      " [ 1  2 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.84        10\n",
      "           1       0.87      0.91      0.89        22\n",
      "           2       0.84      0.84      0.84        19\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.87      0.85      0.86        51\n",
      "weighted avg       0.86      0.86      0.86        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: {}\\n\\n\" .format(total_fold))\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "        \n",
    "    nb_gaussian_pv_dbow = GaussianNB()\n",
    "    nb_gaussian_pv_dbow.fit(X_trained, y_trained)\n",
    "    y_pred = nb_gaussian_pv_dbow.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using Gaussian Naive Bayes: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    nb_gaussian_pv_dm = GaussianNB()\n",
    "    nb_gaussian_pv_dm.fit(X_trained, y_trained)\n",
    "    y_pred_pv_dm = nb_gaussian_pv_dm.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using Gaussian Naive Bayes: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    nb_gaussian_mixed = GaussianNB()\n",
    "    nb_gaussian_mixed.fit(X_trained, y_trained)\n",
    "    y_pred = nb_gaussian_mixed.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PAIRED Model Using Gaussian Naive Bayes: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
