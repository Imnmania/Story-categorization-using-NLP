{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['opencv-text-detection.zip',\n",
       " '.ipynb_checkpoints',\n",
       " 'accepted_projects',\n",
       " 'Andrew W. Trask - Grokking Deep Learning-Manning Publications (2019).pdf',\n",
       " 'Collective_Dataset',\n",
       " 'corpus',\n",
       " 'corpus.zip',\n",
       " 'doc2vec.ipynb',\n",
       " 'doc2vec_with_kfold.html',\n",
       " 'doc2vec_with_kfold.ipynb',\n",
       " 'google-play-store-apps',\n",
       " 'google-play-store-apps.zip',\n",
       " 'helpline-of-all-sorts',\n",
       " 'helpline-of-all-sorts.zip',\n",
       " 'kmeans_clustering.ipynb',\n",
       " 'labels.txt',\n",
       " 'logistic_regression.ipynb',\n",
       " 'MURA-v1.1',\n",
       " 'MURA-v1.1.zip',\n",
       " 'nltk',\n",
       " 'opencv-text-detection',\n",
       " 'Papers',\n",
       " 'Papers.zip',\n",
       " 'Papers_Association_Rule',\n",
       " 'processed_dataset.csv',\n",
       " 'Results',\n",
       " 'reviews.txt',\n",
       " 'sentiment_analysis_from_story_preprocessing.ipynb',\n",
       " 'sentiment_dictionary.csv',\n",
       " 'sklearn_test.ipynb',\n",
       " 'Slides',\n",
       " 'Story-categorization-using-NLP',\n",
       " 'Tan.pdf',\n",
       " 'Testing',\n",
       " 'Udacity',\n",
       " 'videodata.csv',\n",
       " 'word2vec.model',\n",
       " 'word2vector_test.ipynb']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import multiprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "cores = multiprocessing.cpu_count()\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.getcwd() + \"/Collective_Dataset/helpline_data.csv\", usecols=[0,1])#header=None, , names=['story', 'category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was in class 11. I was coming back from my t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A girl jumped on me on my friend's house! She ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I experienced a great trauma and stress due to...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sexual harassment is a very common act that we...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi, I am X from XX University,  Batch 121. The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I don't know whether I should address it as ha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>When I got admitted in a well reputed college,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This is normally about our areas local problem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I went to do a free class on a famous coaching...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I was returning home after duty at about 9 PM....</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               story  category\n",
       "0  I was in class 11. I was coming back from my t...         1\n",
       "1  A girl jumped on me on my friend's house! She ...         2\n",
       "2  I experienced a great trauma and stress due to...         2\n",
       "3  sexual harassment is a very common act that we...         2\n",
       "4  Hi, I am X from XX University,  Batch 121. The...         1\n",
       "5  I don't know whether I should address it as ha...         0\n",
       "6  When I got admitted in a well reputed college,...         1\n",
       "7  This is normally about our areas local problem...         1\n",
       "8  I went to do a free class on a famous coaching...         2\n",
       "9  I was returning home after duty at about 9 PM....         2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      I was in class 11. I was coming back from my t...\n",
       "1      A girl jumped on me on my friend's house! She ...\n",
       "2      I experienced a great trauma and stress due to...\n",
       "3      sexual harassment is a very common act that we...\n",
       "4      Hi, I am X from XX University,  Batch 121. The...\n",
       "                             ...                        \n",
       "142    High school and junior high were hell for me. ...\n",
       "143    My bullying started, or when I first noticed i...\n",
       "144    I was in my class 8 year in a different school...\n",
       "145    today at recess a group of senior guys came up...\n",
       "146    I was just starting Middle school, and I had t...\n",
       "Name: story, Length: 147, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      2\n",
       "2      2\n",
       "3      2\n",
       "4      1\n",
       "      ..\n",
       "142    1\n",
       "143    1\n",
       "144    1\n",
       "145    1\n",
       "146    1\n",
       "Name: category, Length: 147, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = range(147)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29870"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.story.apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnt_pro = df['category'].value_counts()\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('category', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Work Stress Total Data Count:  20\n",
      "Bullying Total Data Count:  94\n",
      "Sexual Harassment Total Data Count:  33\n"
     ]
    }
   ],
   "source": [
    "work_stress_counter, bullying_counter, sexual_harassment_counter = 0, 0, 0\n",
    "for c in df['category']:\n",
    "    if c==0:\n",
    "        work_stress_counter+=1\n",
    "    elif c == 1:\n",
    "        bullying_counter+=1\n",
    "    else:\n",
    "        sexual_harassment_counter+=1\n",
    "print(\"Work Stress Total Data Count: \", work_stress_counter)\n",
    "print(\"Bullying Total Data Count: \", bullying_counter)\n",
    "print(\"Sexual Harassment Total Data Count: \", sexual_harassment_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I experienced a great trauma and stress due to harassment issues .Moreover, I could not concentrate on my study and work for a long time\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(df.story[2])\n",
    "print(df.category[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was in class 11. I was coming back from my tuition classes, at a time a group of boys were passing by. I gave them much space to pass away, but one of the pulled my scarf so harshly that I couldn't balanced and I fell, in that time I became so nervous that I couldn't think about anything and got up quickly, left the place. 2 days I didn't attend my classes as well, because of the fear of 'If that happens again'. That incident left scars in my mind and I always try to avoid crowded areas. \n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(df.story[0])\n",
    "print(df.category[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was returning to Khulna Medical College by an auto the day after Eid. In that auto, a woman was sitting beside me and in front of her there was another woman. After some time, a man came and got seated in front of my seat. From the very fast moment, he was giving me a negative vibe. I was feeling that he was touching my knees with his knees. But it was a running auto, so I decided to stay silent. That man was about 37 - 40 years old. The moment, the woman beside me got down from the auto, that man came to sit beside me. From that time, he was trying to come close to me even though there was a lot of space in the auto. Also, he was continuously touching my waist with his elbow and behaving like he didn�â��t know anything. Then, I told him to sit away from me. He did and after some time, he left the auto.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(df.story[10])\n",
    "print(df.category[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('x', '')\n",
    "    return text\n",
    "df['story'] = df['story'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH TRAIN_TEST_SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size = .20, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged = train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "test_tagged = test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>i was on my way from gabtoli to mirpur on a bu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>the first time was the day after my 20th birth...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>i worked as an office manager, and the only wo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>i have been called hurtful names and i have be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>i am 15 now and i have been bullied since grad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>hi, my name is hope and like many people today...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>when i was in 7th grade, i had a semester of m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>i was fresh off of moving to a new town and en...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>hello! first of all, i'm sorry if there are a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>as a final year student, completing the final ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 story  category\n",
       "14   i was on my way from gabtoli to mirpur on a bu...         2\n",
       "99   the first time was the day after my 20th birth...         2\n",
       "128  i worked as an office manager, and the only wo...         2\n",
       "120  i have been called hurtful names and i have be...         1\n",
       "45   i am 15 now and i have been bullied since grad...         1\n",
       "..                                                 ...       ...\n",
       "75   hi, my name is hope and like many people today...         1\n",
       "43   when i was in 7th grade, i had a semester of m...         1\n",
       "22   i was fresh off of moving to a new town and en...         1\n",
       "72   hello! first of all, i'm sorry if there are a ...         1\n",
       "15   as a final year student, completing the final ...         0\n",
       "\n",
       "[117 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>i am currently 17 years old and a junior in hi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>as a librarian, i've been threatened with stal...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seual harassment is a very common act that we ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>i had this one particular kid who would pick o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>my whole life i’ve had a speech impediment whe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>i have been teased all my life. i've been call...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>i used to work for a call center and the men t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>i was 14 years old at that time and went to vi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>i was around 12-13 years old and at this “meet...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>i feel my whole body hurting. my mental health...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i don't know whether i should address it as ha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>i was bullied from 1st all the way to 5th grad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>this guy is a well known face in bangladesh me...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>i had debilitating migraines for three years b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>i am a teenage girl now, i’m in the middle sch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>i’m 18 years old. well my story about being bu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>i’ve always been super flat-chested and i’m st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>and im 14 years old. all my life, i been bulli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>this was in 9th grade. there was the kid who w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>i had everything going for me; being popular, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>well ever since i've been in second grade i wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>i love my work, but hate going each day becaus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ever since first grade, my three best friends ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i eperienced a great trauma and stress due to ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>i couldn’t get pregnant for years in my toic o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>i have a weird name. i don't even know why i h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>today at recess a group of senior guys came up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>one time, my freshman year of high school, i w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>i am a tall guy, i have always been tall for m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>my dad is active military and was deployed for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 story  category\n",
       "91   i am currently 17 years old and a junior in hi...         1\n",
       "127  as a librarian, i've been threatened with stal...         2\n",
       "3    seual harassment is a very common act that we ...         2\n",
       "35   i had this one particular kid who would pick o...         1\n",
       "134  my whole life i’ve had a speech impediment whe...         1\n",
       "82   i have been teased all my life. i've been call...         1\n",
       "129  i used to work for a call center and the men t...         2\n",
       "12   i was 14 years old at that time and went to vi...         2\n",
       "42   i was around 12-13 years old and at this “meet...         1\n",
       "126  i feel my whole body hurting. my mental health...         0\n",
       "5    i don't know whether i should address it as ha...         0\n",
       "53   i was bullied from 1st all the way to 5th grad...         1\n",
       "93   this guy is a well known face in bangladesh me...         2\n",
       "122  i had debilitating migraines for three years b...         0\n",
       "48   i am a teenage girl now, i’m in the middle sch...         1\n",
       "59   i’m 18 years old. well my story about being bu...         1\n",
       "24   i’ve always been super flat-chested and i’m st...         1\n",
       "86   and im 14 years old. all my life, i been bulli...         1\n",
       "41   this was in 9th grade. there was the kid who w...         1\n",
       "77   i had everything going for me; being popular, ...         1\n",
       "85   well ever since i've been in second grade i wa...         1\n",
       "123  i love my work, but hate going each day becaus...         0\n",
       "25   ever since first grade, my three best friends ...         1\n",
       "2    i eperienced a great trauma and stress due to ...         2\n",
       "114  i couldn’t get pregnant for years in my toic o...         0\n",
       "64   i have a weird name. i don't even know why i h...         1\n",
       "145  today at recess a group of senior guys came up...         1\n",
       "23   one time, my freshman year of high school, i w...         1\n",
       "44   i am a tall guy, i have always been tall for m...         1\n",
       "54   my dad is active military and was deployed for...         1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14     ([was, on, my, way, from, gabtoli, to, mirpur,...\n",
       "99     ([the, first, time, was, the, day, after, my, ...\n",
       "128    ([worked, as, an, office, manager, and, the, o...\n",
       "120    ([have, been, called, hurtful, names, and, hav...\n",
       "45     ([am, 15, now, and, have, been, bullied, since...\n",
       "                             ...                        \n",
       "75     ([hi, my, name, is, hope, and, like, many, peo...\n",
       "43     ([when, was, in, 7th, grade, had, semester, of...\n",
       "22     ([was, fresh, off, of, moving, to, new, town, ...\n",
       "72     ([hello, first, of, all, 'm, sorry, if, there,...\n",
       "15     ([as, final, year, student, completing, the, f...\n",
       "Length: 117, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['worked', 'as', 'an', 'office', 'manager', 'and', 'the', 'only', 'woman', 'for', 'an', 'industrial', 'insulation', 'company', 'had', 'just', 'come', 'back', 'from', 'maternity', 'leave', 'and', 'was', 'worried', 'about', 'my', 'milk', 'supply', 'went', 'into', 'the', 'bathroom', 'to', 'pump', 'for', 'about', '15', 'minutes', 'every', 'two', 'hours', 'and', 'all', 'of', 'the', 'men', 'in', 'the', 'office', 'would', 'stand', 'in', 'the', 'break', 'area', 'right', 'in', 'front', 'of', 'the', 'bathroom', 'door', 'and', 'make', 'baby', 'crying', 'noises', 'to', 'make', 'fun', 'of', 'me', 'eventually', 'it', 'progressed', 'to', 'the', 'point', 'that', 'they', 'would', 'make', 'crying', 'noises', 'every', 'time', 'they', 'passed', 'my', 'desk', 'in', 'hopes', 'that', 'would', 'leak', 'through', 'my', 'shirt', 'they', 'would', 'also', 'make', 'comments', 'about', 'how', 'much', 'larger', 'my', 'breasts', 'were', 'since', 'having', 'baby', 'felt', 'so', 'harassed', 'and', 'unsafe', 'that', 'would', 'dread', 'going', 'to', 'work', 'every', 'day', 'and', 'even', 'had', 'more', 'than', 'few', 'nervous', 'breakdowns', 'my', 'husband', 'was', 'furious', 'and', 'had', 'to', 'convince', 'him', 'not', 'to', 'take', 'any', 'drastic', 'action', 'so', 'that', 'could', 'be', 'sure', 'to', 'have', 'good', 'reference', 'if', 'needed', 'to', 'find', 'another', 'job', 'we', 'had', 'long', 'conversation', 'and', 'looked', 'at', 'our', 'finances', 'and', 'decided', 'the', 'etra', 'money', 'was', \"n't\", 'worth', 'the', 'emotional', 'distress', 'ended', 'up', 'quitting', 'my', 'job', 'and', 'staying', 'home', 'with', 'our', 'kids'], tags=[2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged.values[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['as', 'librarian', \"'ve\", 'been', 'threatened', 'with', 'stalking', 'for', 'not', 'giving', 'my', 'phone', 'number', 'to', 'patron', \"'ve\", 'also', 'been', 'shown', 'naked', 'seually', 'eplicit', 'photos', 'and', 'propositioned', 'by', 'countless', 'men', 'some', 'of', 'which', 'were', 'married', 'and', 'standing', 'right', 'net', 'to', 'their', 'children', 'some', 'of', 'my', 'female', 'colleagues', 'have', 'had', 'male', 'patrons', 'waiting', 'for', 'them', 'in', 'the', 'parking', 'lot', 'after', 'work', 'and', 'they', \"'ve\", 'received', 'eplicit', 'messages', 'over', 'social', 'media'], tags=[2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tagged.values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PV-DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [00:00<00:00, 778942.17it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [00:00<00:00, 336349.26it/s]\n",
      "100%|██████████| 117/117 [00:00<00:00, 427468.26it/s]\n",
      "100%|██████████| 117/117 [00:00<00:00, 386708.88it/s]\n",
      "100%|██████████| 117/117 [00:00<00:00, 469152.55it/s]\n",
      "100%|██████████| 117/117 [00:00<00:00, 557651.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 209 ms, sys: 10.9 ms, total: 220 ms\n",
      "Wall time: 117 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(5):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 1.0\n",
      "Testing F1 score: 1.0\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 5  0  0]\n",
      " [ 0 19  0]\n",
      " [ 0  0  6]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "           1       1.00      1.00      1.00        19\n",
      "           2       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "print('\\nTesting Confusion Matrix: ')\n",
    "print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "print('Testing Classification Report: ')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PV-DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [00:00<00:00, 384286.27it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [00:00<00:00, 958464.00it/s]\n",
      "100%|██████████| 117/117 [00:00<00:00, 517105.97it/s]\n",
      "100%|██████████| 117/117 [00:00<00:00, 681574.40it/s]\n",
      "100%|██████████| 117/117 [00:00<00:00, 566666.94it/s]\n",
      "100%|██████████| 117/117 [00:00<00:00, 733532.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 264 ms, sys: 12.7 ms, total: 277 ms\n",
      "Wall time: 139 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(5):\n",
    "    model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dmm.alpha -= 0.002\n",
    "    model_dmm.min_alpha = model_dmm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dmm, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dmm, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.8333333333333334\n",
      "Testing F1 score: 0.8425925925925924\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 5  0  0]\n",
      " [ 2 17  0]\n",
      " [ 3  0  3]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         5\n",
      "           1       1.00      0.89      0.94        19\n",
      "           2       1.00      0.50      0.67         6\n",
      "\n",
      "    accuracy                           0.83        30\n",
      "   macro avg       0.83      0.80      0.76        30\n",
      "weighted avg       0.92      0.83      0.84        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "print('\\nTesting Confusion Matrix: ')\n",
    "print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "print('Testing Classification Report: ')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = get_vectors(new_model, train_tagged)\n",
    "y_test, X_test = get_vectors(new_model, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.7666666666666667\n",
      "Testing F1 score: 0.7708708708708709\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 4  1  0]\n",
      " [ 2 17  0]\n",
      " [ 4  0  2]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.80      0.53         5\n",
      "           1       0.94      0.89      0.92        19\n",
      "           2       1.00      0.33      0.50         6\n",
      "\n",
      "    accuracy                           0.77        30\n",
      "   macro avg       0.78      0.68      0.65        30\n",
      "weighted avg       0.86      0.77      0.77        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "print('\\nTesting Confusion Matrix: ')\n",
    "print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "print('Testing Classification Report: ')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH K-FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  147\n",
      "Type:  <class 'pandas.core.series.Series'>\n",
      "First Ten Values:\n",
      " 0    i was in class 11. i was coming back from my t...\n",
      "1    a girl jumped on me on my friend's house! she ...\n",
      "2    i eperienced a great trauma and stress due to ...\n",
      "3    seual harassment is a very common act that we ...\n",
      "4    hi, i am  from  university,  batch 121. there ...\n",
      "5    i don't know whether i should address it as ha...\n",
      "6    when i got admitted in a well reputed college,...\n",
      "7    this is normally about our areas local problem...\n",
      "8    i went to do a free class on a famous coaching...\n",
      "9    i was returning home after duty at about 9 pm....\n",
      "Name: story, dtype: object\n",
      "i was in class 11. i was coming back from my tuition classes, at a time a group of boys were passing by. i gave them much space to pass away, but one of the pulled my scarf so harshly that i couldn't balanced and i fell, in that time i became so nervous that i couldn't think about anything and got up quickly, left the place. 2 days i didn't attend my classes as well, because of the fear of 'if that happens again'. that incident left scars in my mind and i always try to avoid crowded areas. \n"
     ]
    }
   ],
   "source": [
    "X = df['story']\n",
    "print(\"Length: \", len(X))\n",
    "print(\"Type: \", type(X))\n",
    "print(\"First Ten Values:\\n\", X[:10])\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  147\n",
      "Type:  <class 'pandas.core.series.Series'>\n",
      "First Ten Values:  0    1\n",
      "1    2\n",
      "2    2\n",
      "3    2\n",
      "4    1\n",
      "5    0\n",
      "6    1\n",
      "7    1\n",
      "8    2\n",
      "9    2\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y = df['category']\n",
    "print(\"Length: \", len(y))\n",
    "print(\"Type: \", type(y))\n",
    "print(\"First Ten Values: \", y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=2, random_state=None, shuffle=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten_fold = KFold(n_splits=2)\n",
    "ten_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten_fold.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-FOLD PV-DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Fold No.:  [ 74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\n",
      "  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
      " 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127\n",
      " 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n",
      " 146]  Test Fold No.:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 484468.66it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 286689.32it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 289672.84it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 459046.76it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 326422.38it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 470329.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  1\n",
      "Testing accuracy 0.8513513513513513\n",
      "Testing F1 score: 0.8600064350064351\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 4  2  0]\n",
      " [ 4 51  3]\n",
      " [ 1  1  8]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.67      0.53         6\n",
      "           1       0.94      0.88      0.91        58\n",
      "           2       0.73      0.80      0.76        10\n",
      "\n",
      "    accuracy                           0.85        74\n",
      "   macro avg       0.71      0.78      0.74        74\n",
      "weighted avg       0.87      0.85      0.86        74\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Fold No.:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73]  Test Fold No.:  [ 74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\n",
      "  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
      " 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127\n",
      " 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n",
      " 146]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:00<00:00, 300463.21it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 436537.97it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 524288.00it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 485725.35it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 492664.28it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 347568.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  2\n",
      "Testing accuracy 0.5616438356164384\n",
      "Testing F1 score: 0.4533710003623006\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 0 13  1]\n",
      " [ 0 36  0]\n",
      " [ 0 18  5]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.54      1.00      0.70        36\n",
      "           2       0.83      0.22      0.34        23\n",
      "\n",
      "    accuracy                           0.56        73\n",
      "   macro avg       0.46      0.41      0.35        73\n",
      "weighted avg       0.53      0.56      0.45        73\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "    print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg.fit(X_trained, y_trained)\n",
    "    y_pred = logreg.predict(X_tested)\n",
    "    \n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-FOLD PV-DM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Fold No.:  [ 74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\n",
      "  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
      " 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127\n",
      " 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n",
      " 146]  Test Fold No.:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 373851.27it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 237905.36it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 299886.57it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 296690.11it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 304660.89it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 522498.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  1\n",
      "Testing accuracy 0.8243243243243243\n",
      "Testing F1 score: 0.8412030187731122\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 5  0  1]\n",
      " [ 2 48  8]\n",
      " [ 1  1  8]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.83      0.71         6\n",
      "           1       0.98      0.83      0.90        58\n",
      "           2       0.47      0.80      0.59        10\n",
      "\n",
      "    accuracy                           0.82        74\n",
      "   macro avg       0.69      0.82      0.73        74\n",
      "weighted avg       0.88      0.82      0.84        74\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Fold No.:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73]  Test Fold No.:  [ 74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\n",
      "  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
      " 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127\n",
      " 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n",
      " 146]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:00<00:00, 525175.12it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 51954.89it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 537917.67it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 480462.07it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 281905.99it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 193986.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  2\n",
      "Testing accuracy 0.6575342465753424\n",
      "Testing F1 score: 0.5815691158156912\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 0  9  5]\n",
      " [ 0 34  2]\n",
      " [ 0  9 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.65      0.94      0.77        36\n",
      "           2       0.67      0.61      0.64        23\n",
      "\n",
      "    accuracy                           0.66        73\n",
      "   macro avg       0.44      0.52      0.47        73\n",
      "weighted avg       0.53      0.66      0.58        73\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "    print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg.fit(X_trained, y_trained)\n",
    "    y_pred = logreg.predict(X_tested)\n",
    "    \n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n\\n\")\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAIRED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 641895.58it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 270480.73it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 621063.27it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 256435.67it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 524288.00it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 447637.71it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 686511.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8378378378378378\n",
      "Testing F1 score: 0.8497448497448498\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 4  1  1]\n",
      " [ 1 50  7]\n",
      " [ 1  1  8]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         6\n",
      "           1       0.96      0.86      0.91        58\n",
      "           2       0.50      0.80      0.62        10\n",
      "\n",
      "    accuracy                           0.84        74\n",
      "   macro avg       0.71      0.78      0.73        74\n",
      "weighted avg       0.88      0.84      0.85        74\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 73/73 [00:00<00:00, 326770.75it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 443103.03it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 433075.24it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 285886.27it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 303754.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8378378378378378\n",
      "Testing F1 score: 0.8512328288029222\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  0  0]\n",
      " [ 2 48  8]\n",
      " [ 1  1  8]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         6\n",
      "           1       0.98      0.83      0.90        58\n",
      "           2       0.50      0.80      0.62        10\n",
      "\n",
      "    accuracy                           0.84        74\n",
      "   macro avg       0.72      0.88      0.77        74\n",
      "weighted avg       0.89      0.84      0.85        74\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model: \n",
      "Testing accuracy 0.8648648648648649\n",
      "Testing F1 score: 0.8721558557046217\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 5  1  0]\n",
      " [ 2 51  5]\n",
      " [ 1  1  8]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.83      0.71         6\n",
      "           1       0.96      0.88      0.92        58\n",
      "           2       0.62      0.80      0.70        10\n",
      "\n",
      "    accuracy                           0.86        74\n",
      "   macro avg       0.73      0.84      0.78        74\n",
      "weighted avg       0.89      0.86      0.87        74\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:00<00:00, 664622.05it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 768263.60it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 378510.36it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 261261.36it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 330541.53it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 401524.57it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 68713.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.547945205479452\n",
      "Testing F1 score: 0.44345957561106636\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 0 13  1]\n",
      " [ 0 35  1]\n",
      " [ 0 18  5]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.53      0.97      0.69        36\n",
      "           2       0.71      0.22      0.33        23\n",
      "\n",
      "    accuracy                           0.55        73\n",
      "   macro avg       0.41      0.40      0.34        73\n",
      "weighted avg       0.49      0.55      0.44        73\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 74/74 [00:00<00:00, 263479.20it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 612186.38it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 503861.19it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 358404.73it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 259080.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.6438356164383562\n",
      "Testing F1 score: 0.5624048706240488\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 0 12  2]\n",
      " [ 0 36  0]\n",
      " [ 0 12 11]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.60      1.00      0.75        36\n",
      "           2       0.85      0.48      0.61        23\n",
      "\n",
      "    accuracy                           0.64        73\n",
      "   macro avg       0.48      0.49      0.45        73\n",
      "weighted avg       0.56      0.64      0.56        73\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model: \n",
      "Testing accuracy 0.5753424657534246\n",
      "Testing F1 score: 0.4750998619588995\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 0 14  0]\n",
      " [ 0 36  0]\n",
      " [ 0 17  6]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.54      1.00      0.70        36\n",
      "           2       1.00      0.26      0.41        23\n",
      "\n",
      "    accuracy                           0.58        73\n",
      "   macro avg       0.51      0.42      0.37        73\n",
      "weighted avg       0.58      0.58      0.48        73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: 2\\n\\n\")\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    logreg_dbow = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg_dbow.fit(X_trained, y_trained)\n",
    "    y_pred = logreg_dbow.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    logreg_dm = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg_dm.fit(X_trained, y_trained)\n",
    "    y_pred = logreg_dm.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    logreg_paired = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg_paired.fit(X_train, y_train)\n",
    "    y_pred = logreg_paired.predict(X_test)\n",
    "    \n",
    "    print(\"FOR Paired Model: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 461120.77it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 624865.70it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 633921.72it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 457674.43it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 408790.64it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 589950.27it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 536224.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Entropy: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8243243243243243\n",
      "Testing F1 score: 0.8267710189906987\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 2  3  1]\n",
      " [ 1 52  5]\n",
      " [ 2  1  7]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.33      0.36         6\n",
      "           1       0.93      0.90      0.91        58\n",
      "           2       0.54      0.70      0.61        10\n",
      "\n",
      "    accuracy                           0.82        74\n",
      "   macro avg       0.62      0.64      0.63        74\n",
      "weighted avg       0.83      0.82      0.83        74\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 73/73 [00:00<00:00, 240144.46it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 644598.30it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 377539.08it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 272648.43it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 674414.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Entropy:  \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8243243243243243\n",
      "Testing F1 score: 0.8458303237421706\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  0  0]\n",
      " [ 7 50  1]\n",
      " [ 4  1  5]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      1.00      0.52         6\n",
      "           1       0.98      0.86      0.92        58\n",
      "           2       0.83      0.50      0.62        10\n",
      "\n",
      "    accuracy                           0.82        74\n",
      "   macro avg       0.72      0.79      0.69        74\n",
      "weighted avg       0.91      0.82      0.85        74\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Entropy: \n",
      "Testing accuracy 0.8243243243243243\n",
      "Testing F1 score: 0.8458303237421706\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  0  0]\n",
      " [ 7 50  1]\n",
      " [ 4  1  5]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      1.00      0.52         6\n",
      "           1       0.98      0.86      0.92        58\n",
      "           2       0.83      0.50      0.62        10\n",
      "\n",
      "    accuracy                           0.82        74\n",
      "   macro avg       0.72      0.79      0.69        74\n",
      "weighted avg       0.91      0.82      0.85        74\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:00<00:00, 227884.36it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 468849.69it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 230764.68it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 355124.14it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 284489.91it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 274914.52it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 348739.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Entropy: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.6301369863013698\n",
      "Testing F1 score: 0.5476491966649674\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 0 13  1]\n",
      " [ 0 36  0]\n",
      " [ 0 13 10]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.58      1.00      0.73        36\n",
      "           2       0.91      0.43      0.59        23\n",
      "\n",
      "    accuracy                           0.63        73\n",
      "   macro avg       0.50      0.48      0.44        73\n",
      "weighted avg       0.57      0.63      0.55        73\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 74/74 [00:00<00:00, 500610.48it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 57691.17it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 538851.56it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 266190.82it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 580146.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Entropy:  \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.6301369863013698\n",
      "Testing F1 score: 0.5476491966649674\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 0 13  1]\n",
      " [ 0 36  0]\n",
      " [ 0 13 10]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.58      1.00      0.73        36\n",
      "           2       0.91      0.43      0.59        23\n",
      "\n",
      "    accuracy                           0.63        73\n",
      "   macro avg       0.50      0.48      0.44        73\n",
      "weighted avg       0.57      0.63      0.55        73\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Entropy: \n",
      "Testing accuracy 0.6301369863013698\n",
      "Testing F1 score: 0.5476491966649674\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 0 13  1]\n",
      " [ 0 36  0]\n",
      " [ 0 13 10]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.58      1.00      0.73        36\n",
      "           2       0.91      0.43      0.59        23\n",
      "\n",
      "    accuracy                           0.63        73\n",
      "   macro avg       0.50      0.48      0.44        73\n",
      "weighted avg       0.57      0.63      0.55        73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: 2\\n\\n\")\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"entropy\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using Decision Tree with Entropy: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"entropy\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using Decision Tree with Entropy:  \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"entropy\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR Paired Model Using Decision Tree with Entropy: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 208288.57it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 483703.30it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 272891.44it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 289399.05it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 436159.82it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 655640.67it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 353153.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Gini: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8108108108108109\n",
      "Testing F1 score: 0.8157991056221143\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 2  3  1]\n",
      " [ 1 51  6]\n",
      " [ 2  1  7]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.33      0.36         6\n",
      "           1       0.93      0.88      0.90        58\n",
      "           2       0.50      0.70      0.58        10\n",
      "\n",
      "    accuracy                           0.81        74\n",
      "   macro avg       0.61      0.64      0.62        74\n",
      "weighted avg       0.83      0.81      0.82        74\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 73/73 [00:00<00:00, 502765.50it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 362777.48it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 727278.37it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 472506.47it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 235164.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Gini:  \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.7297297297297297\n",
      "Testing F1 score: 0.7356403132104067\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  0  0]\n",
      " [ 9 48  1]\n",
      " [ 9  1  0]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      1.00      0.40         6\n",
      "           1       0.98      0.83      0.90        58\n",
      "           2       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.73        74\n",
      "   macro avg       0.41      0.61      0.43        74\n",
      "weighted avg       0.79      0.73      0.74        74\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Gini: \n",
      "Testing accuracy 0.7297297297297297\n",
      "Testing F1 score: 0.7356403132104067\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  0  0]\n",
      " [ 9 48  1]\n",
      " [ 9  1  0]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      1.00      0.40         6\n",
      "           1       0.98      0.83      0.90        58\n",
      "           2       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.73        74\n",
      "   macro avg       0.41      0.61      0.43        74\n",
      "weighted avg       0.79      0.73      0.74        74\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:00<00:00, 407320.86it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 453107.29it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 787762.68it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 498199.83it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 425759.25it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 326027.83it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 488016.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Gini: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.5616438356164384\n",
      "Testing F1 score: 0.5032872869795769\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 1 13  0]\n",
      " [ 1 34  1]\n",
      " [ 7 10  6]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.07      0.09        14\n",
      "           1       0.60      0.94      0.73        36\n",
      "           2       0.86      0.26      0.40        23\n",
      "\n",
      "    accuracy                           0.56        73\n",
      "   macro avg       0.52      0.43      0.41        73\n",
      "weighted avg       0.59      0.56      0.50        73\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 74/74 [00:00<00:00, 396903.45it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 459819.99it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 407320.86it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 473860.30it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 475311.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Gini:  \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.6301369863013698\n",
      "Testing F1 score: 0.546089131882099\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 0 12  2]\n",
      " [ 0 36  0]\n",
      " [ 0 13 10]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.59      1.00      0.74        36\n",
      "           2       0.83      0.43      0.57        23\n",
      "\n",
      "    accuracy                           0.63        73\n",
      "   macro avg       0.47      0.48      0.44        73\n",
      "weighted avg       0.55      0.63      0.55        73\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Gini: \n",
      "Testing accuracy 0.6301369863013698\n",
      "Testing F1 score: 0.546089131882099\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 0 12  2]\n",
      " [ 0 36  0]\n",
      " [ 0 13 10]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.59      1.00      0.74        36\n",
      "           2       0.83      0.43      0.57        23\n",
      "\n",
      "    accuracy                           0.63        73\n",
      "   macro avg       0.47      0.48      0.44        73\n",
      "weighted avg       0.55      0.63      0.55        73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: 2\\n\\n\")\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using Decision Tree with Gini: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using Decision Tree with Gini:  \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR Paired Model Using Decision Tree with Gini: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbour with Minkowski Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 445035.16it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 533421.94it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 231782.13it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 326770.75it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 479161.49it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 536224.50it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 266247.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Minkowski Distance: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8243243243243243\n",
      "Testing F1 score: 0.8364846135116405\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 3  1  2]\n",
      " [ 2 51  5]\n",
      " [ 2  1  7]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.50      0.46         6\n",
      "           1       0.96      0.88      0.92        58\n",
      "           2       0.50      0.70      0.58        10\n",
      "\n",
      "    accuracy                           0.82        74\n",
      "   macro avg       0.63      0.69      0.65        74\n",
      "weighted avg       0.86      0.82      0.84        74\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 73/73 [00:00<00:00, 383209.25it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 284293.59it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 299300.29it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 324691.61it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 571239.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Minkowski Distance: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.7702702702702703\n",
      "Testing F1 score: 0.8078523608798839\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 4  2  0]\n",
      " [ 9 49  0]\n",
      " [ 6  0  4]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.67      0.32         6\n",
      "           1       0.96      0.84      0.90        58\n",
      "           2       1.00      0.40      0.57        10\n",
      "\n",
      "    accuracy                           0.77        74\n",
      "   macro avg       0.72      0.64      0.60        74\n",
      "weighted avg       0.91      0.77      0.81        74\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Minkowski Distance: \n",
      "Testing accuracy 0.7702702702702703\n",
      "Testing F1 score: 0.8078523608798839\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 4  2  0]\n",
      " [ 9 49  0]\n",
      " [ 6  0  4]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.67      0.32         6\n",
      "           1       0.96      0.84      0.90        58\n",
      "           2       1.00      0.40      0.57        10\n",
      "\n",
      "    accuracy                           0.77        74\n",
      "   macro avg       0.72      0.64      0.60        74\n",
      "weighted avg       0.91      0.77      0.81        74\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:00<00:00, 281139.94it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 278116.93it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 504680.48it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 230422.05it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 350314.33it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 483455.60it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 617054.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Minkowski Distance: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.5753424657534246\n",
      "Testing F1 score: 0.4741337630942788\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 0 13  1]\n",
      " [ 0 36  0]\n",
      " [ 0 17  6]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.55      1.00      0.71        36\n",
      "           2       0.86      0.26      0.40        23\n",
      "\n",
      "    accuracy                           0.58        73\n",
      "   macro avg       0.47      0.42      0.37        73\n",
      "weighted avg       0.54      0.58      0.47        73\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 74/74 [00:00<00:00, 474584.86it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 260603.27it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 292809.90it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 618283.86it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 493447.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Minkowski Distance: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.547945205479452\n",
      "Testing F1 score: 0.43143158211651367\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 0 13  1]\n",
      " [ 0 36  0]\n",
      " [ 0 19  4]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.53      1.00      0.69        36\n",
      "           2       0.80      0.17      0.29        23\n",
      "\n",
      "    accuracy                           0.55        73\n",
      "   macro avg       0.44      0.39      0.33        73\n",
      "weighted avg       0.51      0.55      0.43        73\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Minkowski Distance: \n",
      "Testing accuracy 0.547945205479452\n",
      "Testing F1 score: 0.43143158211651367\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 0 13  1]\n",
      " [ 0 36  0]\n",
      " [ 0 19  4]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.53      1.00      0.69        36\n",
      "           2       0.80      0.17      0.29        23\n",
      "\n",
      "    accuracy                           0.55        73\n",
      "   macro avg       0.44      0.39      0.33        73\n",
      "weighted avg       0.51      0.55      0.43        73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: 2\\n\\n\")\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "        \n",
    "    knn_dbow = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski')\n",
    "    knn_dbow.fit(X_trained, y_trained)\n",
    "    y_pred = knn_dbow.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using KNN with Minkowski Distance: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    knn_dm = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski')\n",
    "    knn_dm.fit(X_trained, y_trained)\n",
    "    y_pred = knn_dm.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using KNN with Minkowski Distance: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    knn_mixed = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski')\n",
    "    knn_mixed.fit(X_trained, y_trained)\n",
    "    y_pred = knn_mixed.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PAIRED Model Using KNN with Minkowski Distance: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbour with Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 196776.47it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 489112.13it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 468888.50it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 219959.91it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 319941.68it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 455631.24it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 283241.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Euclidean Distance: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8108108108108109\n",
      "Testing F1 score: 0.828931203931204\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 3  1  2]\n",
      " [ 5 50  3]\n",
      " [ 2  1  7]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.50      0.37         6\n",
      "           1       0.96      0.86      0.91        58\n",
      "           2       0.58      0.70      0.64        10\n",
      "\n",
      "    accuracy                           0.81        74\n",
      "   macro avg       0.61      0.69      0.64        74\n",
      "weighted avg       0.86      0.81      0.83        74\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 73/73 [00:00<00:00, 493845.47it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 442462.71it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 297555.09it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 298716.28it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 678900.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Euclidean Distance: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.6891891891891891\n",
      "Testing F1 score: 0.7267222547596379\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 3  1  2]\n",
      " [11 47  0]\n",
      " [ 8  1  1]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.50      0.21         6\n",
      "           1       0.96      0.81      0.88        58\n",
      "           2       0.33      0.10      0.15        10\n",
      "\n",
      "    accuracy                           0.69        74\n",
      "   macro avg       0.48      0.47      0.42        74\n",
      "weighted avg       0.81      0.69      0.73        74\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Euclidean Distance: \n",
      "Testing accuracy 0.6891891891891891\n",
      "Testing F1 score: 0.7267222547596379\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 3  1  2]\n",
      " [11 47  0]\n",
      " [ 8  1  1]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.50      0.21         6\n",
      "           1       0.96      0.81      0.88        58\n",
      "           2       0.33      0.10      0.15        10\n",
      "\n",
      "    accuracy                           0.69        74\n",
      "   macro avg       0.48      0.47      0.42        74\n",
      "weighted avg       0.81      0.69      0.73        74\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:00<00:00, 285012.39it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 493447.53it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 379900.24it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 520769.29it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 79482.33it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 387973.12it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 263255.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Euclidean Distance: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.6027397260273972\n",
      "Testing F1 score: 0.5126027397260274\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 0 13  1]\n",
      " [ 0 36  0]\n",
      " [ 0 15  8]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.56      1.00      0.72        36\n",
      "           2       0.89      0.35      0.50        23\n",
      "\n",
      "    accuracy                           0.60        73\n",
      "   macro avg       0.48      0.45      0.41        73\n",
      "weighted avg       0.56      0.60      0.51        73\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 74/74 [00:00<00:00, 364294.01it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 390904.91it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 305490.65it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 367747.03it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 173298.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Euclidean Distance: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.5205479452054794\n",
      "Testing F1 score: 0.3822506721290488\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 0 14  0]\n",
      " [ 0 36  0]\n",
      " [ 0 21  2]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.51      1.00      0.67        36\n",
      "           2       1.00      0.09      0.16        23\n",
      "\n",
      "    accuracy                           0.52        73\n",
      "   macro avg       0.50      0.36      0.28        73\n",
      "weighted avg       0.57      0.52      0.38        73\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Euclidean Distance: \n",
      "Testing accuracy 0.5205479452054794\n",
      "Testing F1 score: 0.3822506721290488\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 0 14  0]\n",
      " [ 0 36  0]\n",
      " [ 0 21  2]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.51      1.00      0.67        36\n",
      "           2       1.00      0.09      0.16        23\n",
      "\n",
      "    accuracy                           0.52        73\n",
      "   macro avg       0.50      0.36      0.28        73\n",
      "weighted avg       0.57      0.52      0.38        73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: 2\\n\\n\")\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "        \n",
    "    knn_dbow = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\n",
    "    knn_dbow.fit(X_trained, y_trained)\n",
    "    y_pred = knn_dbow.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using KNN with Euclidean Distance: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    knn_dm = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\n",
    "    knn_dm.fit(X_trained, y_trained)\n",
    "    y_pred = knn_dm.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using KNN with Euclidean Distance: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    knn_mixed = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\n",
    "    knn_mixed.fit(X_trained, y_trained)\n",
    "    y_pred = knn_mixed.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PAIRED Model Using KNN with Euclidean Distance: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
