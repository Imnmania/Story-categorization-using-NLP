{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['opencv-text-detection.zip',\n",
       " '.ipynb_checkpoints',\n",
       " 'accepted_projects',\n",
       " 'Andrew W. Trask - Grokking Deep Learning-Manning Publications (2019).pdf',\n",
       " 'Collective_Dataset',\n",
       " 'corpus',\n",
       " 'corpus.zip',\n",
       " 'doc2vec.ipynb',\n",
       " 'doc2vec_with_kfold.html',\n",
       " 'doc2vec_with_kfold.ipynb',\n",
       " 'Final Slides and Books',\n",
       " 'google-play-store-apps',\n",
       " 'google-play-store-apps.zip',\n",
       " 'helpline-of-all-sorts',\n",
       " 'helpline-of-all-sorts.zip',\n",
       " 'kmeans_clustering.ipynb',\n",
       " 'labels.txt',\n",
       " 'logistic_regression.ipynb',\n",
       " 'MURA-v1.1',\n",
       " 'MURA-v1.1.zip',\n",
       " 'nltk',\n",
       " 'opencv-text-detection',\n",
       " 'Papers',\n",
       " 'Papers.zip',\n",
       " 'Papers_Association_Rule',\n",
       " 'processed_dataset.csv',\n",
       " 'Results',\n",
       " 'reviews.txt',\n",
       " 'sentiment_analysis_from_story_preprocessing.ipynb',\n",
       " 'sentiment_dictionary.csv',\n",
       " 'sklearn_test.ipynb',\n",
       " 'Slides',\n",
       " 'Story-categorization-using-NLP',\n",
       " 'Tan.pdf',\n",
       " 'Testing',\n",
       " 'Udacity',\n",
       " 'videodata.csv',\n",
       " 'word2vec.model',\n",
       " 'word2vector_test.ipynb']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import multiprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "cores = multiprocessing.cpu_count()\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.getcwd() + \"/Collective_Dataset/419_data - Sheet1.csv\", usecols=[0,1])#header=None, , names=['story', 'category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just like any other day, employees arrived in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My so-called ‘friends’ in middle school used t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i have been called hurtful names and i have be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at my old school kids would hit me and call me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I had debilitating migraines for three years b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I love my work, but hate going each day becaus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I have a chronic illness which was doing well ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The other part is that sense of worthlessness....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I feel my whole body hurting. My mental health...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>As a librarian, I've been threatened with stal...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               story  category\n",
       "0  Just like any other day, employees arrived in ...         0\n",
       "1  My so-called ‘friends’ in middle school used t...         1\n",
       "2  i have been called hurtful names and i have be...         1\n",
       "3  at my old school kids would hit me and call me...         1\n",
       "4  I had debilitating migraines for three years b...         0\n",
       "5  I love my work, but hate going each day becaus...         0\n",
       "6  I have a chronic illness which was doing well ...         0\n",
       "7  The other part is that sense of worthlessness....         0\n",
       "8  I feel my whole body hurting. My mental health...         0\n",
       "9  As a librarian, I've been threatened with stal...         2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Just like any other day, employees arrived in ...\n",
       "1      My so-called ‘friends’ in middle school used t...\n",
       "2      i have been called hurtful names and i have be...\n",
       "3      at my old school kids would hit me and call me...\n",
       "4      I had debilitating migraines for three years b...\n",
       "                             ...                        \n",
       "184    This happened when I was about 23 and working ...\n",
       "185    During the holidays I’d wear a santa hat to wo...\n",
       "186    I’m currently working at Starbucks, and as a 2...\n",
       "187    I’ve had to deal with male customers who stop ...\n",
       "188    I worked at a well-known hardware store and a ...\n",
       "Name: story, Length: 189, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      0\n",
       "      ..\n",
       "184    2\n",
       "185    2\n",
       "186    2\n",
       "187    2\n",
       "188    2\n",
       "Name: category, Length: 189, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = range(189)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35116"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.story.apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnt_pro = df['category'].value_counts()\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('category', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Work Stress Total Data Count:  26\n",
      "Bullying Total Data Count:  94\n",
      "Sexual Harassment Total Data Count:  69\n"
     ]
    }
   ],
   "source": [
    "work_stress_counter, bullying_counter, sexual_harassment_counter = 0, 0, 0\n",
    "for c in df['category']:\n",
    "    if c==0:\n",
    "        work_stress_counter+=1\n",
    "    elif c == 1:\n",
    "        bullying_counter+=1\n",
    "    else:\n",
    "        sexual_harassment_counter+=1\n",
    "print(\"Work Stress Total Data Count: \", work_stress_counter)\n",
    "print(\"Bullying Total Data Count: \", bullying_counter)\n",
    "print(\"Sexual Harassment Total Data Count: \", sexual_harassment_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have been called hurtful names and i have been called black bitch and people are making fun of me for being black by my suppost to be friend t and she has posted rumors about me and i dont know what to do.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(df.story[2])\n",
    "print(df.category[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just like any other day, employees arrived in the workplace sparingly, filling the cubicles and getting their coffees ready. Once more, the manager was already sitting at his desk, grumbling and shouting - You’re way too slow, again! How am I supposed to get my work done with you slowing me down every day? The other employees were staring at each other. They were embarrassed by his outburst but deep inside they knew he was right. Satisfied by the nods in the assembly, the manager calmed down. As usual, he quickly got absorbed by his screen and numerous emails. He was not the moody type, just a normal guy. His team liked him very much and his performance record was exemplary. He was often described as someone caring and trustworthy with a genuine interest in people. Yet once again, his fist hit the desk loudly as he started screaming. That’s it, I’m done! Slow and unreliable old crap! You made me lose five hours of my life, again. World will be better off without you and I’m going to let the board know about that. He stood up suddenly and clenched his fists in anger. He then walked rapidly to the stairs, climbed and disappeared. Nobody saw him again for the day, but they all knew he was going for a confrontation, if not a fight. It was going to get really ugly up there.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df.story[0])\n",
    "print(df.category[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I worked as an office manager, and the only woman, for an industrial insulation company. I had just come back from maternity leave and I was worried about my milk supply. I went into the bathroom to pump for about 15 minutes every two hours, and all of the men in the office would stand in the break area (right in front of the bathroom door) and make baby crying noises to make fun of me. Eventually it progressed to the point that they would make crying noises every time they passed my desk in hopes that I would leak through my shirt. They would also make comments about how much larger my breasts were since having a baby. I felt so harassed and unsafe that I would dread going to work every day, and I even had more than a few nervous breakdowns. My husband was furious and I had to convince him not to take any drastic action so that I could be sure to have a good reference if I needed to find another job. We had a long conversation and looked at our finances and decided the extra money wasn't worth the emotional distress. I ended up quitting my job and staying home with our kids.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(df.story[10])\n",
    "print(df.category[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('x', '')\n",
    "    return text\n",
    "df['story'] = df['story'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH TRAIN_TEST_SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size = .20, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged = train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "test_tagged = test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>i loved my work, but hated going each day beca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>well it all started in year 7, i told one of m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>i’ve always been super flat-chested and i’m st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>it started in year 7, which i think is like 6t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>i was around 12-13 years old and at this “meet...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>i was on my way from gabtoli to mirpur on a bu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hi, i'm rahul. i was bullied all through schoo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>when i was in 7th grade, i had a semester of m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i got made fun of for being fat in elementary,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>let me tell you about a few of my eperiences. ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 story  category\n",
       "140  i loved my work, but hated going each day beca...         0\n",
       "89   well it all started in year 7, i told one of m...         1\n",
       "53   i’ve always been super flat-chested and i’m st...         1\n",
       "112  it started in year 7, which i think is like 6t...         1\n",
       "71   i was around 12-13 years old and at this “meet...         1\n",
       "..                                                 ...       ...\n",
       "43   i was on my way from gabtoli to mirpur on a bu...         2\n",
       "22   hi, i'm rahul. i was bullied all through schoo...         1\n",
       "72   when i was in 7th grade, i had a semester of m...         1\n",
       "15   i got made fun of for being fat in elementary,...         1\n",
       "168  let me tell you about a few of my eperiences. ...         2\n",
       "\n",
       "[151 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>i was born with a craniofacial disease called ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>my school has always been the type to say they...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>when i got admitted in a well reputed college,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i worked as an office manager, and the only wo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>i got beaten up and verbally abused from 1st g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i have been called hurtful names and i have be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>i don’t know how to start this but i really ne...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>i was in class 11. i was coming back from my t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>the first time was the day after my 20th birth...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>my story might be short because i only remembe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>since my childhood, i was very much sincere to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>when i was in elementary school, i was a very ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>when i was 13 or 14, i was babysitting two kid...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>i’m a waitress, and i get harassed daily by cu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>in 1998 i attended a corporate management retr...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at my old school kids would hit me and call me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i have a chronic illness which was doing well ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>my bullying started, or when i first noticed i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>i’ve had customers follow me around the store ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>i had this one particular kid who would pick o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>i met a girl in 7th grade, let's call her a, a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>it was the late ‘60s, and apparently one of my...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>an office boy had harassed me multiple times. ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>and im 14 years old. all my life, i been bulli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>when i attended a parochial school, a group of...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>back in 2011 when i was in the 1st year of my ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>as a librarian, i've been threatened with stal...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>as a final year student, completing the final ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>i had everything going for me; being popular, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i work in law enforcement. i started out as a ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>senior year at muhlenberg, my roommate and i w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>i work at subway. one time, a man told my cowo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>ever since first grade, my three best friends ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>in brief it is hardly possible to describe my ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>i was 14 years old at that time and went to vi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>i was most definitely bullied in my high schoo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>my first job is the lehigh valley (mid-’70s) w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>i was bullied since age 8. it all started in 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 story  category\n",
       "23   i was born with a craniofacial disease called ...         1\n",
       "78   my school has always been the type to say they...         1\n",
       "35   when i got admitted in a well reputed college,...         1\n",
       "10   i worked as an office manager, and the only wo...         2\n",
       "66   i got beaten up and verbally abused from 1st g...         1\n",
       "2    i have been called hurtful names and i have be...         1\n",
       "138  i don’t know how to start this but i really ne...         2\n",
       "29   i was in class 11. i was coming back from my t...         1\n",
       "128  the first time was the day after my 20th birth...         2\n",
       "85   my story might be short because i only remembe...         1\n",
       "45   since my childhood, i was very much sincere to...         0\n",
       "79   when i was in elementary school, i was a very ...         1\n",
       "12   when i was 13 or 14, i was babysitting two kid...         2\n",
       "14   i’m a waitress, and i get harassed daily by cu...         2\n",
       "157  in 1998 i attended a corporate management retr...         2\n",
       "3    at my old school kids would hit me and call me...         1\n",
       "6    i have a chronic illness which was doing well ...         0\n",
       "25   my bullying started, or when i first noticed i...         1\n",
       "176  i’ve had customers follow me around the store ...         2\n",
       "64   i had this one particular kid who would pick o...         1\n",
       "109  i met a girl in 7th grade, let's call her a, a...         1\n",
       "159  it was the late ‘60s, and apparently one of my...         2\n",
       "125  an office boy had harassed me multiple times. ...         2\n",
       "115  and im 14 years old. all my life, i been bulli...         1\n",
       "169  when i attended a parochial school, a group of...         2\n",
       "57   back in 2011 when i was in the 1st year of my ...         1\n",
       "9    as a librarian, i've been threatened with stal...         2\n",
       "44   as a final year student, completing the final ...         0\n",
       "106  i had everything going for me; being popular, ...         1\n",
       "13   i work in law enforcement. i started out as a ...         2\n",
       "163  senior year at muhlenberg, my roommate and i w...         2\n",
       "171  i work at subway. one time, a man told my cowo...         2\n",
       "54   ever since first grade, my three best friends ...         1\n",
       "48   in brief it is hardly possible to describe my ...         0\n",
       "41   i was 14 years old at that time and went to vi...         2\n",
       "65   i was most definitely bullied in my high schoo...         1\n",
       "153  my first job is the lehigh valley (mid-’70s) w...         2\n",
       "92   i was bullied since age 8. it all started in 2...         1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140    ([loved, my, work, but, hated, going, each, da...\n",
       "89     ([well, it, all, started, in, year, told, one,...\n",
       "53     ([ve, always, been, super, flat-chested, and, ...\n",
       "112    ([it, started, in, year, which, think, is, lik...\n",
       "71     ([was, around, 12-13, years, old, and, at, thi...\n",
       "                             ...                        \n",
       "43     ([was, on, my, way, from, gabtoli, to, mirpur,...\n",
       "22     ([hi, 'm, rahul, was, bullied, all, through, s...\n",
       "72     ([when, was, in, 7th, grade, had, semester, of...\n",
       "15     ([got, made, fun, of, for, being, fat, in, ele...\n",
       "168    ([let, me, tell, you, about, few, of, my, eper...\n",
       "Length: 151, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['ve', 'always', 'been', 'super', 'flat-chested', 'and', 'still', 'totally', 'traumatized', 'by', 'something', 'that', 'happened', 'in', 'seventh', 'grade', 'these', 'two', 'guys', 'came', 'up', 'to', 'me', 'and', 'asked', 'if', 'they', 'could', 'call', 'me', 'stingray', 'because', 'you', 're', 'really', 'flat.', 'pretty', 'sure', 'the', 'name', 'continued', 'for', 'the', 'rest', 'of', 'that', 'school', 'year'], tags=[1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged.values[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['my', 'school', 'has', 'always', 'been', 'the', 'type', 'to', 'say', 'they', 'would', 'do', 'everything', 'they', 'could', 'to', 'prevent', 'bullying', 'tons', 'of', 'assembly', 'about', 'anti-bullying', 'but', 'in', '7th', 'grade', 'felt', 'like', 'would', 'be', 'better', 'off', 'dead', 'this', 'one', 'girl', 'and', 'were', 'friends', 'kinda', 'we', 'laughed', 'together', 'but', 'she', 'didn', 'know', 'the', 'limits', 'she', 'started', 'hurting', 'me', 'physically', 'and', 'mentally', 'she', 'hit', 'me', 'square', 'in', 'the', 'stomach', 'one', 'day', 'that', 'when', 'realized', 'that', 'this', 'wasn', 'joke', 'to', 'her', 'she', 'kicked', 'me', 'she', 'slapped', 'me', 'left', 'red', 'marks', 'on', 'my', 'face', 'she', 'would', 'even', 'go', 'as', 'far', 'as', 'grabbing', 'my', 'hand', 'with', 'such', 'powerful', 'grip', 'that', 'couldn', 'get', 'away', 'and', 'she', 'would', 'dig', 'three', 'nails', 'into', 'my', 'wrist', 'leaving', 'purple', 'mark', 'afterwards', 'one', 'time', 'started', 'bleeding', 'she', 'left', 'scar', 'there', 'for', 'two', 'weeks', 'at', 'time', 'was', 'upset', 'didn', 'know', 'what', 'to', 'do', 'some', 'of', 'my', 'friends', 'saw', 'it', 'too', 'but', 'did', 'nothing', 'felt', 'hopeless', 'she', 'decided', 'to', 'go', 'to', 'our', 'schools', 'assistant', 'principal', 'and', 'twist', 'the', 'story', 'to', 'say', 'that', 'was', 'hurting', 'her', 'and', 'causing', 'her', 'harm', 'thought', 'that', 'maybe', 'this', 'would', 'be', 'the', 'way', 'out', 'of', 'this', 'by', 'telling', 'the', 'assistant', 'principal', 'that', 'it', 'was', 'the', 'other', 'way', 'around', 'told', 'him', 'about', 'the', 'marks', 'on', 'me', 'and', 'showed', 'him', 'the', 'mark', 'and', 'he', 'even', 'called', 'in', 'some', 'of', 'my', 'friends', 'to', 'interview', 'them', 'on', 'this', 'they', 'said', 'that', 'it', 'was', 'true', 'thought', 'yes', 'will', 'finally', 'be', 'free', 'of', 'her.', 'it', 'was', 'dream', 'come', 'true', 'or', 'so', 'thought', 'it', 'would', 'be', 'the', 'assistant', 'ended', 'up', 'just', 'telling', 'us', 'to', 'keep', 'our', 'distance', 'was', 'heartbroken', 'even', 'resorted', 'to', 'self', 'harm', 'she', 'got', 'nothing', 'for', 'my', 'times', 'spent', 'crying', 'in', 'the', 'shower', 'or', 'random', 'breakdowns', 'my', 'separation', 'from', 'the', 'outside', 'world', 'have', 'boyfriend', 'now', 'who', 'is', 'the', 'light', 'of', 'my', 'world', 'always', 'there', 'for', 'me', 'he', 'understands', 'me', 'grateful', 'but', 'still', 'want', 'this', 'girl', 'to', 'at', 'least', 'understand', 'that', 'she', 'hurt', 'me', 'even', 'though', 'it', 'been', 'while', 'long', 'time', 'still', 'remember', 'it', 'like', 'it', 'was', 'yesterday', 'don', 'want', 'this', 'girl', 'to', 'die', 'or', 'anything', 'never', 'but', 'want', 'her', 'to', 'have', 'little', 'understanding', 'of', 'what', 'felt', 'like', 'if', 'she', 'has', 'genuine', 'apology', 'would', 'start', 'taking', 'baby', 'steps', 'to', 'forgiving', 'her'], tags=[1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tagged.values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PV-DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:00<00:00, 161731.33it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:00<00:00, 481627.30it/s]\n",
      "100%|██████████| 151/151 [00:00<00:00, 597490.48it/s]\n",
      "100%|██████████| 151/151 [00:00<00:00, 953825.16it/s]\n",
      "100%|██████████| 151/151 [00:00<00:00, 574197.56it/s]\n",
      "100%|██████████| 151/151 [00:00<00:00, 559982.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 249 ms, sys: 2.89 ms, total: 252 ms\n",
      "Wall time: 105 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(5):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.8421052631578947\n",
      "Testing F1 score: 0.8434847803268856\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 3  1  0]\n",
      " [ 0 18  0]\n",
      " [ 3  2 11]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.75      0.60         4\n",
      "           1       0.86      1.00      0.92        18\n",
      "           2       1.00      0.69      0.81        16\n",
      "\n",
      "    accuracy                           0.84        38\n",
      "   macro avg       0.79      0.81      0.78        38\n",
      "weighted avg       0.88      0.84      0.84        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "print('\\nTesting Confusion Matrix: ')\n",
    "print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "print('Testing Classification Report: ')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PV-DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:00<00:00, 838860.80it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:00<00:00, 641681.77it/s]\n",
      "100%|██████████| 151/151 [00:00<00:00, 556049.08it/s]\n",
      "100%|██████████| 151/151 [00:00<00:00, 907363.76it/s]\n",
      "100%|██████████| 151/151 [00:00<00:00, 607811.81it/s]\n",
      "100%|██████████| 151/151 [00:00<00:00, 654276.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 308 ms, sys: 20.6 ms, total: 328 ms\n",
      "Wall time: 158 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(5):\n",
    "    model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dmm.alpha -= 0.002\n",
    "    model_dmm.min_alpha = model_dmm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dmm, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dmm, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.8947368421052632\n",
      "Testing F1 score: 0.8975988357991752\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 4  0  0]\n",
      " [ 1 16  1]\n",
      " [ 1  1 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         4\n",
      "           1       0.94      0.89      0.91        18\n",
      "           2       0.93      0.88      0.90        16\n",
      "\n",
      "    accuracy                           0.89        38\n",
      "   macro avg       0.85      0.92      0.87        38\n",
      "weighted avg       0.91      0.89      0.90        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "print('\\nTesting Confusion Matrix: ')\n",
    "print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "print('Testing Classification Report: ')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = get_vectors(new_model, train_tagged)\n",
    "y_test, X_test = get_vectors(new_model, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.8947368421052632\n",
      "Testing F1 score: 0.8969833717565115\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 4  0  0]\n",
      " [ 1 17  0]\n",
      " [ 1  2 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         4\n",
      "           1       0.89      0.94      0.92        18\n",
      "           2       1.00      0.81      0.90        16\n",
      "\n",
      "    accuracy                           0.89        38\n",
      "   macro avg       0.85      0.92      0.87        38\n",
      "weighted avg       0.92      0.89      0.90        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "print('\\nTesting Confusion Matrix: ')\n",
    "print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "print('Testing Classification Report: ')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH K-FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  189\n",
      "Type:  <class 'pandas.core.series.Series'>\n",
      "First Ten Values:\n",
      " 0    just like any other day, employees arrived in ...\n",
      "1    my so-called ‘friends’ in middle school used t...\n",
      "2    i have been called hurtful names and i have be...\n",
      "3    at my old school kids would hit me and call me...\n",
      "4    i had debilitating migraines for three years b...\n",
      "5    i love my work, but hate going each day becaus...\n",
      "6    i have a chronic illness which was doing well ...\n",
      "7    the other part is that sense of worthlessness....\n",
      "8    i feel my whole body hurting. my mental health...\n",
      "9    as a librarian, i've been threatened with stal...\n",
      "Name: story, dtype: object\n",
      "just like any other day, employees arrived in the workplace sparingly, filling the cubicles and getting their coffees ready. once more, the manager was already sitting at his desk, grumbling and shouting - you’re way too slow, again! how am i supposed to get my work done with you slowing me down every day? the other employees were staring at each other. they were embarrassed by his outburst but deep inside they knew he was right. satisfied by the nods in the assembly, the manager calmed down. as usual, he quickly got absorbed by his screen and numerous emails. he was not the moody type, just a normal guy. his team liked him very much and his performance record was eemplary. he was often described as someone caring and trustworthy with a genuine interest in people. yet once again, his fist hit the desk loudly as he started screaming. that’s it, i’m done! slow and unreliable old crap! you made me lose five hours of my life, again. world will be better off without you and i’m going to let the board know about that. he stood up suddenly and clenched his fists in anger. he then walked rapidly to the stairs, climbed and disappeared. nobody saw him again for the day, but they all knew he was going for a confrontation, if not a fight. it was going to get really ugly up there.\n"
     ]
    }
   ],
   "source": [
    "X = df['story']\n",
    "print(\"Length: \", len(X))\n",
    "print(\"Type: \", type(X))\n",
    "print(\"First Ten Values:\\n\", X[:10])\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  189\n",
      "Type:  <class 'pandas.core.series.Series'>\n",
      "First Ten Values:  0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "8    0\n",
      "9    2\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y = df['category']\n",
    "print(\"Length: \", len(y))\n",
    "print(\"Type: \", type(y))\n",
    "print(\"First Ten Values: \", y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=2, random_state=None, shuffle=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten_fold = KFold(n_splits=2)\n",
    "ten_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten_fold.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-FOLD PV-DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Fold No.:  [ 95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112\n",
      " 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130\n",
      " 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148\n",
      " 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166\n",
      " 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184\n",
      " 185 186 187 188]  Test Fold No.:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:00<00:00, 278435.44it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 811243.98it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 272470.34it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 284462.18it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 401900.69it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 529214.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  1\n",
      "Testing accuracy 0.7263157894736842\n",
      "Testing F1 score: 0.7465149359886202\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  0  1]\n",
      " [12 44 11]\n",
      " [ 2  0 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.92      0.59        12\n",
      "           1       1.00      0.66      0.79        67\n",
      "           2       0.54      0.88      0.67        16\n",
      "\n",
      "    accuracy                           0.73        95\n",
      "   macro avg       0.66      0.82      0.68        95\n",
      "weighted avg       0.85      0.73      0.75        95\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Fold No.:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94]  Test Fold No.:  [ 95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112\n",
      " 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130\n",
      " 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148\n",
      " 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166\n",
      " 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184\n",
      " 185 186 187 188]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:00<00:00, 340563.15it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 554184.81it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 359295.65it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 316739.97it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 753230.40it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 257902.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  2\n",
      "Testing accuracy 0.5957446808510638\n",
      "Testing F1 score: 0.5916427065363236\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  6  0]\n",
      " [ 0 27  0]\n",
      " [ 2 30 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.57      0.67        14\n",
      "           1       0.43      1.00      0.60        27\n",
      "           2       1.00      0.40      0.57        53\n",
      "\n",
      "    accuracy                           0.60        94\n",
      "   macro avg       0.74      0.66      0.61        94\n",
      "weighted avg       0.81      0.60      0.59        94\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "    print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg.fit(X_trained, y_trained)\n",
    "    y_pred = logreg.predict(X_tested)\n",
    "    \n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-FOLD PV-DM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Fold No.:  [ 95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112\n",
      " 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130\n",
      " 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148\n",
      " 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166\n",
      " 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184\n",
      " 185 186 187 188]  Test Fold No.:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:00<00:00, 297782.91it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 317955.30it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 291185.06it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 316170.47it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 205346.13it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 892001.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  1\n",
      "Testing accuracy 0.8\n",
      "Testing F1 score: 0.8161223571749888\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  0  2]\n",
      " [ 3 50 14]\n",
      " [ 0  0 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.80        12\n",
      "           1       1.00      0.75      0.85        67\n",
      "           2       0.50      1.00      0.67        16\n",
      "\n",
      "    accuracy                           0.80        95\n",
      "   macro avg       0.76      0.86      0.77        95\n",
      "weighted avg       0.89      0.80      0.82        95\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Fold No.:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94]  Test Fold No.:  [ 95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112\n",
      " 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130\n",
      " 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148\n",
      " 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166\n",
      " 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184\n",
      " 185 186 187 188]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:00<00:00, 299143.30it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 331496.57it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 409515.81it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 274043.25it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 879600.18it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 699050.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  2\n",
      "Testing accuracy 0.5851063829787234\n",
      "Testing F1 score: 0.5752574494063856\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  5  0]\n",
      " [ 0 27  0]\n",
      " [ 2 32 19]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.64      0.72        14\n",
      "           1       0.42      1.00      0.59        27\n",
      "           2       1.00      0.36      0.53        53\n",
      "\n",
      "    accuracy                           0.59        94\n",
      "   macro avg       0.75      0.67      0.61        94\n",
      "weighted avg       0.81      0.59      0.58        94\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "    print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg.fit(X_trained, y_trained)\n",
    "    y_pred = logreg.predict(X_tested)\n",
    "    \n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n\\n\")\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAIRED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:00<00:00, 523591.73it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 265856.09it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 573893.12it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 410264.91it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 228957.36it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 434212.09it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 811243.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.7368421052631579\n",
      "Testing F1 score: 0.7533585807270018\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  0  1]\n",
      " [11 44 12]\n",
      " [ 1  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.92      0.63        12\n",
      "           1       1.00      0.66      0.79        67\n",
      "           2       0.54      0.94      0.68        16\n",
      "\n",
      "    accuracy                           0.74        95\n",
      "   macro avg       0.67      0.84      0.70        95\n",
      "weighted avg       0.86      0.74      0.75        95\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 94/94 [00:00<00:00, 594667.54it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 860839.69it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 529214.20it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 342838.76it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 405205.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8105263157894737\n",
      "Testing F1 score: 0.8223390069809394\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  1  2]\n",
      " [ 3 53 11]\n",
      " [ 0  1 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75        12\n",
      "           1       0.96      0.79      0.87        67\n",
      "           2       0.54      0.94      0.68        16\n",
      "\n",
      "    accuracy                           0.81        95\n",
      "   macro avg       0.75      0.83      0.77        95\n",
      "weighted avg       0.86      0.81      0.82        95\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model: \n",
      "Testing accuracy 0.7789473684210526\n",
      "Testing F1 score: 0.7926801168180478\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  1  1]\n",
      " [ 6 49 12]\n",
      " [ 1  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.83      0.69        12\n",
      "           1       0.98      0.73      0.84        67\n",
      "           2       0.54      0.94      0.68        16\n",
      "\n",
      "    accuracy                           0.78        95\n",
      "   macro avg       0.70      0.83      0.74        95\n",
      "weighted avg       0.86      0.78      0.79        95\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:00<00:00, 757526.39it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 596495.33it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 255750.24it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 746177.68it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 601901.63it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 266171.60it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 883500.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.6276595744680851\n",
      "Testing F1 score: 0.6311803444782168\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  6  1]\n",
      " [ 1 26  0]\n",
      " [ 2 25 26]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.50      0.58        14\n",
      "           1       0.46      0.96      0.62        27\n",
      "           2       0.96      0.49      0.65        53\n",
      "\n",
      "    accuracy                           0.63        94\n",
      "   macro avg       0.71      0.65      0.62        94\n",
      "weighted avg       0.78      0.63      0.63        94\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 95/95 [00:00<00:00, 357683.02it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 400461.19it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 754656.97it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 386478.06it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 334277.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.648936170212766\n",
      "Testing F1 score: 0.6467945128779395\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  4  1]\n",
      " [ 1 26  0]\n",
      " [ 8 19 26]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.64      0.56        14\n",
      "           1       0.53      0.96      0.68        27\n",
      "           2       0.96      0.49      0.65        53\n",
      "\n",
      "    accuracy                           0.65        94\n",
      "   macro avg       0.66      0.70      0.63        94\n",
      "weighted avg       0.77      0.65      0.65        94\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model: \n",
      "Testing accuracy 0.44680851063829785\n",
      "Testing F1 score: 0.35802354242846085\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  4  0]\n",
      " [ 1 26  0]\n",
      " [11 36  6]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.71      0.56        14\n",
      "           1       0.39      0.96      0.56        27\n",
      "           2       1.00      0.11      0.20        53\n",
      "\n",
      "    accuracy                           0.45        94\n",
      "   macro avg       0.62      0.60      0.44        94\n",
      "weighted avg       0.74      0.45      0.36        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: 2\\n\\n\")\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    logreg_dbow = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg_dbow.fit(X_trained, y_trained)\n",
    "    y_pred = logreg_dbow.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    logreg_dm = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg_dm.fit(X_trained, y_trained)\n",
    "    y_pred = logreg_dm.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    logreg_paired = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg_paired.fit(X_train, y_train)\n",
    "    y_pred = logreg_paired.predict(X_test)\n",
    "    \n",
    "    print(\"FOR Paired Model: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:00<00:00, 577254.14it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 589334.19it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 334974.15it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 716844.68it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 427155.55it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 390360.97it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 441505.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Entropy: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.6631578947368421\n",
      "Testing F1 score: 0.6828198117671802\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  0  1]\n",
      " [18 37 12]\n",
      " [ 1  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.92      0.52        12\n",
      "           1       1.00      0.55      0.71        67\n",
      "           2       0.54      0.94      0.68        16\n",
      "\n",
      "    accuracy                           0.66        95\n",
      "   macro avg       0.63      0.80      0.64        95\n",
      "weighted avg       0.84      0.66      0.68        95\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 94/94 [00:00<00:00, 281818.85it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 316932.94it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 422124.81it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 432782.19it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 667114.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Entropy:  \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.6736842105263158\n",
      "Testing F1 score: 0.7023572275141565\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  0  5]\n",
      " [13 42 12]\n",
      " [ 1  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.58      0.42        12\n",
      "           1       1.00      0.63      0.77        67\n",
      "           2       0.47      0.94      0.62        16\n",
      "\n",
      "    accuracy                           0.67        95\n",
      "   macro avg       0.60      0.72      0.61        95\n",
      "weighted avg       0.83      0.67      0.70        95\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Entropy: \n",
      "Testing accuracy 0.6736842105263158\n",
      "Testing F1 score: 0.7023572275141565\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  0  5]\n",
      " [13 42 12]\n",
      " [ 1  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.58      0.42        12\n",
      "           1       1.00      0.63      0.77        67\n",
      "           2       0.47      0.94      0.62        16\n",
      "\n",
      "    accuracy                           0.67        95\n",
      "   macro avg       0.60      0.72      0.61        95\n",
      "weighted avg       0.83      0.67      0.70        95\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:00<00:00, 490712.91it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 335121.01it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 359295.65it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 479493.24it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 498696.97it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 428911.60it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 340854.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Entropy: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.5957446808510638\n",
      "Testing F1 score: 0.5799538579851321\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  2  2]\n",
      " [ 1 26  0]\n",
      " [ 5 28 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.71      0.67        14\n",
      "           1       0.46      0.96      0.63        27\n",
      "           2       0.91      0.38      0.53        53\n",
      "\n",
      "    accuracy                           0.60        94\n",
      "   macro avg       0.67      0.68      0.61        94\n",
      "weighted avg       0.74      0.60      0.58        94\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 95/95 [00:00<00:00, 326605.64it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 597389.63it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 424343.86it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 610197.37it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 519503.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Entropy:  \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.3404255319148936\n",
      "Testing F1 score: 0.24422521515331494\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 2 12  0]\n",
      " [ 0 26  1]\n",
      " [ 2 47  4]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.14      0.22        14\n",
      "           1       0.31      0.96      0.46        27\n",
      "           2       0.80      0.08      0.14        53\n",
      "\n",
      "    accuracy                           0.34        94\n",
      "   macro avg       0.54      0.39      0.27        94\n",
      "weighted avg       0.61      0.34      0.24        94\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Entropy: \n",
      "Testing accuracy 0.3404255319148936\n",
      "Testing F1 score: 0.24422521515331494\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 2 12  0]\n",
      " [ 0 26  1]\n",
      " [ 2 47  4]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.14      0.22        14\n",
      "           1       0.31      0.96      0.46        27\n",
      "           2       0.80      0.08      0.14        53\n",
      "\n",
      "    accuracy                           0.34        94\n",
      "   macro avg       0.54      0.39      0.27        94\n",
      "weighted avg       0.61      0.34      0.24        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: 2\\n\\n\")\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"entropy\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using Decision Tree with Entropy: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"entropy\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using Decision Tree with Entropy:  \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"entropy\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR Paired Model Using Decision Tree with Entropy: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:00<00:00, 283032.72it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 807919.21it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 714247.42it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 419877.08it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 248121.19it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 342838.76it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 816282.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Gini: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.6210526315789474\n",
      "Testing F1 score: 0.6621756880890008\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  0  1]\n",
      " [23 37  7]\n",
      " [ 5  0 11]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.92      0.43        12\n",
      "           1       1.00      0.55      0.71        67\n",
      "           2       0.58      0.69      0.63        16\n",
      "\n",
      "    accuracy                           0.62        95\n",
      "   macro avg       0.62      0.72      0.59        95\n",
      "weighted avg       0.84      0.62      0.66        95\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 94/94 [00:00<00:00, 617969.55it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 411979.70it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 659305.31it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 417653.15it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 435170.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Gini:  \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.7157894736842105\n",
      "Testing F1 score: 0.7408612440191387\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  0  2]\n",
      " [17 43  7]\n",
      " [ 1  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.83      0.50        12\n",
      "           1       1.00      0.64      0.78        67\n",
      "           2       0.62      0.94      0.75        16\n",
      "\n",
      "    accuracy                           0.72        95\n",
      "   macro avg       0.66      0.80      0.68        95\n",
      "weighted avg       0.86      0.72      0.74        95\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Gini: \n",
      "Testing accuracy 0.7157894736842105\n",
      "Testing F1 score: 0.7408612440191387\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  0  2]\n",
      " [17 43  7]\n",
      " [ 1  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.83      0.50        12\n",
      "           1       1.00      0.64      0.78        67\n",
      "           2       0.62      0.94      0.75        16\n",
      "\n",
      "    accuracy                           0.72        95\n",
      "   macro avg       0.66      0.80      0.68        95\n",
      "weighted avg       0.86      0.72      0.74        95\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:00<00:00, 548841.43it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 298024.59it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 562001.24it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 306271.24it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 630472.91it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 342318.63it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 374139.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Gini: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.5\n",
      "Testing F1 score: 0.4675960802500557\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  6  0]\n",
      " [ 1 26  0]\n",
      " [ 4 36 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.57      0.59        14\n",
      "           1       0.38      0.96      0.55        27\n",
      "           2       1.00      0.25      0.39        53\n",
      "\n",
      "    accuracy                           0.50        94\n",
      "   macro avg       0.67      0.59      0.51        94\n",
      "weighted avg       0.77      0.50      0.47        94\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 95/95 [00:00<00:00, 607406.83it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 374139.79it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 453825.60it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 633479.94it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 760417.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Gini:  \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.44680851063829785\n",
      "Testing F1 score: 0.39518238853345244\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  5  3]\n",
      " [ 1 26  0]\n",
      " [ 5 38 10]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.43      0.46        14\n",
      "           1       0.38      0.96      0.54        27\n",
      "           2       0.77      0.19      0.30        53\n",
      "\n",
      "    accuracy                           0.45        94\n",
      "   macro avg       0.55      0.53      0.44        94\n",
      "weighted avg       0.62      0.45      0.40        94\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Gini: \n",
      "Testing accuracy 0.44680851063829785\n",
      "Testing F1 score: 0.39518238853345244\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  5  3]\n",
      " [ 1 26  0]\n",
      " [ 5 38 10]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.43      0.46        14\n",
      "           1       0.38      0.96      0.54        27\n",
      "           2       0.77      0.19      0.30        53\n",
      "\n",
      "    accuracy                           0.45        94\n",
      "   macro avg       0.55      0.53      0.44        94\n",
      "weighted avg       0.62      0.45      0.40        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: 2\\n\\n\")\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using Decision Tree with Gini: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using Decision Tree with Gini:  \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR Paired Model Using Decision Tree with Gini: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbour with Minkowski Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:00<00:00, 385024.00it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 324497.59it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 656014.27it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 446000.65it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 407298.12it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 431834.15it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 321324.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Minkowski Distance: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.6631578947368421\n",
      "Testing F1 score: 0.7026781930672091\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  0  2]\n",
      " [20 41  6]\n",
      " [ 4  0 12]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.83      0.43        12\n",
      "           1       1.00      0.61      0.76        67\n",
      "           2       0.60      0.75      0.67        16\n",
      "\n",
      "    accuracy                           0.66        95\n",
      "   macro avg       0.63      0.73      0.62        95\n",
      "weighted avg       0.84      0.66      0.70        95\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 94/94 [00:00<00:00, 312908.39it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 625816.79it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 261795.87it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 386913.22it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 369161.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Minkowski Distance: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.6105263157894737\n",
      "Testing F1 score: 0.6704878709304634\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  0  1]\n",
      " [32 35  0]\n",
      " [ 4  0 12]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.92      0.37        12\n",
      "           1       1.00      0.52      0.69        67\n",
      "           2       0.92      0.75      0.83        16\n",
      "\n",
      "    accuracy                           0.61        95\n",
      "   macro avg       0.72      0.73      0.63        95\n",
      "weighted avg       0.89      0.61      0.67        95\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Minkowski Distance: \n",
      "Testing accuracy 0.6105263157894737\n",
      "Testing F1 score: 0.6704878709304634\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  0  1]\n",
      " [32 35  0]\n",
      " [ 4  0 12]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.92      0.37        12\n",
      "           1       1.00      0.52      0.69        67\n",
      "           2       0.92      0.75      0.83        16\n",
      "\n",
      "    accuracy                           0.61        95\n",
      "   macro avg       0.72      0.73      0.63        95\n",
      "weighted avg       0.89      0.61      0.67        95\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:00<00:00, 271429.75it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 57143.11it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 351684.80it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 380208.85it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 527062.01it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 304399.45it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 471548.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Minkowski Distance: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.6170212765957447\n",
      "Testing F1 score: 0.6122699819207076\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  3  3]\n",
      " [ 0 27  0]\n",
      " [ 1 29 23]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.57      0.70        14\n",
      "           1       0.46      1.00      0.63        27\n",
      "           2       0.88      0.43      0.58        53\n",
      "\n",
      "    accuracy                           0.62        94\n",
      "   macro avg       0.74      0.67      0.64        94\n",
      "weighted avg       0.76      0.62      0.61        94\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 95/95 [00:00<00:00, 635500.61it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 212624.80it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 263009.16it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 311296.00it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 331221.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Minkowski Distance: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.40425531914893614\n",
      "Testing F1 score: 0.3387668725692061\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 3 10  1]\n",
      " [ 0 27  0]\n",
      " [ 1 44  8]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.21      0.33        14\n",
      "           1       0.33      1.00      0.50        27\n",
      "           2       0.89      0.15      0.26        53\n",
      "\n",
      "    accuracy                           0.40        94\n",
      "   macro avg       0.66      0.46      0.36        94\n",
      "weighted avg       0.71      0.40      0.34        94\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Minkowski Distance: \n",
      "Testing accuracy 0.40425531914893614\n",
      "Testing F1 score: 0.3387668725692061\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 3 10  1]\n",
      " [ 0 27  0]\n",
      " [ 1 44  8]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.21      0.33        14\n",
      "           1       0.33      1.00      0.50        27\n",
      "           2       0.89      0.15      0.26        53\n",
      "\n",
      "    accuracy                           0.40        94\n",
      "   macro avg       0.66      0.46      0.36        94\n",
      "weighted avg       0.71      0.40      0.34        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: 2\\n\\n\")\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "        \n",
    "    knn_dbow = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski')\n",
    "    knn_dbow.fit(X_trained, y_trained)\n",
    "    y_pred = knn_dbow.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using KNN with Minkowski Distance: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    knn_dm = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski')\n",
    "    knn_dm.fit(X_trained, y_trained)\n",
    "    y_pred = knn_dm.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using KNN with Minkowski Distance: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    knn_mixed = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski')\n",
    "    knn_mixed.fit(X_trained, y_trained)\n",
    "    y_pred = knn_mixed.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PAIRED Model Using KNN with Minkowski Distance: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbour with Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:00<00:00, 603774.24it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 355834.45it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 408564.33it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 868424.18it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 438071.75it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 443991.64it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 802982.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Euclidean Distance: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.6421052631578947\n",
      "Testing F1 score: 0.6923286711402161\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  0  1]\n",
      " [27 38  2]\n",
      " [ 4  0 12]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.92      0.41        12\n",
      "           1       1.00      0.57      0.72        67\n",
      "           2       0.80      0.75      0.77        16\n",
      "\n",
      "    accuracy                           0.64        95\n",
      "   macro avg       0.69      0.74      0.64        95\n",
      "weighted avg       0.87      0.64      0.69        95\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 94/94 [00:00<00:00, 307538.67it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 214041.57it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 500335.76it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 406877.79it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 648461.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Euclidean Distance: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.6526315789473685\n",
      "Testing F1 score: 0.6919933379932862\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  1  3]\n",
      " [24 39  4]\n",
      " [ 1  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.67      0.36        12\n",
      "           1       0.97      0.58      0.73        67\n",
      "           2       0.68      0.94      0.79        16\n",
      "\n",
      "    accuracy                           0.65        95\n",
      "   macro avg       0.63      0.73      0.62        95\n",
      "weighted avg       0.83      0.65      0.69        95\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Euclidean Distance: \n",
      "Testing accuracy 0.6526315789473685\n",
      "Testing F1 score: 0.6919933379932862\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  1  3]\n",
      " [24 39  4]\n",
      " [ 1  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.67      0.36        12\n",
      "           1       0.97      0.58      0.73        67\n",
      "           2       0.68      0.94      0.79        16\n",
      "\n",
      "    accuracy                           0.65        95\n",
      "   macro avg       0.63      0.73      0.62        95\n",
      "weighted avg       0.83      0.65      0.69        95\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:00<00:00, 369285.34it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 279816.63it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 347999.02it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 328490.42it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 387229.23it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 380208.85it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 485925.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Euclidean Distance: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.6702127659574468\n",
      "Testing F1 score: 0.6718910856615883\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 4 10  0]\n",
      " [ 0 27  0]\n",
      " [ 1 20 32]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.29      0.42        14\n",
      "           1       0.47      1.00      0.64        27\n",
      "           2       1.00      0.60      0.75        53\n",
      "\n",
      "    accuracy                           0.67        94\n",
      "   macro avg       0.76      0.63      0.61        94\n",
      "weighted avg       0.82      0.67      0.67        94\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 95/95 [00:00<00:00, 306271.24it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 314242.02it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 424343.86it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 442240.71it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 604641.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Euclidean Distance: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.5212765957446809\n",
      "Testing F1 score: 0.49327524990924465\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 2  6  6]\n",
      " [ 0 26  1]\n",
      " [ 2 30 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.14      0.22        14\n",
      "           1       0.42      0.96      0.58        27\n",
      "           2       0.75      0.40      0.52        53\n",
      "\n",
      "    accuracy                           0.52        94\n",
      "   macro avg       0.56      0.50      0.44        94\n",
      "weighted avg       0.62      0.52      0.49        94\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Euclidean Distance: \n",
      "Testing accuracy 0.5212765957446809\n",
      "Testing F1 score: 0.49327524990924465\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 2  6  6]\n",
      " [ 0 26  1]\n",
      " [ 2 30 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.14      0.22        14\n",
      "           1       0.42      0.96      0.58        27\n",
      "           2       0.75      0.40      0.52        53\n",
      "\n",
      "    accuracy                           0.52        94\n",
      "   macro avg       0.56      0.50      0.44        94\n",
      "weighted avg       0.62      0.52      0.49        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: 2\\n\\n\")\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "        \n",
    "    knn_dbow = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\n",
    "    knn_dbow.fit(X_trained, y_trained)\n",
    "    y_pred = knn_dbow.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using KNN with Euclidean Distance: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    knn_dm = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\n",
    "    knn_dm.fit(X_trained, y_trained)\n",
    "    y_pred = knn_dm.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using KNN with Euclidean Distance: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    knn_mixed = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\n",
    "    knn_mixed.fit(X_trained, y_trained)\n",
    "    y_pred = knn_mixed.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PAIRED Model Using KNN with Euclidean Distance: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
