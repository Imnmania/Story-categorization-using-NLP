{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['opencv-text-detection.zip',\n",
       " '.ipynb_checkpoints',\n",
       " 'accepted_projects',\n",
       " 'Andrew W. Trask - Grokking Deep Learning-Manning Publications (2019).pdf',\n",
       " 'Collective_Dataset',\n",
       " 'corpus',\n",
       " 'corpus.zip',\n",
       " 'doc2vec.ipynb',\n",
       " 'doc2vec_with_kfold.html',\n",
       " 'doc2vec_with_kfold.ipynb',\n",
       " 'Final Slides and Books',\n",
       " 'google-play-store-apps',\n",
       " 'google-play-store-apps.zip',\n",
       " 'helpline-of-all-sorts',\n",
       " 'helpline-of-all-sorts.zip',\n",
       " 'kmeans_clustering.html',\n",
       " 'kmeans_clustering.ipynb',\n",
       " 'labels.txt',\n",
       " 'logistic_regression.ipynb',\n",
       " 'MURA-v1.1',\n",
       " 'MURA-v1.1.zip',\n",
       " 'nltk',\n",
       " 'opencv-text-detection',\n",
       " 'Papers',\n",
       " 'Papers.zip',\n",
       " 'Papers_Association_Rule',\n",
       " 'processed_dataset.csv',\n",
       " 'Results',\n",
       " 'reviews.txt',\n",
       " 'sentiment_analysis_from_story_preprocessing.html',\n",
       " 'sentiment_analysis_from_story_preprocessing.ipynb',\n",
       " 'sentiment_dictionary.csv',\n",
       " 'sklearn_test.ipynb',\n",
       " 'Slides',\n",
       " 'Story-categorization-using-NLP',\n",
       " 'Tan.pdf',\n",
       " 'Testing',\n",
       " 'Udacity',\n",
       " 'videodata.csv',\n",
       " 'word2vec.model',\n",
       " 'word2vector_test.ipynb']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import multiprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "cores = multiprocessing.cpu_count()\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.getcwd() + \"/Collective_Dataset/419_data - Sheet1.csv\", usecols=[0,1])#header=None, , names=['story', 'category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just like any other day, employees arrived in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My so-called ‘friends’ in middle school used t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i have been called hurtful names and i have be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at my old school kids would hit me and call me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I had debilitating migraines for three years b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I love my work, but hate going each day becaus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I have a chronic illness which was doing well ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The other part is that sense of worthlessness....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I feel my whole body hurting. My mental health...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>As a librarian, I've been threatened with stal...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               story  category\n",
       "0  Just like any other day, employees arrived in ...         0\n",
       "1  My so-called ‘friends’ in middle school used t...         1\n",
       "2  i have been called hurtful names and i have be...         1\n",
       "3  at my old school kids would hit me and call me...         1\n",
       "4  I had debilitating migraines for three years b...         0\n",
       "5  I love my work, but hate going each day becaus...         0\n",
       "6  I have a chronic illness which was doing well ...         0\n",
       "7  The other part is that sense of worthlessness....         0\n",
       "8  I feel my whole body hurting. My mental health...         0\n",
       "9  As a librarian, I've been threatened with stal...         2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Just like any other day, employees arrived in ...\n",
       "1      My so-called ‘friends’ in middle school used t...\n",
       "2      i have been called hurtful names and i have be...\n",
       "3      at my old school kids would hit me and call me...\n",
       "4      I had debilitating migraines for three years b...\n",
       "                             ...                        \n",
       "254    My stress strikes when I find out that I am th...\n",
       "255    I went back to college last year. I am nearly ...\n",
       "256    The stress got me. I completely mentally check...\n",
       "257    I’ve recently come out of hospital myself afte...\n",
       "258    Today morning, me and my cousin were going to ...\n",
       "Name: story, Length: 259, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      0\n",
       "      ..\n",
       "254    0\n",
       "255    0\n",
       "256    0\n",
       "257    0\n",
       "258    2\n",
       "Name: category, Length: 259, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(259, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = range(259)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48315"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.story.apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnt_pro = df['category'].value_counts()\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('category', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Work Stress Total Data Count:  67\n",
      "Bullying Total Data Count:  97\n",
      "Sexual Harassment Total Data Count:  95\n"
     ]
    }
   ],
   "source": [
    "work_stress_counter, bullying_counter, sexual_harassment_counter = 0, 0, 0\n",
    "for c in df['category']:\n",
    "    if c==0:\n",
    "        work_stress_counter+=1\n",
    "    elif c == 1:\n",
    "        bullying_counter+=1\n",
    "    else:\n",
    "        sexual_harassment_counter+=1\n",
    "print(\"Work Stress Total Data Count: \", work_stress_counter)\n",
    "print(\"Bullying Total Data Count: \", bullying_counter)\n",
    "print(\"Sexual Harassment Total Data Count: \", sexual_harassment_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have been called hurtful names and i have been called black bitch and people are making fun of me for being black by my suppost to be friend t and she has posted rumors about me and i dont know what to do.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(df.story[2])\n",
    "print(df.category[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just like any other day, employees arrived in the workplace sparingly, filling the cubicles and getting their coffees ready. Once more, the manager was already sitting at his desk, grumbling and shouting - You’re way too slow, again! How am I supposed to get my work done with you slowing me down every day? The other employees were staring at each other. They were embarrassed by his outburst but deep inside they knew he was right. Satisfied by the nods in the assembly, the manager calmed down. As usual, he quickly got absorbed by his screen and numerous emails. He was not the moody type, just a normal guy. His team liked him very much and his performance record was exemplary. He was often described as someone caring and trustworthy with a genuine interest in people. Yet once again, his fist hit the desk loudly as he started screaming. That’s it, I’m done! Slow and unreliable old crap! You made me lose five hours of my life, again. World will be better off without you and I’m going to let the board know about that. He stood up suddenly and clenched his fists in anger. He then walked rapidly to the stairs, climbed and disappeared. Nobody saw him again for the day, but they all knew he was going for a confrontation, if not a fight. It was going to get really ugly up there.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df.story[0])\n",
    "print(df.category[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I worked as an office manager, and the only woman, for an industrial insulation company. I had just come back from maternity leave and I was worried about my milk supply. I went into the bathroom to pump for about 15 minutes every two hours, and all of the men in the office would stand in the break area (right in front of the bathroom door) and make baby crying noises to make fun of me. Eventually it progressed to the point that they would make crying noises every time they passed my desk in hopes that I would leak through my shirt. They would also make comments about how much larger my breasts were since having a baby. I felt so harassed and unsafe that I would dread going to work every day, and I even had more than a few nervous breakdowns. My husband was furious and I had to convince him not to take any drastic action so that I could be sure to have a good reference if I needed to find another job. We had a long conversation and looked at our finances and decided the extra money wasn't worth the emotional distress. I ended up quitting my job and staying home with our kids.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(df.story[10])\n",
    "print(df.category[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('x', '')\n",
    "    return text\n",
    "df['story'] = df['story'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH TRAIN_TEST_SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size = .20, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged = train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "test_tagged = test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>i had been seeing this guy for just over three...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>i noticed movement out of the corner of my eye...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>i'm 13 years old and live in nsw, australia wi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>my bullying started, or when i first noticed i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>my story might be short because i only remembe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>i was on my way from gabtoli to mirpur on a bu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hi, i'm rahul. i was bullied all through schoo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>when i was in 7th grade, i had a semester of m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i got made fun of for being fat in elementary,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>let me tell you about a few of my eperiences. ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 story  category\n",
       "197  i had been seeing this guy for just over three...         2\n",
       "201  i noticed movement out of the corner of my eye...         2\n",
       "94   i'm 13 years old and live in nsw, australia wi...         1\n",
       "25   my bullying started, or when i first noticed i...         1\n",
       "85   my story might be short because i only remembe...         1\n",
       "..                                                 ...       ...\n",
       "43   i was on my way from gabtoli to mirpur on a bu...         2\n",
       "22   hi, i'm rahul. i was bullied all through schoo...         1\n",
       "72   when i was in 7th grade, i had a semester of m...         1\n",
       "15   i got made fun of for being fat in elementary,...         1\n",
       "168  let me tell you about a few of my eperiences. ...         2\n",
       "\n",
       "[207 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>a girl jumped on me on my friend's house! she ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>with that 22 years of service — including four...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>i loved my work, but hated going each day beca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i worked as an office manager, and the only wo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>this one time, a man came in with his son. whe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>when i was 77, i was at a brunch and man came ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>my trainer facetimed me multiple times. i igno...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>when i got admitted in a well reputed college,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>hello! first of all, i'm sorry if there are a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i work in law enforcement. i started out as a ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at my old school kids would hit me and call me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>i'm 16 and a boy. i really don't know how to d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>i have been seually assaulted, harassed, shame...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>i have a weird name. i don't even know why i h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>ever since first grade, my three best friends ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>i had a part-time job at a bar. i was groped d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>it started in year 7, which i think is like 6t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>i have been teased all my life. i've been call...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>i was around 12-13 years old and at this “meet...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>hy myself , right now i am studying in one of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>when we were just kids, maybe 7 or 8, our moth...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>in college days, gangs of boys beat me up when...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>i was 14 years old at that time and went to vi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>actually it is okay to work more when needed b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>it all began in the 2nd grade, i was about 3 y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>i was on the bus on my way home after shopping...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>i’ve been seually harassed in the past. three ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>i work at subway. one time, a man told my cowo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>during my college days, i always thought of gr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>i met a girl in 7th grade, let's call her a, a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>i was most definitely bullied in my high schoo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>well ever since i've been in second grade i wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>high school and junior high were hell for me. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>i got beaten up and verbally abused from 1st g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>i was doing what i thought was my dream job. i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>i work in a pharmacy, and my manager at the ti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>yesterday during lunchbreak thoe girls called ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>i was just starting middle school, and i had t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>i’ve always been super flat-chested and i’m st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>i have a chronic illness which was doing well ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>i am 15 now and i have been bullied since grad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>the first time was the day after my 20th birth...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>as i was studying in paris, i also worked nigh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>i find myself ready to leave my 6-figure job t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>i got so much shit one day because i was on my...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>i was born with a craniofacial disease called ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>i was working at petsmart a few years back. a ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>i worked for a company that was highly stressf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>i’m a waitress, and i get harassed daily by cu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>i was in class 11. i was coming back from my t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>i’m a visiting nurse for a local hospital. whe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>one time, my freshman year of high school, i w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 story  category\n",
       "30   a girl jumped on me on my friend's house! she ...         2\n",
       "244  with that 22 years of service — including four...         0\n",
       "140  i loved my work, but hated going each day beca...         0\n",
       "10   i worked as an office manager, and the only wo...         2\n",
       "181  this one time, a man came in with his son. whe...         2\n",
       "160  when i was 77, i was at a brunch and man came ...         2\n",
       "192  my trainer facetimed me multiple times. i igno...         2\n",
       "35   when i got admitted in a well reputed college,...         1\n",
       "101  hello! first of all, i'm sorry if there are a ...         1\n",
       "13   i work in law enforcement. i started out as a ...         2\n",
       "3    at my old school kids would hit me and call me...         1\n",
       "134  i'm 16 and a boy. i really don't know how to d...         2\n",
       "133  i have been seually assaulted, harassed, shame...         2\n",
       "93   i have a weird name. i don't even know why i h...         1\n",
       "54   ever since first grade, my three best friends ...         1\n",
       "180  i had a part-time job at a bar. i was groped d...         2\n",
       "112  it started in year 7, which i think is like 6t...         1\n",
       "111  i have been teased all my life. i've been call...         1\n",
       "71   i was around 12-13 years old and at this “meet...         1\n",
       "236  hy myself , right now i am studying in one of ...         0\n",
       "161  when we were just kids, maybe 7 or 8, our moth...         2\n",
       "235  in college days, gangs of boys beat me up when...         1\n",
       "41   i was 14 years old at that time and went to vi...         2\n",
       "233  actually it is okay to work more when needed b...         0\n",
       "84   it all began in the 2nd grade, i was about 3 y...         1\n",
       "209  i was on the bus on my way home after shopping...         2\n",
       "130  i’ve been seually harassed in the past. three ...         2\n",
       "171  i work at subway. one time, a man told my cowo...         2\n",
       "249  during my college days, i always thought of gr...         0\n",
       "109  i met a girl in 7th grade, let's call her a, a...         1\n",
       "65   i was most definitely bullied in my high schoo...         1\n",
       "114  well ever since i've been in second grade i wa...         1\n",
       "24   high school and junior high were hell for me. ...         1\n",
       "66   i got beaten up and verbally abused from 1st g...         1\n",
       "251  i was doing what i thought was my dream job. i...         0\n",
       "214  i work in a pharmacy, and my manager at the ti...         0\n",
       "20   yesterday during lunchbreak thoe girls called ...         1\n",
       "28   i was just starting middle school, and i had t...         1\n",
       "53   i’ve always been super flat-chested and i’m st...         1\n",
       "142  i have a chronic illness which was doing well ...         0\n",
       "74   i am 15 now and i have been bullied since grad...         1\n",
       "128  the first time was the day after my 20th birth...         2\n",
       "229  as i was studying in paris, i also worked nigh...         0\n",
       "228  i find myself ready to leave my 6-figure job t...         0\n",
       "227  i got so much shit one day because i was on my...         0\n",
       "23   i was born with a craniofacial disease called ...         1\n",
       "173  i was working at petsmart a few years back. a ...         2\n",
       "147  i worked for a company that was highly stressf...         0\n",
       "14   i’m a waitress, and i get harassed daily by cu...         2\n",
       "29   i was in class 11. i was coming back from my t...         1\n",
       "158  i’m a visiting nurse for a local hospital. whe...         2\n",
       "18   one time, my freshman year of high school, i w...         1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197    ([had, been, seeing, this, guy, for, just, ove...\n",
       "201    ([noticed, movement, out, of, the, corner, of,...\n",
       "94     (['m, 13, years, old, and, live, in, nsw, aust...\n",
       "25     ([my, bullying, started, or, when, first, noti...\n",
       "85     ([my, story, might, be, short, because, only, ...\n",
       "                             ...                        \n",
       "43     ([was, on, my, way, from, gabtoli, to, mirpur,...\n",
       "22     ([hi, 'm, rahul, was, bullied, all, through, s...\n",
       "72     ([when, was, in, 7th, grade, had, semester, of...\n",
       "15     ([got, made, fun, of, for, being, fat, in, ele...\n",
       "168    ([let, me, tell, you, about, few, of, my, eper...\n",
       "Length: 207, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=[\"'m\", '13', 'years', 'old', 'and', 'live', 'in', 'nsw', 'australia', 'with', 'my', 'mum', 'brother', 'sister', 'and', 'dad', 'was', 'first', 'bullied', 'in', 'year', 'after', 'recently', 'transferring', 'from', 'private', 'school', 'to', 'public', 'school', 'was', 'very', 'shy', 'back', 'then', 'and', 'therefore', 'did', 'not', 'make', 'friends', 'easily', 'as', 'we', 'lived', 'on', 'farm', 'had', 'to', 'catch', 'bus', 'home', 'every', 'afternoon', 'with', 'bunch', 'of', 'cruel', 'dirty', 'seniors', 'who', 'constantly', 'pushed', 'and', 'shoved', 'me', 'around', 'there', 'was', 'this', 'one', 'girl', 'cody', 'who', 'used', 'to', 'call', 'me', 'names', 'all', 'the', 'time', 'some', 'better', 'than', 'others', 'and', 'would', 'slap', 'me', 'around', 'whenever', 'tried', 'to', 'tell', 'the', 'teacher', 'it', 'stopped', 'after', 'she', 'left', 'and', 'soon', 'began', 'to', 'make', 'friends', 'and', 'enjoy', 'school', 'well', 'you', 'ca', \"n't\", 'say', \"'enjoy\", 'more', 'like', \"'tolerated\", 'my', 'year', 'teacher', 'was', 'an', 'absolute', 'cow', 'she', 'humiliated', 'me', 'in', 'front', 'of', 'the', 'class', 'and', 'abused', 'my', 'family', 'in', 'private', 'whenever', 'told', 'my', 'parents', 'they', 'would', 'just', 'ignore', 'me', 'and', 'continue', 'with', 'whatever', 'they', 'were', 'doing', 'was', 'lost', 'consoled', 'in', 'my', 'best', 'friend', 'jakeisha', 'at', 'the', 'time', 'and', 'she', 'brought', 'her', 'phone', 'in', 'recorded', 'an', 'echange', 'between', 'me', 'and', 'the', 'teacher', 'she', 'then', 'showed', 'it', 'to', 'the', 'principal', 'and', 'the', 'teacher', 'was', 'suspended', 'got', 'through', 'the', 'rest', 'of', 'primary', 'school', 'pretty', 'well', 'was', 'reasonably', 'popular', 'top', 'in', 'every', 'class', 'and', 'played', 'in', 'every', 'school', 'sport', 'team', 'there', 'was', 'had', 'recently', 'reunited', 'with', 'my', 'best', 'friend', 'leie', 'from', 'kindergarten', 'when', 'more', 'trouble', 'started', 'was', 'in', 'year', 'and', 'the', 'guy', 'liked', 'had', 'crush', 'on', 'leie', 'normally', 'would', \"n't\", 'care', 'but', 'because', 'although', 'she', 'knew', 'liked', 'him', 'like', 'really', 'liked', 'him', 'she', 'went', 'out', 'with', 'him', 'and', 'then', 'said', 'to', 'me', '``', 'you', 'can', 'have', 'him', 'when', \"'m\", 'finished', 'with', 'him', \"''\", 'was', 'heartbroken', 'and', 'stopped', 'talking', 'to', 'her', 'for', 'couple', 'of', 'years', 'in', 'that', 'time', 'she', 'spread', 'rumours', 'about', 'me', 'and', 'my', 'family', 'life', 'my', 'dad', 'was', 'cheating', 'on', 'my', 'mum', 'and', 'was', 'going', 'to', 'move', 'out', 'some', 'were', 'true', 'some', 'were', 'not', 'the', 'fact', 'that', 'she', 'embarrassed', 'me', 'in', 'front', 'of', 'the', 'whole', 'school', 'did', 'not', 'matter', 'to', 'her', 'and', 'she', 'just', 'waved', 'it', 'off', 'it', 'was', \"n't\", 'til', 'the', 'end', 'of', 'year', 'that', 'we', 'became', 'friends', 'again', 'and', 'even', 'then', 'did', \"n't\", 'like', 'her', 'that', 'much', 'we', 'started', 'high', 'school', 'together', 'and', 'grew', 'closer', 'as', 'each', 'day', 'passed', 'it', 'was', 'then', 'when', 'met', 'eliza', 'kind', 'caring', 'girl', 'who', 'had', 'had', 'trouble', 'with', 'her', 'previous', 'friends', 'decided', 'to', 'become', 'friends', 'with', 'her', 'even', 'though', 'leie', 'did', \"n't\", 'like', 'it', 'and', 'we', 'soon', 'became', 'etremely', 'good', 'friends', 'soon', 'leie', 'came', 'to', 'accept', 'eliza', 'and', 'they', 'became', 'good', 'friends', 'which', 'was', 'nice', 'but', 'unfortunately', 'leie', 'decided', 'to', 'steal', 'her', 'away', 'from', 'me', 'at', 'first', 'eliza', 'refused', 'but', 'because', 'of', 'leie', \"'s\", 'persuasive', 'techniques', 'and', 'constant', 'convincing', 'she', 'soon', 'stopped', 'hanging', 'out', 'with', 'me', 'and', 'spent', 'more', 'time', 'with', 'leie', 'after', 'couple', 'of', 'months', 'we', 'started', 'being', 'friends', 'again', 'and', 'have', 'stayed', 'that', 'way', 'since', 'as', 'did', \"n't\", 'tell', 'anyone', 'and', 'those', 'did', 'did', \"n't\", 'listen', 'started', 'thinking', 'about', 'suicide', 'of', 'course', 'did', \"n't\", 'act', 'on', 'it', 'but', 'often', 'thought', 'about', 'what', 'they', 'would', 'say', 'if', 'they', 'woke', 'up', 'one', 'day', 'and', 'was', 'gone'], tags=[1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged.values[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['with', 'that', '22', 'years', 'of', 'service', 'including', 'four', 'overseas', 'deployments', 'rwanda', 'and', 'afghanistan', 'number', 'of', 'in-canada', 'deployments', 'manitoba', 'flood', 'quebec', 'ice', 'storm', 'training', 'searches', 'for', 'missing', 'persons', 'two', 'marriage', 'breakdowns', 'and', 'number', 'of', 'injuries', 'comes', 'barrack', 'bo', 'full', 'of', 'mental', 'issues', 'that', 'need', 'to', 'be', 'resolved', 'found', 'myself', 'reaching', 'the', 'point', 'where', 'my', 'psychological', 'issues', 'were', 'ruining', 'my', 'life', 'my', 'relationships', 'and', 'my', 'work', 'and', 'felt', 'need', 'to', 'break', 'free', 'of', 'the', 'military', 'stigma', 'about', 'discussing', 'one', \"'s\", 'mental', 'problems', 'so', 'one', 'day', 'walked', 'into', 'the', 'warrior', 'support', 'centre', 'in', 'petawawa', 'and', 'asked', 'for', 'help', 'spilling', 'your', 'guts', 'to', 'stranger', 'is', 'not', 'an', 'easy', 'task', 'perhaps', 'especially', 'for', 'someone', 'who', 'has', 'been', 'trained', 'as', 'soldier', 'to', 'do', 'the', 'job', 'and', 'not', 'complain', 'but', 'felt', 'needed', 'to', 'gain', 'control', 'of', 'my', 'life', 'and', 'my', 'future', 'do', \"n't\", 'believe', 'my', 'psychological', 'issues', 'are', 'tour-related', 'but', 'have', 'issues', 'nonetheless', 'and', 'to', 'be', 'an', 'effective', 'leader', 'and', 'soldier', 'needed', 'to', 'deal', 'with', 'them', 'although', 'am', 'content', 'with', 'my', 'current', 'military', 'job', 'have', 'reached', 'point', 'in', 'my', 'life', 'especially', 'after', 'the', 'pace', 'of', 'these', 'last', 'years', 'where', 'think', 'have', 'given', 'enough', 'having', 'the', 'ability', 'to', 'know', 'where', 'will', 'be', 'or', 'what', 'job', 'will', 'be', 'working', 'at', 'in', 'two', 'or', 'three', 'years', 'or', 'even', 'what', 'time', 'will', 'be', 'home', 'has', 'become', 'important', 'to', 'me', 'and', 'am', 'looking', 'for', 'that', 'stability', 'maybe', 'the', 'hinted', 'posting', 'to', 'ottawa', 'this', 'year', 'will', 'give', 'me', 'that', 'sense', 'of', '``', 'normal', \"''\", 'am', 'looking', 'for', 'sort', 'myself', 'out', 'and', 'recharge', 'for', 'my', 'generation', 'of', 'the', 'soldier', 'the', 'canadian', 'forces', 'once', 'offered', 'fairly', 'predictable', 'career', 'path', 'based', 'on', 'training', 'and', 'ability', 'but', 'the', 'afghanistan', 'eperience', 'raised', 'both', 'huge', 'epectations', 'and', 'demands', 'on', 'soldiers', 'and', 'their', 'families', 'that', 'we', 'are', 'all', 'in', 'our', 'own', 'ways', 'struggling', 'to', 'deal', 'with'], tags=[0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tagged.values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PV-DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 888660.11it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 558341.43it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1356595.20it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1260117.46it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1073202.63it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 815997.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 371 ms, sys: 9.69 ms, total: 381 ms\n",
      "Wall time: 135 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(5):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.8269230769230769\n",
      "Testing F1 score: 0.8204652490366775\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  2  2]\n",
      " [ 0 22  0]\n",
      " [ 2  3 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73        12\n",
      "           1       0.81      1.00      0.90        22\n",
      "           2       0.87      0.72      0.79        18\n",
      "\n",
      "    accuracy                           0.83        52\n",
      "   macro avg       0.83      0.80      0.80        52\n",
      "weighted avg       0.83      0.83      0.82        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "print('\\nTesting Confusion Matrix: ')\n",
    "print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "print('Testing Classification Report: ')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PV-DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1273051.21it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 629601.83it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 671997.62it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 552303.39it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 857078.90it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1129025.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 450 ms, sys: 17.9 ms, total: 468 ms\n",
      "Wall time: 193 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(5):\n",
    "    model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dmm.alpha -= 0.002\n",
    "    model_dmm.min_alpha = model_dmm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dmm, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dmm, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.8269230769230769\n",
      "Testing F1 score: 0.8251719041192727\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  1  3]\n",
      " [ 0 20  2]\n",
      " [ 1  2 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.87      0.91      0.89        22\n",
      "           2       0.75      0.83      0.79        18\n",
      "\n",
      "    accuracy                           0.83        52\n",
      "   macro avg       0.84      0.80      0.81        52\n",
      "weighted avg       0.83      0.83      0.83        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "print('\\nTesting Confusion Matrix: ')\n",
    "print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "print('Testing Classification Report: ')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = get_vectors(new_model, train_tagged)\n",
    "y_test, X_test = get_vectors(new_model, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.7884615384615384\n",
      "Testing F1 score: 0.785570950788342\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  1  3]\n",
      " [ 1 20  1]\n",
      " [ 2  3 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.67      0.70        12\n",
      "           1       0.83      0.91      0.87        22\n",
      "           2       0.76      0.72      0.74        18\n",
      "\n",
      "    accuracy                           0.79        52\n",
      "   macro avg       0.78      0.77      0.77        52\n",
      "weighted avg       0.79      0.79      0.79        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "print('\\nTesting Confusion Matrix: ')\n",
    "print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "print('Testing Classification Report: ')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH K-FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  259\n",
      "Type:  <class 'pandas.core.series.Series'>\n",
      "First Ten Values:\n",
      " 0    just like any other day, employees arrived in ...\n",
      "1    my so-called ‘friends’ in middle school used t...\n",
      "2    i have been called hurtful names and i have be...\n",
      "3    at my old school kids would hit me and call me...\n",
      "4    i had debilitating migraines for three years b...\n",
      "5    i love my work, but hate going each day becaus...\n",
      "6    i have a chronic illness which was doing well ...\n",
      "7    the other part is that sense of worthlessness....\n",
      "8    i feel my whole body hurting. my mental health...\n",
      "9    as a librarian, i've been threatened with stal...\n",
      "Name: story, dtype: object\n",
      "just like any other day, employees arrived in the workplace sparingly, filling the cubicles and getting their coffees ready. once more, the manager was already sitting at his desk, grumbling and shouting - you’re way too slow, again! how am i supposed to get my work done with you slowing me down every day? the other employees were staring at each other. they were embarrassed by his outburst but deep inside they knew he was right. satisfied by the nods in the assembly, the manager calmed down. as usual, he quickly got absorbed by his screen and numerous emails. he was not the moody type, just a normal guy. his team liked him very much and his performance record was eemplary. he was often described as someone caring and trustworthy with a genuine interest in people. yet once again, his fist hit the desk loudly as he started screaming. that’s it, i’m done! slow and unreliable old crap! you made me lose five hours of my life, again. world will be better off without you and i’m going to let the board know about that. he stood up suddenly and clenched his fists in anger. he then walked rapidly to the stairs, climbed and disappeared. nobody saw him again for the day, but they all knew he was going for a confrontation, if not a fight. it was going to get really ugly up there.\n"
     ]
    }
   ],
   "source": [
    "X = df['story']\n",
    "print(\"Length: \", len(X))\n",
    "print(\"Type: \", type(X))\n",
    "print(\"First Ten Values:\\n\", X[:10])\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  259\n",
      "Type:  <class 'pandas.core.series.Series'>\n",
      "First Ten Values:  0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "8    0\n",
      "9    2\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y = df['category']\n",
    "print(\"Length: \", len(y))\n",
    "print(\"Type: \", type(y))\n",
    "print(\"First Ten Values: \", y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=5, random_state=42, shuffle=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten_fold = KFold(n_splits=5, shuffle = True, random_state=42)\n",
    "ten_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold:  5\n",
      "X_train:  (207,) X_test:  (52,)\n",
      "X_train:  (207,) X_test:  (52,)\n",
      "X_train:  (207,) X_test:  (52,)\n",
      "X_train:  (207,) X_test:  (52,)\n",
      "X_train:  (208,) X_test:  (51,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Fold: \", ten_fold.get_n_splits(X))\n",
    "fold_no = 1\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"X_train: \", X_train.shape, \"X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "total_fold = ten_fold.get_n_splits(X)\n",
    "print(total_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-FOLD PV-DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold:  5\n",
      "Train Fold No.:  [  0   1   2   3   4   5   7   8  11  12  13  14  17  20  21  22  23  26\n",
      "  27  28  29  31  32  34  35  36  37  38  39  40  41  42  43  44  47  48\n",
      "  49  50  51  52  53  54  56  57  58  59  61  62  63  64  65  69  70  71\n",
      "  72  74  76  77  78  79  80  81  82  83  85  86  87  88  89  91  93  94\n",
      "  95  96  97  98  99 100 102 103 105 106 107 110 111 112 113 114 115 116\n",
      " 117 118 120 121 122 123 124 126 127 128 129 130 131 133 134 135 136 138\n",
      " 139 140 141 142 144 145 146 147 149 150 151 153 154 155 156 157 158 159\n",
      " 160 161 162 163 164 166 167 169 170 171 172 173 174 175 177 179 181 182\n",
      " 183 186 187 188 189 191 192 193 194 195 196 197 198 199 201 202 203 204\n",
      " 207 208 209 210 211 213 214 215 216 217 218 219 220 221 222 223 224 225\n",
      " 226 228 230 231 232 233 234 235 236 238 239 240 242 243 244 245 246 247\n",
      " 248 249 251 252 253 254 255 256 258]  Test Fold No.:  [  6   9  10  15  16  18  19  24  25  30  33  45  46  55  60  66  67  68\n",
      "  73  75  84  90  92 101 104 108 109 119 125 132 137 143 148 152 165 168\n",
      " 176 178 180 184 185 190 200 205 206 212 227 229 237 241 250 257]\n",
      "No. of Training Dataset in Fold:  1  207\n",
      "No. of Testing Dataset in Fold:  1  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1047311.13it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 718725.93it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1038541.78it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1066610.48it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1124638.51it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1256470.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  1\n",
      "Testing accuracy 0.8653846153846154\n",
      "Testing F1 score: 0.8651988108509848\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  1  1]\n",
      " [ 0 20  2]\n",
      " [ 1  2 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87        12\n",
      "           1       0.87      0.91      0.89        22\n",
      "           2       0.83      0.83      0.83        18\n",
      "\n",
      "    accuracy                           0.87        52\n",
      "   macro avg       0.87      0.86      0.86        52\n",
      "weighted avg       0.87      0.87      0.87        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Fold No.:  [  0   1   3   4   6   7   8   9  10  11  13  14  15  16  17  18  19  20\n",
      "  21  22  23  24  25  26  27  30  32  33  34  36  37  39  40  41  43  44\n",
      "  45  46  47  48  49  50  51  52  53  54  55  57  58  59  60  61  62  63\n",
      "  64  66  67  68  70  71  72  73  74  75  76  78  80  81  83  84  87  88\n",
      "  89  90  91  92  93  94  95  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 112 113 114 116 118 119 120 121 122 123 124 125 127 128 129 130\n",
      " 131 132 133 134 135 137 138 143 144 145 146 148 149 150 151 152 153 154\n",
      " 156 157 159 160 161 163 165 166 168 169 170 171 172 173 174 176 177 178\n",
      " 179 180 182 183 184 185 186 187 188 189 190 191 192 196 197 198 199 200\n",
      " 201 202 203 205 206 207 208 209 210 212 213 214 215 216 217 218 219 220\n",
      " 221 222 224 226 227 229 231 232 233 234 235 237 238 239 240 241 242 243\n",
      " 244 247 248 250 253 254 256 257 258]  Test Fold No.:  [  2   5  12  28  29  31  35  38  42  56  65  69  77  79  82  85  86  96\n",
      "  97 111 115 117 126 136 139 140 141 142 147 155 158 162 164 167 175 181\n",
      " 193 194 195 204 211 223 225 228 230 236 245 246 249 251 252 255]\n",
      "No. of Training Dataset in Fold:  2  207\n",
      "No. of Testing Dataset in Fold:  2  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1404888.23it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1352369.05it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1013093.26it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1179648.00it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 849531.24it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 168684.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  2\n",
      "Testing accuracy 0.8846153846153846\n",
      "Testing F1 score: 0.8843640020110608\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[16  0  0]\n",
      " [ 2 15  1]\n",
      " [ 2  1 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89        16\n",
      "           1       0.94      0.83      0.88        18\n",
      "           2       0.94      0.83      0.88        18\n",
      "\n",
      "    accuracy                           0.88        52\n",
      "   macro avg       0.89      0.89      0.88        52\n",
      "weighted avg       0.90      0.88      0.88        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Fold No.:  [  1   2   3   5   6   7   8   9  10  12  13  14  15  16  17  18  19  20\n",
      "  21  23  24  25  28  29  30  31  33  34  35  37  38  39  40  42  43  44\n",
      "  45  46  47  48  49  50  52  53  54  55  56  57  58  59  60  63  64  65\n",
      "  66  67  68  69  71  72  73  74  75  77  79  80  81  82  83  84  85  86\n",
      "  87  88  89  90  91  92  94  96  97  99 101 102 103 104 105 106 107 108\n",
      " 109 110 111 115 116 117 119 121 123 125 126 129 130 131 132 133 134 136\n",
      " 137 139 140 141 142 143 145 147 148 149 151 152 153 155 156 157 158 160\n",
      " 161 162 163 164 165 166 167 168 169 171 174 175 176 178 179 180 181 184\n",
      " 185 187 188 189 190 191 192 193 194 195 196 199 200 201 202 203 204 205\n",
      " 206 207 210 211 212 213 214 215 216 217 218 219 220 222 223 224 225 226\n",
      " 227 228 229 230 231 232 234 235 236 237 239 240 241 242 244 245 246 247\n",
      " 248 249 250 251 252 254 255 257 258]  Test Fold No.:  [  0   4  11  22  26  27  32  36  41  51  61  62  70  76  78  93  95  98\n",
      " 100 112 113 114 118 120 122 124 127 128 135 138 144 146 150 154 159 170\n",
      " 172 173 177 182 183 186 197 198 208 209 221 233 238 243 253 256]\n",
      "No. of Training Dataset in Fold:  3  207\n",
      "No. of Testing Dataset in Fold:  3  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 673039.48it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 856233.66it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 891397.26it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1224571.13it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1062693.91it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 709910.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  3\n",
      "Testing accuracy 0.8653846153846154\n",
      "Testing F1 score: 0.8663230977060764\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  0  3]\n",
      " [ 0 17  2]\n",
      " [ 2  0 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.73      0.76        11\n",
      "           1       1.00      0.89      0.94        19\n",
      "           2       0.80      0.91      0.85        22\n",
      "\n",
      "    accuracy                           0.87        52\n",
      "   macro avg       0.87      0.84      0.85        52\n",
      "weighted avg       0.87      0.87      0.87        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Fold No.:  [  0   1   2   4   5   6   9  10  11  12  14  15  16  18  19  20  21  22\n",
      "  24  25  26  27  28  29  30  31  32  33  35  36  37  38  41  42  45  46\n",
      "  48  50  51  52  54  55  56  57  58  60  61  62  63  65  66  67  68  69\n",
      "  70  71  73  74  75  76  77  78  79  82  84  85  86  87  88  90  92  93\n",
      "  95  96  97  98  99 100 101 102 103 104 106 107 108 109 111 112 113 114\n",
      " 115 116 117 118 119 120 121 122 124 125 126 127 128 129 130 132 135 136\n",
      " 137 138 139 140 141 142 143 144 146 147 148 149 150 151 152 154 155 157\n",
      " 158 159 160 162 164 165 167 168 169 170 172 173 174 175 176 177 178 180\n",
      " 181 182 183 184 185 186 187 188 189 190 191 193 194 195 197 198 200 202\n",
      " 203 204 205 206 207 208 209 210 211 212 213 214 216 217 218 219 221 223\n",
      " 225 227 228 229 230 232 233 235 236 237 238 241 242 243 245 246 247 249\n",
      " 250 251 252 253 254 255 256 257 258]  Test Fold No.:  [  3   7   8  13  17  23  34  39  40  43  44  47  49  53  59  64  72  80\n",
      "  81  83  89  91  94 105 110 123 131 133 134 145 153 156 161 163 166 171\n",
      " 179 192 196 199 201 215 220 222 224 226 231 234 239 240 244 248]\n",
      "No. of Training Dataset in Fold:  4  207\n",
      "No. of Testing Dataset in Fold:  4  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1154549.11it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 962550.92it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 559060.48it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 782181.02it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 590224.97it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 729597.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  4\n",
      "Testing accuracy 0.9038461538461539\n",
      "Testing F1 score: 0.9036552777247567\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 1 15  1]\n",
      " [ 0  1 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90        16\n",
      "           1       0.88      0.88      0.88        17\n",
      "           2       0.90      0.95      0.92        19\n",
      "\n",
      "    accuracy                           0.90        52\n",
      "   macro avg       0.91      0.90      0.90        52\n",
      "weighted avg       0.90      0.90      0.90        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Fold No.:  [  0   2   3   4   5   6   7   8   9  10  11  12  13  15  16  17  18  19\n",
      "  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  38  39  40\n",
      "  41  42  43  44  45  46  47  49  51  53  55  56  59  60  61  62  64  65\n",
      "  66  67  68  69  70  72  73  75  76  77  78  79  80  81  82  83  84  85\n",
      "  86  89  90  91  92  93  94  95  96  97  98 100 101 104 105 108 109 110\n",
      " 111 112 113 114 115 117 118 119 120 122 123 124 125 126 127 128 131 132\n",
      " 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 150 152\n",
      " 153 154 155 156 158 159 161 162 163 164 165 166 167 168 170 171 172 173\n",
      " 175 176 177 178 179 180 181 182 183 184 185 186 190 192 193 194 195 196\n",
      " 197 198 199 200 201 204 205 206 208 209 211 212 215 220 221 222 223 224\n",
      " 225 226 227 228 229 230 231 233 234 236 237 238 239 240 241 243 244 245\n",
      " 246 248 249 250 251 252 253 255 256 257]  Test Fold No.:  [  1  14  20  21  37  48  50  52  54  57  58  63  71  74  87  88  99 102\n",
      " 103 106 107 116 121 129 130 149 151 157 160 169 174 187 188 189 191 202\n",
      " 203 207 210 213 214 216 217 218 219 232 235 242 247 254 258]\n",
      "No. of Training Dataset in Fold:  5  208\n",
      "No. of Testing Dataset in Fold:  5  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [00:00<00:00, 1051102.69it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1446791.43it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1262540.13it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 846183.54it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 971509.17it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 904995.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  5\n",
      "Testing accuracy 0.8235294117647058\n",
      "Testing F1 score: 0.8218056453350571\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  2  2]\n",
      " [ 0 18  3]\n",
      " [ 1  1 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.86      0.86      0.86        21\n",
      "           2       0.76      0.89      0.82        18\n",
      "\n",
      "    accuracy                           0.82        51\n",
      "   macro avg       0.84      0.80      0.81        51\n",
      "weighted avg       0.83      0.82      0.82        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Fold: \", total_fold)\n",
    "fold_no = 1\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "    print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold: \", fold_no, \"\", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold: \", fold_no, \"\", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg.fit(X_trained, y_trained)\n",
    "    y_pred = logreg.predict(X_tested)\n",
    "    \n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-FOLD PV-DM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold:  5\n",
      "No. of Training Dataset in Fold  1 :  207\n",
      "No. of Testing Dataset in Fold  1 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1143901.09it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1454306.41it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1002564.58it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 950953.92it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 801681.37it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 679359.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  1\n",
      "Testing accuracy 0.8461538461538461\n",
      "Testing F1 score: 0.845484949832776\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  1  1]\n",
      " [ 0 20  2]\n",
      " [ 1  3 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87        12\n",
      "           1       0.83      0.91      0.87        22\n",
      "           2       0.82      0.78      0.80        18\n",
      "\n",
      "    accuracy                           0.85        52\n",
      "   macro avg       0.86      0.84      0.85        52\n",
      "weighted avg       0.85      0.85      0.85        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  207\n",
      "No. of Testing Dataset in Fold  2 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 422491.94it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1278675.89it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 679891.09it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 762935.79it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 765626.92it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 716945.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  2\n",
      "Testing accuracy 0.9230769230769231\n",
      "Testing F1 score: 0.9223653929536282\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[16  0  0]\n",
      " [ 1 15  2]\n",
      " [ 1  0 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94        16\n",
      "           1       1.00      0.83      0.91        18\n",
      "           2       0.89      0.94      0.92        18\n",
      "\n",
      "    accuracy                           0.92        52\n",
      "   macro avg       0.93      0.93      0.92        52\n",
      "weighted avg       0.93      0.92      0.92        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  207\n",
      "No. of Testing Dataset in Fold  3 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1083921.26it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 702444.12it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1094856.15it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 709330.82it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 947839.44it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 769017.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  3\n",
      "Testing accuracy 0.8076923076923077\n",
      "Testing F1 score: 0.8063359345968042\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  2  2]\n",
      " [ 0 16  3]\n",
      " [ 0  3 19]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.64      0.78        11\n",
      "           1       0.76      0.84      0.80        19\n",
      "           2       0.79      0.86      0.83        22\n",
      "\n",
      "    accuracy                           0.81        52\n",
      "   macro avg       0.85      0.78      0.80        52\n",
      "weighted avg       0.82      0.81      0.81        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  207\n",
      "No. of Testing Dataset in Fold  4 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 664793.97it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1179648.00it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 852869.28it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 844572.89it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 723517.44it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1536674.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  4\n",
      "Testing accuracy 0.8846153846153846\n",
      "Testing F1 score: 0.884032634032634\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  0  1]\n",
      " [ 2 14  1]\n",
      " [ 0  2 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91        16\n",
      "           1       0.88      0.82      0.85        17\n",
      "           2       0.89      0.89      0.89        19\n",
      "\n",
      "    accuracy                           0.88        52\n",
      "   macro avg       0.88      0.89      0.88        52\n",
      "weighted avg       0.88      0.88      0.88        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  208\n",
      "No. of Testing Dataset in Fold  5 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [00:00<00:00, 734356.26it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 627636.86it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 897546.53it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 804811.10it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 819169.23it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 759282.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  5\n",
      "Testing accuracy 0.803921568627451\n",
      "Testing F1 score: 0.79484484124127\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  2  4]\n",
      " [ 0 19  2]\n",
      " [ 1  1 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.50      0.63        12\n",
      "           1       0.86      0.90      0.88        21\n",
      "           2       0.73      0.89      0.80        18\n",
      "\n",
      "    accuracy                           0.80        51\n",
      "   macro avg       0.82      0.76      0.77        51\n",
      "weighted avg       0.81      0.80      0.79        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold: \", total_fold)\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg.fit(X_trained, y_trained)\n",
    "    y_pred = logreg.predict(X_tested)\n",
    "    \n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n\\n\")\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAIRED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 5\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  1 :  207\n",
      "No. of Testing Dataset in Fold  1 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1386934.39it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 472629.79it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1550394.51it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 855390.08it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1604844.60it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1303635.03it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 885036.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8653846153846154\n",
      "Testing F1 score: 0.8640827257848535\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  2  0]\n",
      " [ 0 21  1]\n",
      " [ 2  2 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83        12\n",
      "           1       0.84      0.95      0.89        22\n",
      "           2       0.93      0.78      0.85        18\n",
      "\n",
      "    accuracy                           0.87        52\n",
      "   macro avg       0.87      0.86      0.86        52\n",
      "weighted avg       0.87      0.87      0.86        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 1209221.35it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 893231.41it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1186094.16it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 819076.35it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 793620.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8653846153846154\n",
      "Testing F1 score: 0.8624009857419469\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  1  3]\n",
      " [ 0 21  1]\n",
      " [ 0  2 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80        12\n",
      "           1       0.88      0.95      0.91        22\n",
      "           2       0.80      0.89      0.84        18\n",
      "\n",
      "    accuracy                           0.87        52\n",
      "   macro avg       0.89      0.84      0.85        52\n",
      "weighted avg       0.88      0.87      0.86        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model: \n",
      "Testing accuracy 0.8269230769230769\n",
      "Testing F1 score: 0.8231236848258126\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  1  3]\n",
      " [ 0 21  1]\n",
      " [ 1  3 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.84      0.95      0.89        22\n",
      "           2       0.78      0.78      0.78        18\n",
      "\n",
      "    accuracy                           0.83        52\n",
      "   macro avg       0.84      0.80      0.81        52\n",
      "weighted avg       0.83      0.83      0.82        52\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  207\n",
      "No. of Testing Dataset in Fold  2 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 627780.86it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1398101.33it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 775197.26it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1295852.13it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1179648.00it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 651328.53it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 643603.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.9230769230769231\n",
      "Testing F1 score: 0.922927072927073\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[16  0  0]\n",
      " [ 1 17  0]\n",
      " [ 2  1 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91        16\n",
      "           1       0.94      0.94      0.94        18\n",
      "           2       1.00      0.83      0.91        18\n",
      "\n",
      "    accuracy                           0.92        52\n",
      "   macro avg       0.93      0.93      0.92        52\n",
      "weighted avg       0.93      0.92      0.92        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 646478.73it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1315486.25it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 694021.53it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 754974.72it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 690159.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.9423076923076923\n",
      "Testing F1 score: 0.9423076923076923\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[16  0  0]\n",
      " [ 1 16  1]\n",
      " [ 1  0 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94        16\n",
      "           1       1.00      0.89      0.94        18\n",
      "           2       0.94      0.94      0.94        18\n",
      "\n",
      "    accuracy                           0.94        52\n",
      "   macro avg       0.94      0.94      0.94        52\n",
      "weighted avg       0.95      0.94      0.94        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model: \n",
      "Testing accuracy 0.8846153846153846\n",
      "Testing F1 score: 0.884892884892885\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[16  0  0]\n",
      " [ 1 15  2]\n",
      " [ 3  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89        16\n",
      "           1       1.00      0.83      0.91        18\n",
      "           2       0.88      0.83      0.86        18\n",
      "\n",
      "    accuracy                           0.88        52\n",
      "   macro avg       0.89      0.89      0.89        52\n",
      "weighted avg       0.90      0.88      0.88        52\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  207\n",
      "No. of Testing Dataset in Fold  3 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1512580.01it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1420983.52it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1117401.45it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1120285.07it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1269328.84it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 935582.90it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 787144.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8461538461538461\n",
      "Testing F1 score: 0.8381410256410255\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  1  4]\n",
      " [ 1 17  1]\n",
      " [ 0  1 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.55      0.67        11\n",
      "           1       0.89      0.89      0.89        19\n",
      "           2       0.81      0.95      0.88        22\n",
      "\n",
      "    accuracy                           0.85        52\n",
      "   macro avg       0.85      0.80      0.81        52\n",
      "weighted avg       0.85      0.85      0.84        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 1008386.68it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1313496.11it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1121732.47it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 842932.94it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 663777.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8461538461538461\n",
      "Testing F1 score: 0.8423076923076924\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  1  3]\n",
      " [ 1 18  0]\n",
      " [ 1  2 19]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.64      0.70        11\n",
      "           1       0.86      0.95      0.90        19\n",
      "           2       0.86      0.86      0.86        22\n",
      "\n",
      "    accuracy                           0.85        52\n",
      "   macro avg       0.83      0.82      0.82        52\n",
      "weighted avg       0.84      0.85      0.84        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model: \n",
      "Testing accuracy 0.8653846153846154\n",
      "Testing F1 score: 0.861423405654175\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  1  3]\n",
      " [ 1 18  0]\n",
      " [ 1  1 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.64      0.70        11\n",
      "           1       0.90      0.95      0.92        19\n",
      "           2       0.87      0.91      0.89        22\n",
      "\n",
      "    accuracy                           0.87        52\n",
      "   macro avg       0.85      0.83      0.84        52\n",
      "weighted avg       0.86      0.87      0.86        52\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  207\n",
      "No. of Testing Dataset in Fold  4 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 589423.58it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1449450.63it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 586635.76it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 648894.57it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 908180.89it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 775197.26it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 803908.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9230769230769231\n",
      "Testing F1 score: 0.9230769230769231\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  1  0]\n",
      " [ 1 15  1]\n",
      " [ 0  1 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        16\n",
      "           1       0.88      0.88      0.88        17\n",
      "           2       0.95      0.95      0.95        19\n",
      "\n",
      "    accuracy                           0.92        52\n",
      "   macro avg       0.92      0.92      0.92        52\n",
      "weighted avg       0.92      0.92      0.92        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 1258291.20it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 542638.08it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 834025.87it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1000254.53it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 884135.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9038461538461539\n",
      "Testing F1 score: 0.9044401544401544\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  1  0]\n",
      " [ 1 15  1]\n",
      " [ 0  2 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        16\n",
      "           1       0.83      0.88      0.86        17\n",
      "           2       0.94      0.89      0.92        19\n",
      "\n",
      "    accuracy                           0.90        52\n",
      "   macro avg       0.91      0.90      0.90        52\n",
      "weighted avg       0.91      0.90      0.90        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model: \n",
      "Testing accuracy 0.9230769230769231\n",
      "Testing F1 score: 0.9230769230769231\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  1  0]\n",
      " [ 1 15  1]\n",
      " [ 0  1 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        16\n",
      "           1       0.88      0.88      0.88        17\n",
      "           2       0.95      0.95      0.95        19\n",
      "\n",
      "    accuracy                           0.92        52\n",
      "   macro avg       0.92      0.92      0.92        52\n",
      "weighted avg       0.92      0.92      0.92        52\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  208\n",
      "No. of Testing Dataset in Fold  5 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [00:00<00:00, 713924.09it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1442008.65it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1053641.58it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1086444.87it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 820710.47it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 913523.80it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1290555.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8431372549019608\n",
      "Testing F1 score: 0.8426681677455671\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  2  1]\n",
      " [ 0 18  3]\n",
      " [ 1  1 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.75      0.82        12\n",
      "           1       0.86      0.86      0.86        21\n",
      "           2       0.80      0.89      0.84        18\n",
      "\n",
      "    accuracy                           0.84        51\n",
      "   macro avg       0.85      0.83      0.84        51\n",
      "weighted avg       0.85      0.84      0.84        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 208/208 [00:00<00:00, 1127151.46it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1182134.46it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 788802.20it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 900325.32it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 953459.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.7843137254901961\n",
      "Testing F1 score: 0.7771977824177406\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  1  5]\n",
      " [ 0 19  2]\n",
      " [ 1  2 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.50      0.63        12\n",
      "           1       0.86      0.90      0.88        21\n",
      "           2       0.68      0.83      0.75        18\n",
      "\n",
      "    accuracy                           0.78        51\n",
      "   macro avg       0.80      0.75      0.76        51\n",
      "weighted avg       0.80      0.78      0.78        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model: \n",
      "Testing accuracy 0.803921568627451\n",
      "Testing F1 score: 0.8055270888843342\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  0  4]\n",
      " [ 0 18  3]\n",
      " [ 1  2 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.90      0.86      0.88        21\n",
      "           2       0.68      0.83      0.75        18\n",
      "\n",
      "    accuracy                           0.80        51\n",
      "   macro avg       0.82      0.79      0.80        51\n",
      "weighted avg       0.82      0.80      0.81        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: {}\\n\\n\" .format(total_fold))\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    logreg_dbow = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg_dbow.fit(X_trained, y_trained)\n",
    "    y_pred = logreg_dbow.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    logreg_dm = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg_dm.fit(X_trained, y_trained)\n",
    "    y_pred = logreg_dm.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    logreg_paired = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg_paired.fit(X_train, y_train)\n",
    "    y_pred = logreg_paired.predict(X_test)\n",
    "    \n",
    "    print(\"FOR Paired Model: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Using Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 5\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  1 :  207\n",
      "No. of Testing Dataset in Fold  1 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 514653.78it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1389153.48it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 780072.71it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1547630.89it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 853707.89it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1162277.01it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 772438.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Entropy: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8461538461538461\n",
      "Testing F1 score: 0.8457516339869281\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  1  1]\n",
      " [ 1 20  1]\n",
      " [ 2  2 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.80        12\n",
      "           1       0.87      0.91      0.89        22\n",
      "           2       0.88      0.78      0.82        18\n",
      "\n",
      "    accuracy                           0.85        52\n",
      "   macro avg       0.84      0.84      0.84        52\n",
      "weighted avg       0.85      0.85      0.85        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 1252844.05it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 864761.88it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1162277.01it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1339847.11it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 720515.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Entropy:  \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.7692307692307693\n",
      "Testing F1 score: 0.7669191422388354\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  2  1]\n",
      " [ 0 19  3]\n",
      " [ 3  3 12]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75        12\n",
      "           1       0.79      0.86      0.83        22\n",
      "           2       0.75      0.67      0.71        18\n",
      "\n",
      "    accuracy                           0.77        52\n",
      "   macro avg       0.76      0.76      0.76        52\n",
      "weighted avg       0.77      0.77      0.77        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Entropy: \n",
      "Testing accuracy 0.7692307692307693\n",
      "Testing F1 score: 0.7669191422388354\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  2  1]\n",
      " [ 0 19  3]\n",
      " [ 3  3 12]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75        12\n",
      "           1       0.79      0.86      0.83        22\n",
      "           2       0.75      0.67      0.71        18\n",
      "\n",
      "    accuracy                           0.77        52\n",
      "   macro avg       0.76      0.76      0.76        52\n",
      "weighted avg       0.77      0.77      0.77        52\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  207\n",
      "No. of Testing Dataset in Fold  2 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 615759.52it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 984377.47it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 792172.38it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1041032.29it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 878766.12it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 738911.43it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1205862.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Entropy: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.9038461538461539\n",
      "Testing F1 score: 0.9048884343001989\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  0  1]\n",
      " [ 0 16  2]\n",
      " [ 2  0 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91        16\n",
      "           1       1.00      0.89      0.94        18\n",
      "           2       0.84      0.89      0.86        18\n",
      "\n",
      "    accuracy                           0.90        52\n",
      "   macro avg       0.91      0.91      0.91        52\n",
      "weighted avg       0.91      0.90      0.90        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 1309533.83it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 663270.38it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 754318.79it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1154549.11it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 853707.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Entropy:  \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8653846153846154\n",
      "Testing F1 score: 0.8663273001508295\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  3  0]\n",
      " [ 0 17  1]\n",
      " [ 1  2 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.87        16\n",
      "           1       0.77      0.94      0.85        18\n",
      "           2       0.94      0.83      0.88        18\n",
      "\n",
      "    accuracy                           0.87        52\n",
      "   macro avg       0.88      0.86      0.87        52\n",
      "weighted avg       0.88      0.87      0.87        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Entropy: \n",
      "Testing accuracy 0.8653846153846154\n",
      "Testing F1 score: 0.8663273001508295\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  3  0]\n",
      " [ 0 17  1]\n",
      " [ 1  2 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.87        16\n",
      "           1       0.77      0.94      0.85        18\n",
      "           2       0.94      0.83      0.88        18\n",
      "\n",
      "    accuracy                           0.87        52\n",
      "   macro avg       0.88      0.86      0.87        52\n",
      "weighted avg       0.88      0.87      0.87        52\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  207\n",
      "No. of Testing Dataset in Fold  3 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 981040.60it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 612717.66it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 524921.96it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1273051.21it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 739540.82it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 836436.35it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 650352.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Entropy: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.9038461538461539\n",
      "Testing F1 score: 0.9038461538461539\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  0  2]\n",
      " [ 1 18  0]\n",
      " [ 1  1 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82        11\n",
      "           1       0.95      0.95      0.95        19\n",
      "           2       0.91      0.91      0.91        22\n",
      "\n",
      "    accuracy                           0.90        52\n",
      "   macro avg       0.89      0.89      0.89        52\n",
      "weighted avg       0.90      0.90      0.90        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 1197546.11it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1276795.48it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1256470.23it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1148440.38it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 796532.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Entropy:  \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8269230769230769\n",
      "Testing F1 score: 0.820609281279596\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  3  2]\n",
      " [ 0 18  1]\n",
      " [ 0  3 19]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.55      0.71        11\n",
      "           1       0.75      0.95      0.84        19\n",
      "           2       0.86      0.86      0.86        22\n",
      "\n",
      "    accuracy                           0.83        52\n",
      "   macro avg       0.87      0.79      0.80        52\n",
      "weighted avg       0.85      0.83      0.82        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Entropy: \n",
      "Testing accuracy 0.8269230769230769\n",
      "Testing F1 score: 0.820609281279596\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  3  2]\n",
      " [ 0 18  1]\n",
      " [ 0  3 19]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.55      0.71        11\n",
      "           1       0.75      0.95      0.84        19\n",
      "           2       0.86      0.86      0.86        22\n",
      "\n",
      "    accuracy                           0.83        52\n",
      "   macro avg       0.87      0.79      0.80        52\n",
      "weighted avg       0.85      0.83      0.82        52\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  207\n",
      "No. of Testing Dataset in Fold  4 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1442227.46it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1469070.94it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 363120.42it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 874341.32it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 757609.88it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 574220.19it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 781476.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Entropy: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.8846153846153846\n",
      "Testing F1 score: 0.8861103659490757\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  2  0]\n",
      " [ 1 15  1]\n",
      " [ 0  2 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90        16\n",
      "           1       0.79      0.88      0.83        17\n",
      "           2       0.94      0.89      0.92        19\n",
      "\n",
      "    accuracy                           0.88        52\n",
      "   macro avg       0.89      0.88      0.89        52\n",
      "weighted avg       0.89      0.88      0.89        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 1278675.89it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 744614.86it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 832426.58it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1207539.54it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 724725.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Entropy:  \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9230769230769231\n",
      "Testing F1 score: 0.9230551730920535\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  0  2]\n",
      " [ 1 15  1]\n",
      " [ 0  0 19]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90        16\n",
      "           1       1.00      0.88      0.94        17\n",
      "           2       0.86      1.00      0.93        19\n",
      "\n",
      "    accuracy                           0.92        52\n",
      "   macro avg       0.93      0.92      0.92        52\n",
      "weighted avg       0.93      0.92      0.92        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Entropy: \n",
      "Testing accuracy 0.9230769230769231\n",
      "Testing F1 score: 0.9230551730920535\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  0  2]\n",
      " [ 1 15  1]\n",
      " [ 0  0 19]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90        16\n",
      "           1       1.00      0.88      0.94        17\n",
      "           2       0.86      1.00      0.93        19\n",
      "\n",
      "    accuracy                           0.92        52\n",
      "   macro avg       0.93      0.92      0.92        52\n",
      "weighted avg       0.93      0.92      0.92        52\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  208\n",
      "No. of Testing Dataset in Fold  5 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [00:00<00:00, 1122799.53it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 543560.89it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1344245.35it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 867211.96it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 616983.90it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1367421.99it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 690201.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Entropy: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8235294117647058\n",
      "Testing F1 score: 0.8210552034081446\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  2  2]\n",
      " [ 0 19  2]\n",
      " [ 1  2 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.83      0.90      0.86        21\n",
      "           2       0.79      0.83      0.81        18\n",
      "\n",
      "    accuracy                           0.82        51\n",
      "   macro avg       0.83      0.80      0.81        51\n",
      "weighted avg       0.83      0.82      0.82        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 208/208 [00:00<00:00, 672122.67it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1354682.04it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 826933.87it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 942133.08it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 943151.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Entropy:  \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8431372549019608\n",
      "Testing F1 score: 0.8447155387462782\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  2  1]\n",
      " [ 2 19  0]\n",
      " [ 2  1 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.75      0.72        12\n",
      "           1       0.86      0.90      0.88        21\n",
      "           2       0.94      0.83      0.88        18\n",
      "\n",
      "    accuracy                           0.84        51\n",
      "   macro avg       0.83      0.83      0.83        51\n",
      "weighted avg       0.85      0.84      0.84        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Entropy: \n",
      "Testing accuracy 0.8431372549019608\n",
      "Testing F1 score: 0.8447155387462782\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  2  1]\n",
      " [ 2 19  0]\n",
      " [ 2  1 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.75      0.72        12\n",
      "           1       0.86      0.90      0.88        21\n",
      "           2       0.94      0.83      0.88        18\n",
      "\n",
      "    accuracy                           0.84        51\n",
      "   macro avg       0.83      0.83      0.83        51\n",
      "weighted avg       0.85      0.84      0.84        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: {}\\n\\n\" .format(total_fold))\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"entropy\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using Decision Tree with Entropy: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"entropy\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using Decision Tree with Entropy:  \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"entropy\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR Paired Model Using Decision Tree with Entropy: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Using Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 5\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  1 :  207\n",
      "No. of Testing Dataset in Fold  1 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1284350.49it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1222846.38it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 614887.34it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1411741.35it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 789291.75it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1131969.92it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 869960.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Gini: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8653846153846154\n",
      "Testing F1 score: 0.863805509395105\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  1  1]\n",
      " [ 0 21  1]\n",
      " [ 1  3 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87        12\n",
      "           1       0.84      0.95      0.89        22\n",
      "           2       0.88      0.78      0.82        18\n",
      "\n",
      "    accuracy                           0.87        52\n",
      "   macro avg       0.87      0.86      0.86        52\n",
      "weighted avg       0.87      0.87      0.86        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 1456746.52it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 989989.66it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1199200.18it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 735780.45it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 846219.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Gini:  \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8269230769230769\n",
      "Testing F1 score: 0.8202724819746098\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  2  3]\n",
      " [ 0 21  1]\n",
      " [ 1  2 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.84      0.95      0.89        22\n",
      "           2       0.79      0.83      0.81        18\n",
      "\n",
      "    accuracy                           0.83        52\n",
      "   macro avg       0.83      0.79      0.80        52\n",
      "weighted avg       0.83      0.83      0.82        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Gini: \n",
      "Testing accuracy 0.8269230769230769\n",
      "Testing F1 score: 0.8202724819746098\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  2  3]\n",
      " [ 0 21  1]\n",
      " [ 1  2 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.84      0.95      0.89        22\n",
      "           2       0.79      0.83      0.81        18\n",
      "\n",
      "    accuracy                           0.83        52\n",
      "   macro avg       0.83      0.79      0.80        52\n",
      "weighted avg       0.83      0.83      0.82        52\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  207\n",
      "No. of Testing Dataset in Fold  2 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1107424.65it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1082569.74it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 617072.44it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 822178.91it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1148440.38it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 788574.87it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1288161.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Gini: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.9038461538461539\n",
      "Testing F1 score: 0.903813833225598\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[16  0  0]\n",
      " [ 0 16  2]\n",
      " [ 3  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91        16\n",
      "           1       1.00      0.89      0.94        18\n",
      "           2       0.88      0.83      0.86        18\n",
      "\n",
      "    accuracy                           0.90        52\n",
      "   macro avg       0.91      0.91      0.90        52\n",
      "weighted avg       0.91      0.90      0.90        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 1154549.11it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 894151.32it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1015463.07it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 944745.30it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 803164.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Gini:  \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8653846153846154\n",
      "Testing F1 score: 0.8668831168831169\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  0  1]\n",
      " [ 1 15  2]\n",
      " [ 3  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86        16\n",
      "           1       1.00      0.83      0.91        18\n",
      "           2       0.83      0.83      0.83        18\n",
      "\n",
      "    accuracy                           0.87        52\n",
      "   macro avg       0.87      0.87      0.87        52\n",
      "weighted avg       0.88      0.87      0.87        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Gini: \n",
      "Testing accuracy 0.8653846153846154\n",
      "Testing F1 score: 0.8668831168831169\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  0  1]\n",
      " [ 1 15  2]\n",
      " [ 3  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86        16\n",
      "           1       1.00      0.83      0.91        18\n",
      "           2       0.83      0.83      0.83        18\n",
      "\n",
      "    accuracy                           0.87        52\n",
      "   macro avg       0.87      0.87      0.87        52\n",
      "weighted avg       0.88      0.87      0.87        52\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  207\n",
      "No. of Testing Dataset in Fold  3 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1247443.86it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1278675.89it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 873461.70it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 752357.82it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1350265.83it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1291995.43it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 673039.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Gini: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8653846153846154\n",
      "Testing F1 score: 0.8640468227424749\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  0  3]\n",
      " [ 1 17  1]\n",
      " [ 0  2 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.73      0.80        11\n",
      "           1       0.89      0.89      0.89        19\n",
      "           2       0.83      0.91      0.87        22\n",
      "\n",
      "    accuracy                           0.87        52\n",
      "   macro avg       0.87      0.84      0.85        52\n",
      "weighted avg       0.87      0.87      0.86        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 1233268.36it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1371597.04it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1288161.61it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 840484.93it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1507328.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Gini:  \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8076923076923077\n",
      "Testing F1 score: 0.8012820512820512\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  1  4]\n",
      " [ 1 16  2]\n",
      " [ 0  2 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.55      0.67        11\n",
      "           1       0.84      0.84      0.84        19\n",
      "           2       0.77      0.91      0.83        22\n",
      "\n",
      "    accuracy                           0.81        52\n",
      "   macro avg       0.82      0.77      0.78        52\n",
      "weighted avg       0.81      0.81      0.80        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Gini: \n",
      "Testing accuracy 0.8076923076923077\n",
      "Testing F1 score: 0.8012820512820512\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  1  4]\n",
      " [ 1 16  2]\n",
      " [ 0  2 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.55      0.67        11\n",
      "           1       0.84      0.84      0.84        19\n",
      "           2       0.77      0.91      0.83        22\n",
      "\n",
      "    accuracy                           0.81        52\n",
      "   macro avg       0.82      0.77      0.78        52\n",
      "weighted avg       0.81      0.81      0.80        52\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  207\n",
      "No. of Testing Dataset in Fold  4 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 568952.12it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 661249.75it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 719917.85it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 711073.65it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1028697.78it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 809907.58it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 670958.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Gini: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9038461538461539\n",
      "Testing F1 score: 0.9038461538461539\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 2 15  0]\n",
      " [ 0  1 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88        16\n",
      "           1       0.88      0.88      0.88        17\n",
      "           2       0.95      0.95      0.95        19\n",
      "\n",
      "    accuracy                           0.90        52\n",
      "   macro avg       0.90      0.90      0.90        52\n",
      "weighted avg       0.90      0.90      0.90        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 598360.39it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1561548.43it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 532650.88it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 692913.75it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1083921.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Gini:  \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9230769230769231\n",
      "Testing F1 score: 0.9238157004286036\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  0  1]\n",
      " [ 0 15  2]\n",
      " [ 0  1 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        16\n",
      "           1       0.94      0.88      0.91        17\n",
      "           2       0.86      0.95      0.90        19\n",
      "\n",
      "    accuracy                           0.92        52\n",
      "   macro avg       0.93      0.92      0.93        52\n",
      "weighted avg       0.93      0.92      0.92        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Gini: \n",
      "Testing accuracy 0.9230769230769231\n",
      "Testing F1 score: 0.9238157004286036\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  0  1]\n",
      " [ 0 15  2]\n",
      " [ 0  1 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        16\n",
      "           1       0.94      0.88      0.91        17\n",
      "           2       0.86      0.95      0.90        19\n",
      "\n",
      "    accuracy                           0.92        52\n",
      "   macro avg       0.93      0.92      0.93        52\n",
      "weighted avg       0.93      0.92      0.92        52\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  208\n",
      "No. of Testing Dataset in Fold  5 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [00:00<00:00, 1407121.34it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1230486.93it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1363148.80it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 686400.65it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 637264.60it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 412099.78it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1346319.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Gini: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8235294117647058\n",
      "Testing F1 score: 0.821218573108558\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  3  1]\n",
      " [ 0 18  3]\n",
      " [ 1  1 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.82      0.86      0.84        21\n",
      "           2       0.80      0.89      0.84        18\n",
      "\n",
      "    accuracy                           0.82        51\n",
      "   macro avg       0.84      0.80      0.81        51\n",
      "weighted avg       0.83      0.82      0.82        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 208/208 [00:00<00:00, 995907.80it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 405021.00it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 760606.13it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 803328.94it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 676815.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Gini:  \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8431372549019608\n",
      "Testing F1 score: 0.8377395133826607\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  2  2]\n",
      " [ 0 21  0]\n",
      " [ 2  2 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73        12\n",
      "           1       0.84      1.00      0.91        21\n",
      "           2       0.88      0.78      0.82        18\n",
      "\n",
      "    accuracy                           0.84        51\n",
      "   macro avg       0.84      0.81      0.82        51\n",
      "weighted avg       0.84      0.84      0.84        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Gini: \n",
      "Testing accuracy 0.8431372549019608\n",
      "Testing F1 score: 0.8377395133826607\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  2  2]\n",
      " [ 0 21  0]\n",
      " [ 2  2 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73        12\n",
      "           1       0.84      1.00      0.91        21\n",
      "           2       0.88      0.78      0.82        18\n",
      "\n",
      "    accuracy                           0.84        51\n",
      "   macro avg       0.84      0.81      0.82        51\n",
      "weighted avg       0.84      0.84      0.84        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: {}\\n\\n\" .format(total_fold))\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using Decision Tree with Gini: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using Decision Tree with Gini:  \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR Paired Model Using Decision Tree with Gini: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbour with Minkowski Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 5\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  1 :  207\n",
      "No. of Testing Dataset in Fold  1 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1238546.26it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 622827.06it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1200858.82it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 943718.40it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 820624.70it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1060098.81it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 793620.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Minkowski Distance: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8846153846153846\n",
      "Testing F1 score: 0.8833134950156226\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  1  0]\n",
      " [ 0 21  1]\n",
      " [ 1  3 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92        12\n",
      "           1       0.84      0.95      0.89        22\n",
      "           2       0.93      0.78      0.85        18\n",
      "\n",
      "    accuracy                           0.88        52\n",
      "   macro avg       0.90      0.88      0.89        52\n",
      "weighted avg       0.89      0.88      0.88        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 561228.78it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 706445.02it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 848700.81it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 860476.64it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 939633.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Minkowski Distance: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8846153846153846\n",
      "Testing F1 score: 0.8836598184424271\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  1  1]\n",
      " [ 0 21  1]\n",
      " [ 1  2 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87        12\n",
      "           1       0.88      0.95      0.91        22\n",
      "           2       0.88      0.83      0.86        18\n",
      "\n",
      "    accuracy                           0.88        52\n",
      "   macro avg       0.89      0.87      0.88        52\n",
      "weighted avg       0.89      0.88      0.88        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Minkowski Distance: \n",
      "Testing accuracy 0.8846153846153846\n",
      "Testing F1 score: 0.8836598184424271\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  1  1]\n",
      " [ 0 21  1]\n",
      " [ 1  2 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87        12\n",
      "           1       0.88      0.95      0.91        22\n",
      "           2       0.88      0.83      0.86        18\n",
      "\n",
      "    accuracy                           0.88        52\n",
      "   macro avg       0.89      0.87      0.88        52\n",
      "weighted avg       0.89      0.88      0.88        52\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  207\n",
      "No. of Testing Dataset in Fold  2 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1282453.36it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 880548.61it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 873461.70it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1550394.51it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 615323.12it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 815997.11it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 798731.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Minkowski Distance: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.9038461538461539\n",
      "Testing F1 score: 0.9047259929612872\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[16  0  0]\n",
      " [ 1 16  1]\n",
      " [ 3  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89        16\n",
      "           1       1.00      0.89      0.94        18\n",
      "           2       0.94      0.83      0.88        18\n",
      "\n",
      "    accuracy                           0.90        52\n",
      "   macro avg       0.91      0.91      0.90        52\n",
      "weighted avg       0.92      0.90      0.90        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 1071877.69it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1295852.13it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 573461.64it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 488588.03it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 693467.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Minkowski Distance: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.9038461538461539\n",
      "Testing F1 score: 0.9047259929612872\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[16  0  0]\n",
      " [ 1 16  1]\n",
      " [ 3  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89        16\n",
      "           1       1.00      0.89      0.94        18\n",
      "           2       0.94      0.83      0.88        18\n",
      "\n",
      "    accuracy                           0.90        52\n",
      "   macro avg       0.91      0.91      0.90        52\n",
      "weighted avg       0.92      0.90      0.90        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Minkowski Distance: \n",
      "Testing accuracy 0.9038461538461539\n",
      "Testing F1 score: 0.9047259929612872\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[16  0  0]\n",
      " [ 1 16  1]\n",
      " [ 3  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89        16\n",
      "           1       1.00      0.89      0.94        18\n",
      "           2       0.94      0.83      0.88        18\n",
      "\n",
      "    accuracy                           0.90        52\n",
      "   macro avg       0.91      0.91      0.90        52\n",
      "weighted avg       0.92      0.90      0.90        52\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  207\n",
      "No. of Testing Dataset in Fold  3 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1378128.46it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1032367.33it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1290075.67it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 869960.85it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1044790.53it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1118841.40it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 527793.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Minkowski Distance: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8461538461538461\n",
      "Testing F1 score: 0.8388278388278388\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 6  3  2]\n",
      " [ 1 18  0]\n",
      " [ 0  2 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.55      0.67        11\n",
      "           1       0.78      0.95      0.86        19\n",
      "           2       0.91      0.91      0.91        22\n",
      "\n",
      "    accuracy                           0.85        52\n",
      "   macro avg       0.85      0.80      0.81        52\n",
      "weighted avg       0.85      0.85      0.84        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 1295852.13it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1133447.69it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 778673.48it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 906284.89it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1331627.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Minkowski Distance: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8461538461538461\n",
      "Testing F1 score: 0.8432692307692308\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  3  1]\n",
      " [ 1 17  1]\n",
      " [ 1  1 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.64      0.70        11\n",
      "           1       0.81      0.89      0.85        19\n",
      "           2       0.91      0.91      0.91        22\n",
      "\n",
      "    accuracy                           0.85        52\n",
      "   macro avg       0.83      0.81      0.82        52\n",
      "weighted avg       0.84      0.85      0.84        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Minkowski Distance: \n",
      "Testing accuracy 0.8461538461538461\n",
      "Testing F1 score: 0.8432692307692308\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  3  1]\n",
      " [ 1 17  1]\n",
      " [ 1  1 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.64      0.70        11\n",
      "           1       0.81      0.89      0.85        19\n",
      "           2       0.91      0.91      0.91        22\n",
      "\n",
      "    accuracy                           0.85        52\n",
      "   macro avg       0.83      0.81      0.82        52\n",
      "weighted avg       0.84      0.85      0.84        52\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  207\n",
      "No. of Testing Dataset in Fold  4 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1389153.48it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 614452.18it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1598933.57it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 602512.79it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 715173.75it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 787859.28it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 396629.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Minkowski Distance: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9230769230769231\n",
      "Testing F1 score: 0.9236911736911737\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  1  0]\n",
      " [ 2 15  0]\n",
      " [ 0  1 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91        16\n",
      "           1       0.88      0.88      0.88        17\n",
      "           2       1.00      0.95      0.97        19\n",
      "\n",
      "    accuracy                           0.92        52\n",
      "   macro avg       0.92      0.92      0.92        52\n",
      "weighted avg       0.93      0.92      0.92        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 213689.62it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1202522.06it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1337782.63it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1323507.51it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 785009.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Minkowski Distance: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9230769230769231\n",
      "Testing F1 score: 0.9231214731214732\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  1  0]\n",
      " [ 0 16  1]\n",
      " [ 1  1 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        16\n",
      "           1       0.89      0.94      0.91        17\n",
      "           2       0.94      0.89      0.92        19\n",
      "\n",
      "    accuracy                           0.92        52\n",
      "   macro avg       0.92      0.92      0.92        52\n",
      "weighted avg       0.92      0.92      0.92        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Minkowski Distance: \n",
      "Testing accuracy 0.9230769230769231\n",
      "Testing F1 score: 0.9231214731214732\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  1  0]\n",
      " [ 0 16  1]\n",
      " [ 1  1 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        16\n",
      "           1       0.89      0.94      0.91        17\n",
      "           2       0.94      0.89      0.92        19\n",
      "\n",
      "    accuracy                           0.92        52\n",
      "   macro avg       0.92      0.92      0.92        52\n",
      "weighted avg       0.92      0.92      0.92        52\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  208\n",
      "No. of Testing Dataset in Fold  5 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [00:00<00:00, 1251671.78it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1504164.19it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 985779.92it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1104323.08it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 650085.87it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1530553.04it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1609622.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Minkowski Distance: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8823529411764706\n",
      "Testing F1 score: 0.8788843196853209\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  3  1]\n",
      " [ 0 21  0]\n",
      " [ 0  2 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80        12\n",
      "           1       0.81      1.00      0.89        21\n",
      "           2       0.94      0.89      0.91        18\n",
      "\n",
      "    accuracy                           0.88        51\n",
      "   macro avg       0.92      0.85      0.87        51\n",
      "weighted avg       0.90      0.88      0.88        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 208/208 [00:00<00:00, 1481180.36it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1260715.65it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 847828.21it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1574756.74it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1131537.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Minkowski Distance: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8823529411764706\n",
      "Testing F1 score: 0.8816363754804246\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  1  1]\n",
      " [ 0 20  1]\n",
      " [ 2  1 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83        12\n",
      "           1       0.91      0.95      0.93        21\n",
      "           2       0.88      0.83      0.86        18\n",
      "\n",
      "    accuracy                           0.88        51\n",
      "   macro avg       0.87      0.87      0.87        51\n",
      "weighted avg       0.88      0.88      0.88        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Minkowski Distance: \n",
      "Testing accuracy 0.8823529411764706\n",
      "Testing F1 score: 0.8816363754804246\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  1  1]\n",
      " [ 0 20  1]\n",
      " [ 2  1 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83        12\n",
      "           1       0.91      0.95      0.93        21\n",
      "           2       0.88      0.83      0.86        18\n",
      "\n",
      "    accuracy                           0.88        51\n",
      "   macro avg       0.87      0.87      0.87        51\n",
      "weighted avg       0.88      0.88      0.88        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: {}\\n\\n\" .format(total_fold))\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "        \n",
    "    knn_dbow = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski')\n",
    "    knn_dbow.fit(X_trained, y_trained)\n",
    "    y_pred = knn_dbow.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using KNN with Minkowski Distance: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    knn_dm = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski')\n",
    "    knn_dm.fit(X_trained, y_trained)\n",
    "    y_pred = knn_dm.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using KNN with Minkowski Distance: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    knn_mixed = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski')\n",
    "    knn_mixed.fit(X_trained, y_trained)\n",
    "    y_pred = knn_mixed.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PAIRED Model Using KNN with Minkowski Distance: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbour with Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 5\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  1 :  207\n",
      "No. of Testing Dataset in Fold  1 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1149961.49it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1056229.84it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1263785.92it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1348169.14it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1070556.01it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1258291.20it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 699050.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Euclidean Distance: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8846153846153846\n",
      "Testing F1 score: 0.8833134950156226\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  1  0]\n",
      " [ 0 21  1]\n",
      " [ 1  3 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92        12\n",
      "           1       0.84      0.95      0.89        22\n",
      "           2       0.93      0.78      0.85        18\n",
      "\n",
      "    accuracy                           0.88        52\n",
      "   macro avg       0.90      0.88      0.89        52\n",
      "weighted avg       0.89      0.88      0.88        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 962550.92it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 956190.45it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 523024.66it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 606299.53it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 530373.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Euclidean Distance: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8461538461538461\n",
      "Testing F1 score: 0.8414442700156985\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  1  3]\n",
      " [ 0 22  0]\n",
      " [ 0  4 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80        12\n",
      "           1       0.81      1.00      0.90        22\n",
      "           2       0.82      0.78      0.80        18\n",
      "\n",
      "    accuracy                           0.85        52\n",
      "   macro avg       0.88      0.81      0.83        52\n",
      "weighted avg       0.86      0.85      0.84        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Euclidean Distance: \n",
      "Testing accuracy 0.8461538461538461\n",
      "Testing F1 score: 0.8414442700156985\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  1  3]\n",
      " [ 0 22  0]\n",
      " [ 0  4 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80        12\n",
      "           1       0.81      1.00      0.90        22\n",
      "           2       0.82      0.78      0.80        18\n",
      "\n",
      "    accuracy                           0.85        52\n",
      "   macro avg       0.88      0.81      0.83        52\n",
      "weighted avg       0.86      0.85      0.84        52\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  207\n",
      "No. of Testing Dataset in Fold  2 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1245654.13it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1202522.06it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1274920.60it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1117401.45it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1481605.68it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1110256.94it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 817533.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Euclidean Distance: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8846153846153846\n",
      "Testing F1 score: 0.8834518422753717\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[16  0  0]\n",
      " [ 1 15  2]\n",
      " [ 2  1 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91        16\n",
      "           1       0.94      0.83      0.88        18\n",
      "           2       0.88      0.83      0.86        18\n",
      "\n",
      "    accuracy                           0.88        52\n",
      "   macro avg       0.89      0.89      0.88        52\n",
      "weighted avg       0.89      0.88      0.88        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 782181.02it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 518650.49it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 958301.25it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1536674.21it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 831629.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Euclidean Distance: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.9038461538461539\n",
      "Testing F1 score: 0.9047259929612872\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[16  0  0]\n",
      " [ 1 16  1]\n",
      " [ 3  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89        16\n",
      "           1       1.00      0.89      0.94        18\n",
      "           2       0.94      0.83      0.88        18\n",
      "\n",
      "    accuracy                           0.90        52\n",
      "   macro avg       0.91      0.91      0.90        52\n",
      "weighted avg       0.92      0.90      0.90        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Euclidean Distance: \n",
      "Testing accuracy 0.9038461538461539\n",
      "Testing F1 score: 0.9047259929612872\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[16  0  0]\n",
      " [ 1 16  1]\n",
      " [ 3  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89        16\n",
      "           1       1.00      0.89      0.94        18\n",
      "           2       0.94      0.83      0.88        18\n",
      "\n",
      "    accuracy                           0.90        52\n",
      "   macro avg       0.91      0.91      0.90        52\n",
      "weighted avg       0.92      0.90      0.90        52\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  207\n",
      "No. of Testing Dataset in Fold  3 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 561955.29it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1229774.69it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1149961.49it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1523194.61it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 986614.69it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 829246.35it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 773815.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Euclidean Distance: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.9038461538461539\n",
      "Testing F1 score: 0.9037310979618672\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  0  2]\n",
      " [ 0 18  1]\n",
      " [ 0  2 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.82      0.90        11\n",
      "           1       0.90      0.95      0.92        19\n",
      "           2       0.87      0.91      0.89        22\n",
      "\n",
      "    accuracy                           0.90        52\n",
      "   macro avg       0.92      0.89      0.90        52\n",
      "weighted avg       0.91      0.90      0.90        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 1159173.47it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 609277.84it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 761597.31it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 502442.67it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1367277.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Euclidean Distance: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8269230769230769\n",
      "Testing F1 score: 0.8231063746236746\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  1  3]\n",
      " [ 1 18  0]\n",
      " [ 1  3 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.64      0.70        11\n",
      "           1       0.82      0.95      0.88        19\n",
      "           2       0.86      0.82      0.84        22\n",
      "\n",
      "    accuracy                           0.83        52\n",
      "   macro avg       0.82      0.80      0.81        52\n",
      "weighted avg       0.83      0.83      0.82        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Euclidean Distance: \n",
      "Testing accuracy 0.8269230769230769\n",
      "Testing F1 score: 0.8231063746236746\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  1  3]\n",
      " [ 1 18  0]\n",
      " [ 1  3 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.64      0.70        11\n",
      "           1       0.82      0.95      0.88        19\n",
      "           2       0.86      0.82      0.84        22\n",
      "\n",
      "    accuracy                           0.83        52\n",
      "   macro avg       0.82      0.80      0.81        52\n",
      "weighted avg       0.83      0.83      0.82        52\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  207\n",
      "No. of Testing Dataset in Fold  4 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1311511.98it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 771066.54it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 728373.26it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 596715.41it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1273051.21it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1319484.69it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 637460.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Euclidean Distance: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9615384615384616\n",
      "Testing F1 score: 0.9620367886496919\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  1  0]\n",
      " [ 0 17  0]\n",
      " [ 0  1 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        16\n",
      "           1       0.89      1.00      0.94        17\n",
      "           2       1.00      0.95      0.97        19\n",
      "\n",
      "    accuracy                           0.96        52\n",
      "   macro avg       0.96      0.96      0.96        52\n",
      "weighted avg       0.97      0.96      0.96        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 1212599.06it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 822958.23it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1038541.78it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 711073.65it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1165397.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Euclidean Distance: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9038461538461539\n",
      "Testing F1 score: 0.90502553002553\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  1  0]\n",
      " [ 2 15  0]\n",
      " [ 0  2 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91        16\n",
      "           1       0.83      0.88      0.86        17\n",
      "           2       1.00      0.89      0.94        19\n",
      "\n",
      "    accuracy                           0.90        52\n",
      "   macro avg       0.91      0.90      0.90        52\n",
      "weighted avg       0.91      0.90      0.91        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Euclidean Distance: \n",
      "Testing accuracy 0.9038461538461539\n",
      "Testing F1 score: 0.90502553002553\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  1  0]\n",
      " [ 2 15  0]\n",
      " [ 0  2 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91        16\n",
      "           1       0.83      0.88      0.86        17\n",
      "           2       1.00      0.89      0.94        19\n",
      "\n",
      "    accuracy                           0.90        52\n",
      "   macro avg       0.91      0.90      0.90        52\n",
      "weighted avg       0.91      0.90      0.91        52\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  208\n",
      "No. of Testing Dataset in Fold  5 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [00:00<00:00, 1246307.47it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1195089.36it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 925148.71it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1541369.67it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1376049.26it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1262540.13it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 644800.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Euclidean Distance: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8235294117647058\n",
      "Testing F1 score: 0.8210552034081446\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  2  2]\n",
      " [ 0 19  2]\n",
      " [ 1  2 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.83      0.90      0.86        21\n",
      "           2       0.79      0.83      0.81        18\n",
      "\n",
      "    accuracy                           0.82        51\n",
      "   macro avg       0.83      0.80      0.81        51\n",
      "weighted avg       0.83      0.82      0.82        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 208/208 [00:00<00:00, 1269891.17it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1336011.08it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 763936.28it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 692393.04it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 681042.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Euclidean Distance: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8431372549019608\n",
      "Testing F1 score: 0.8408137338481855\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  2  1]\n",
      " [ 0 20  1]\n",
      " [ 2  2 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78        12\n",
      "           1       0.83      0.95      0.89        21\n",
      "           2       0.88      0.78      0.82        18\n",
      "\n",
      "    accuracy                           0.84        51\n",
      "   macro avg       0.84      0.83      0.83        51\n",
      "weighted avg       0.84      0.84      0.84        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Euclidean Distance: \n",
      "Testing accuracy 0.8431372549019608\n",
      "Testing F1 score: 0.8408137338481855\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  2  1]\n",
      " [ 0 20  1]\n",
      " [ 2  2 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78        12\n",
      "           1       0.83      0.95      0.89        21\n",
      "           2       0.88      0.78      0.82        18\n",
      "\n",
      "    accuracy                           0.84        51\n",
      "   macro avg       0.84      0.83      0.83        51\n",
      "weighted avg       0.84      0.84      0.84        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: {}\\n\\n\" .format(total_fold))\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "        \n",
    "    knn_dbow = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\n",
    "    knn_dbow.fit(X_trained, y_trained)\n",
    "    y_pred = knn_dbow.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using KNN with Euclidean Distance: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    knn_dm = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\n",
    "    knn_dm.fit(X_trained, y_trained)\n",
    "    y_pred = knn_dm.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using KNN with Euclidean Distance: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    knn_mixed = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\n",
    "    knn_mixed.fit(X_trained, y_trained)\n",
    "    y_pred = knn_mixed.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PAIRED Model Using KNN with Euclidean Distance: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold With Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 5\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  1 :  207\n",
      "No. of Testing Dataset in Fold  1 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1197546.11it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 513132.94it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 849531.24it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1104606.78it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1103203.21it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 772438.55it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1199200.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Gaussian Naive Bayes: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8653846153846154\n",
      "Testing F1 score: 0.8644993114302577\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  1  0]\n",
      " [ 0 20  2]\n",
      " [ 1  3 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92        12\n",
      "           1       0.83      0.91      0.87        22\n",
      "           2       0.88      0.78      0.82        18\n",
      "\n",
      "    accuracy                           0.87        52\n",
      "   macro avg       0.88      0.87      0.87        52\n",
      "weighted avg       0.87      0.87      0.86        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 816764.75it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1327554.94it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1404888.23it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 735780.45it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 899710.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Gaussian Naive Bayes: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8653846153846154\n",
      "Testing F1 score: 0.8644993114302577\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  1  0]\n",
      " [ 0 20  2]\n",
      " [ 1  3 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92        12\n",
      "           1       0.83      0.91      0.87        22\n",
      "           2       0.88      0.78      0.82        18\n",
      "\n",
      "    accuracy                           0.87        52\n",
      "   macro avg       0.88      0.87      0.87        52\n",
      "weighted avg       0.87      0.87      0.86        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using Gaussian Naive Bayes: \n",
      "Testing accuracy 0.8653846153846154\n",
      "Testing F1 score: 0.8616995749348692\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  1  2]\n",
      " [ 0 22  0]\n",
      " [ 1  3 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.75      0.82        12\n",
      "           1       0.85      1.00      0.92        22\n",
      "           2       0.88      0.78      0.82        18\n",
      "\n",
      "    accuracy                           0.87        52\n",
      "   macro avg       0.87      0.84      0.85        52\n",
      "weighted avg       0.87      0.87      0.86        52\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  207\n",
      "No. of Testing Dataset in Fold  2 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 1217701.16it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1423313.00it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1071877.69it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1258291.20it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 787859.28it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 583481.81it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 683638.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Gaussian Naive Bayes: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.9038461538461539\n",
      "Testing F1 score: 0.903813833225598\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[16  0  0]\n",
      " [ 0 16  2]\n",
      " [ 3  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91        16\n",
      "           1       1.00      0.89      0.94        18\n",
      "           2       0.88      0.83      0.86        18\n",
      "\n",
      "    accuracy                           0.90        52\n",
      "   macro avg       0.91      0.91      0.90        52\n",
      "weighted avg       0.91      0.90      0.90        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 561591.80it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 814466.16it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1404888.23it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 569698.77it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 728373.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Gaussian Naive Bayes: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.9038461538461539\n",
      "Testing F1 score: 0.903813833225598\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[16  0  0]\n",
      " [ 0 16  2]\n",
      " [ 3  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91        16\n",
      "           1       1.00      0.89      0.94        18\n",
      "           2       0.88      0.83      0.86        18\n",
      "\n",
      "    accuracy                           0.90        52\n",
      "   macro avg       0.91      0.91      0.90        52\n",
      "weighted avg       0.91      0.90      0.90        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using Gaussian Naive Bayes: \n",
      "Testing accuracy 0.9038461538461539\n",
      "Testing F1 score: 0.9036963036963037\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[16  0  0]\n",
      " [ 1 15  2]\n",
      " [ 2  0 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91        16\n",
      "           1       1.00      0.83      0.91        18\n",
      "           2       0.89      0.89      0.89        18\n",
      "\n",
      "    accuracy                           0.90        52\n",
      "   macro avg       0.91      0.91      0.90        52\n",
      "weighted avg       0.91      0.90      0.90        52\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  207\n",
      "No. of Testing Dataset in Fold  3 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 680957.59it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1209221.35it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 852869.28it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 631433.40it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 736404.52it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1284350.49it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 775197.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Gaussian Naive Bayes: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8846153846153846\n",
      "Testing F1 score: 0.8830521817363923\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  1  2]\n",
      " [ 0 18  1]\n",
      " [ 0  2 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.73      0.84        11\n",
      "           1       0.86      0.95      0.90        19\n",
      "           2       0.87      0.91      0.89        22\n",
      "\n",
      "    accuracy                           0.88        52\n",
      "   macro avg       0.91      0.86      0.88        52\n",
      "weighted avg       0.89      0.88      0.88        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 1215995.70it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 636525.61it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 610563.24it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1352369.05it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1536674.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Gaussian Naive Bayes: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8846153846153846\n",
      "Testing F1 score: 0.8830521817363923\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  1  2]\n",
      " [ 0 18  1]\n",
      " [ 0  2 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.73      0.84        11\n",
      "           1       0.86      0.95      0.90        19\n",
      "           2       0.87      0.91      0.89        22\n",
      "\n",
      "    accuracy                           0.88        52\n",
      "   macro avg       0.91      0.86      0.88        52\n",
      "weighted avg       0.89      0.88      0.88        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using Gaussian Naive Bayes: \n",
      "Testing accuracy 0.8846153846153846\n",
      "Testing F1 score: 0.8795883940620782\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  2  2]\n",
      " [ 1 18  0]\n",
      " [ 0  1 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.64      0.74        11\n",
      "           1       0.86      0.95      0.90        19\n",
      "           2       0.91      0.95      0.93        22\n",
      "\n",
      "    accuracy                           0.88        52\n",
      "   macro avg       0.88      0.85      0.86        52\n",
      "weighted avg       0.88      0.88      0.88        52\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  207\n",
      "No. of Testing Dataset in Fold  4 :  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 898779.43it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1238546.26it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 908180.89it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 782886.32it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1214295.00it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 839672.08it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1037300.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Gaussian Naive Bayes: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9230769230769231\n",
      "Testing F1 score: 0.9229424421732114\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  0  1]\n",
      " [ 1 15  1]\n",
      " [ 0  1 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        16\n",
      "           1       0.94      0.88      0.91        17\n",
      "           2       0.90      0.95      0.92        19\n",
      "\n",
      "    accuracy                           0.92        52\n",
      "   macro avg       0.92      0.92      0.92        52\n",
      "weighted avg       0.92      0.92      0.92        52\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 207/207 [00:00<00:00, 1192611.16it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 476259.42it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 640753.45it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1430347.49it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 879656.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Gaussian Naive Bayes: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9230769230769231\n",
      "Testing F1 score: 0.9229424421732114\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  0  1]\n",
      " [ 1 15  1]\n",
      " [ 0  1 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        16\n",
      "           1       0.94      0.88      0.91        17\n",
      "           2       0.90      0.95      0.92        19\n",
      "\n",
      "    accuracy                           0.92        52\n",
      "   macro avg       0.92      0.92      0.92        52\n",
      "weighted avg       0.92      0.92      0.92        52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using Gaussian Naive Bayes: \n",
      "Testing accuracy 0.9038461538461539\n",
      "Testing F1 score: 0.9044401544401544\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  1  0]\n",
      " [ 1 15  1]\n",
      " [ 0  2 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        16\n",
      "           1       0.83      0.88      0.86        17\n",
      "           2       0.94      0.89      0.92        19\n",
      "\n",
      "    accuracy                           0.90        52\n",
      "   macro avg       0.91      0.90      0.90        52\n",
      "weighted avg       0.91      0.90      0.90        52\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  208\n",
      "No. of Testing Dataset in Fold  5 :  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [00:00<00:00, 591869.22it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1344245.35it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1583330.73it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 883011.37it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 778942.17it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1671293.55it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 709280.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Gaussian Naive Bayes: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.8600170502983803\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  3  1]\n",
      " [ 0 20  1]\n",
      " [ 0  2 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80        12\n",
      "           1       0.80      0.95      0.87        21\n",
      "           2       0.89      0.89      0.89        18\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.90      0.84      0.85        51\n",
      "weighted avg       0.88      0.86      0.86        51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 208/208 [00:00<00:00, 1223583.78it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 809290.57it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1097377.65it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 723395.71it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 806298.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Gaussian Naive Bayes: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8627450980392157\n",
      "Testing F1 score: 0.8600170502983803\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  3  1]\n",
      " [ 0 20  1]\n",
      " [ 0  2 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80        12\n",
      "           1       0.80      0.95      0.87        21\n",
      "           2       0.89      0.89      0.89        18\n",
      "\n",
      "    accuracy                           0.86        51\n",
      "   macro avg       0.90      0.84      0.85        51\n",
      "weighted avg       0.88      0.86      0.86        51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using Gaussian Naive Bayes: \n",
      "Testing accuracy 0.8431372549019608\n",
      "Testing F1 score: 0.8390243902439024\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  1  4]\n",
      " [ 0 19  2]\n",
      " [ 1  0 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.95      0.90      0.93        21\n",
      "           2       0.74      0.94      0.83        18\n",
      "\n",
      "    accuracy                           0.84        51\n",
      "   macro avg       0.85      0.81      0.82        51\n",
      "weighted avg       0.86      0.84      0.84        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: {}\\n\\n\" .format(total_fold))\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "        \n",
    "    nb_gaussian_pv_dbow = GaussianNB()\n",
    "    nb_gaussian_pv_dbow.fit(X_trained, y_trained)\n",
    "    y_pred = nb_gaussian_pv_dbow.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using Gaussian Naive Bayes: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    nb_gaussian_pv_dm = GaussianNB()\n",
    "    nb_gaussian_pv_dm.fit(X_trained, y_trained)\n",
    "    y_pred_pv_dm = nb_gaussian_pv_dm.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using Gaussian Naive Bayes: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    nb_gaussian_mixed = GaussianNB()\n",
    "    nb_gaussian_mixed.fit(X_trained, y_trained)\n",
    "    y_pred = nb_gaussian_mixed.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PAIRED Model Using Gaussian Naive Bayes: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
