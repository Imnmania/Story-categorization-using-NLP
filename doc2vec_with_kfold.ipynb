{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nybsys/anaconda3/envs/nlp/lib/python3.7/site-packages/tqdm/std.py:656: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['doc2vec.ipynb',\n",
       " 'doc2vec_with_kfold.html',\n",
       " 'doc2vec_with_kfold.ipynb',\n",
       " 'kmeans_clustering.html',\n",
       " 'kmeans_clustering.ipynb',\n",
       " 'processed_dataset.csv',\n",
       " 'readme.docx',\n",
       " 'sentiment_analysis_from_story_preprocessing.html',\n",
       " 'sentiment_analysis_from_story_preprocessing.ipynb',\n",
       " 'sentiment_dictionary.csv',\n",
       " 'word2vec.model',\n",
       " 'Collective_Dataset',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import multiprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "cores = multiprocessing.cpu_count()\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.getcwd() + \"/Collective_Dataset/419_data - Sheet1.csv\", usecols=[0,1])#header=None, , names=['story', 'category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just like any other day, employees arrived in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My so-called ‘friends’ in middle school used t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i have been called hurtful names and i have be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at my old school kids would hit me and call me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I had debilitating migraines for three years b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I love my work, but hate going each day becaus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I have a chronic illness which was doing well ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The other part is that sense of worthlessness....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I feel my whole body hurting. My mental health...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>As a librarian, I've been threatened with stal...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               story  category\n",
       "0  Just like any other day, employees arrived in ...         0\n",
       "1  My so-called ‘friends’ in middle school used t...         1\n",
       "2  i have been called hurtful names and i have be...         1\n",
       "3  at my old school kids would hit me and call me...         1\n",
       "4  I had debilitating migraines for three years b...         0\n",
       "5  I love my work, but hate going each day becaus...         0\n",
       "6  I have a chronic illness which was doing well ...         0\n",
       "7  The other part is that sense of worthlessness....         0\n",
       "8  I feel my whole body hurting. My mental health...         0\n",
       "9  As a librarian, I've been threatened with stal...         2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Just like any other day, employees arrived in ...\n",
       "1      My so-called ‘friends’ in middle school used t...\n",
       "2      i have been called hurtful names and i have be...\n",
       "3      at my old school kids would hit me and call me...\n",
       "4      I had debilitating migraines for three years b...\n",
       "                             ...                        \n",
       "265    I worked at a call center for over 5 years. It...\n",
       "266    I work in a call center as a supervisor. That ...\n",
       "267    I too went throygh a tough 18 month period of ...\n",
       "268    I was employed as a testing Co-ordinator on a ...\n",
       "269    I'm also an injured worker. My workplace havin...\n",
       "Name: story, Length: 270, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      0\n",
       "      ..\n",
       "265    0\n",
       "266    0\n",
       "267    0\n",
       "268    0\n",
       "269    0\n",
       "Name: category, Length: 270, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = range(270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50346"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.story.apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtYAAAEGCAYAAABBxtJ0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAYtklEQVR4nO3debRdZZ3m8e8jSMugjUBAhISgoEJZTh0oHJYTDiggdpc4LA00UlJWOZYTWN1KtW0pWjaWVVa5CgWNgBILUCiwtSiURaOtGJwHUMRAAhECEi2cGPLrP85Oe7zmJtucve+9O/f7Weuse867zznvQ9a65GHz7nenqpAkSZI0mXvNdgBJkiRpa2CxliRJkjpgsZYkSZI6YLGWJEmSOmCxliRJkjpgsZYkSZI6sO1MTJLkDOAI4JaqengztguwHFgMrASeX1W3JwnwPuDZwC+A/1pVX93cHLvttlstXry4l/ySJEnSBlddddWtVbVg6viMFGvgI8D7gY+OjZ0EXFpVpyQ5qXl9IvAsYP/m8UfAB5qfm7R48WJWrFjRcWxJkiTptyW5fmPjM7IUpKouB34yZfgoYFnzfBnw3LHxj9bIl4Cdk+w5EzklSZKkLTWba6z3qKo1AM3P3ZvxvYBVY+9b3YxJkiRJc9ZcvHgxGxnb6H3Xk5yQZEWSFWvXru05liRJkjS92SzWN29Y4tH8vKUZXw0sHHvf3sBNG/uCqjqtqpZU1ZIFC35n/bgkSZI0Y2azWF8IHNs8Pxa4YGz8mIwcAvx0w5IRSZIkaa6aqe32Pg48GdgtyWrgZOAU4BNJjgduAI5u3v5pRlvtXctou73jZiKjJEmSNIkZKdZV9aJpDh26kfcW8Ip+E0mSJEndmosXL0qSJEmDM1M3iJGk39uPP/iC2Y4gTewBL1s+2xEkzRDPWEuSJEkdsFhLkiRJHbBYS5IkSR2wWEuSJEkd8OJFSZL0W166/KWzHUGa2BkvOGPG5/SMtSRJktQBi7UkSZLUAYu1JEmS1AGLtSRJktQBi7UkSZLUAYu1JEmS1AGLtSRJktQBi7UkSZLUAYu1JEmS1AGLtSRJktQBi7UkSZLUAYu1JEmS1AGLtSRJktQBi7UkSZLUAYu1JEmS1AGLtSRJktQBi7UkSZLUAYu1JEmS1AGLtSRJktQBi7UkSZLUAYu1JEmS1AGLtSRJktSBLSrWSbZPsl3XYSRJkqShalWsk7wnycHN88OBnwDrkhzZZzhJkiRpKNqesX4x8O3m+VuBlwDPAd7RRyhJkiRpaLZt+b4dquoXSXYFHlRV5wEk2ae/aJIkSdJwtC3W30/yYmA/4BKAJLsBv+wrmCRJkjQkbZeC/DnwCuCpwFuasWcC/zppgCR/keQ7Sb6d5ONJ7pNk3yRfTvKDJMu9UFKSJElzXatiXVVfqarHVdWTquqHzdjZVbV0ksmT7AW8GlhSVQ8HtgFeCLwLeG9V7Q/cDhw/yTySJElS31pvt5fk6UlOT/IvzeslSZ7aQYZtge2TbAvsAKxhdGb83Ob4MuC5HcwjSZIk9abtdnuvAj4A/AB4YjP8S+Dtk0xeVTcC7wFuYFSofwpcBayrqrubt60G9ppkHkmSJKlvbc9YvxZ4WlWdAqxvxq4GHjrJ5EnuDxwF7As8ENgReNZG3lrTfP6EJCuSrFi7du0kUSRJkqSJtC3W9wVWNc83lNx7A3dOOP/TgB9V1dqqugs4H3gcsHOzNARgb+CmjX24qk6rqiVVtWTBggUTRpEkSZK2XNtifTlw0pSxVwOfn3D+G4BDkuyQJMChwHeb731e855jgQsmnEeSJEnqVdti/SrgPydZCdw3yTXA0cDrJpm8qr7M6CLFrwLfavKcBpwIvC7JtcCuwOmTzCNJkiT1rdUNYqpqTZKDgIOAfRgtC7myqtZv+pOtvvtk4OQpw9cBB0/63V14yfsunu0IUifOes3hsx1BkqStWqtineRRwG1VdSVwZTO2MMkuVfWNPgNKkiRJQ9B2KchZjC5WHLcdcGa3cSRJkqRhalusF1XVdeMDzR0YF3eeSJIkSRqgtsV6dZLHjA80rze6DZ4kSZI037RaYw28F7ggybuBHwIPBt4A/HVfwSRJkqQhabsryAeTrAOOBxYy2hXk9VV1bp/hJEmSpKFoe8aaqvpn4J97zCJJkiQNVutineQZwKOAncbHq+qtXYeSJEmShqbtPtbvB57P6Fbjvxg7VH2EkiRJkoam7RnrFwGPqqpVfYaRJEmShqrtdnu3Aev6DCJJkiQNWdsz1v8LODvJO4Gbxw9MvXGMJEmSNB+1LdYfaH4eMWW8gG26iyNJkiQNU9t9rNsuGZEkSZLmpd+rMCdZmOSQvsJIkiRJQ9WqWCdZlOQLwNXAvzVjz0vyoT7DSZIkSUPR9oz1PwEXA/cF7mrGLgGe3kcoSZIkaWjaXrx4MHB4Va1PUgBV9dMk/7G/aJIkSdJwtD1jfTOw3/hAkgOBGzpPJEmSJA1Q22L9HuCiJMcB2yZ5EbAceFdvySRJkqQBabvd3hlJfgKcAKwCjgHeUlWf6jOcJEmSNBSbLdZJtgFOBv7aIi1JkiRt3GaXglTVPcAr+M1uIJIkSZKmaLvGehnw8j6DSJIkSUP2+2y396okb2K0xro2HKiqJ/YRTJIkSRqStsX6g81DkiRJ0ka0vXjxwYwuXvx1/5EkSZKk4fHiRUmSJKkDXrwoSZIkdcCLFyVJkqQOePGiJEmS1IG2tzRf1ncQSZIkachaFeskL53uWFWd0V0cSZIkaZjaLgVZOuX1AxhtwfcFwGItSZKkea/tUpCnTB1rzmIfMGmAJDsDHwIezuiiyJcC1wDLgcXASuD5VXX7pHNJkiRJfWm73d7GfAQ4voMM7wM+U1UPAx4JfA84Cbi0qvYHLm1eS5IkSXNWq2Kd5F5THjsBJwDrJpk8yf2AJwKnA1TVnVW1DjiK0d7ZND+fO8k8kiRJUt/arrG+m7G9qxs3MirXk3gQsBb4cJJHAlcBrwH2qKo1AFW1JsnuG/twkhM2ZFi0aNGEUSRJkqQt13YpyL6MSvCGxx5VtaiqPjPh/NsCjwE+UFWPBn7O77Hso6pOq6olVbVkwYIFE0aRJEmStlzbYn038LOqur553Jrk/kkeOOH8q4HVVfXl5vW5jIr2zUn2BGh+3jLhPJIkSVKv2hbrTwF7TxnbG/jkJJNX1Y+BVUke2gwdCnwXuBA4thk7FrhgknkkSZKkvrVdY/3QqvrW+EBVfSvJwzrI8Crg7CTbAdcBxzEq/J9IcjxwA3B0B/NIkiRJvWlbrG9Jsl9VXbthIMl+wG2TBqiqrwNLNnLo0Em/W5IkSZopbZeCnAGcl+SIJAcmOZLReugP9RdNkiRJGo62Z6xPAe4C3gMsZLQ843Tg1J5ySZIkSYPS9pbm64G/aR6SJEmSpmh758WTkhw0ZezgJG/qJ5YkSZI0LG3XWL+G0TZ4474LvLbbOJIkSdIwtS3W2zFaYz3uTuA+3caRJEmShqltsb4K+PMpYy8HvtptHEmSJGmY2u4K8hfAJUmWAj8E9gP2AJ7eVzBJkiRpSNruCvKdJA8BjmC03d75wEVVdUef4SRJkqShaHvGGmBP4Hrgqqr6QU95JEmSpEHa7BrrJP8lyUrgGuALwNVJViZ5Xt/hJEmSpKHYZLFOcjjwYeAfgQcB2wMPBj4AfCjJEb0nlCRJkgZgc0tB3gL8aVWdMza2EnhXkhua4xf1lE2SJEkajM0tBfkD4JPTHDsfOLDbOJIkSdIwba5Y/xq43zTHdmZ0kxhJkiRp3ttcsf4M8M5pjr0D+Gy3cSRJkqRh2twa6xOBK5J8EzgPWMNo270/ZnQm+wn9xpMkSZKGYZPFuqpuTPIY4HXAYcBuwK3ABcB7q+on/UeUJEmS5r7N3iCmqm5ntPvHW/qPI0mSJA3TZm8QI0mSJGnzLNaSJElSByzWkiRJUgemLdZJvjT2/OSZiSNJkiQN06bOWD8kyX2a56+fiTCSJEnSUG1qV5ALgO8nWQlsn+Tyjb2pqp7YRzBJkiRpSKYt1lV1XJInAIuBg4DTZyqUJEmSNDSbu0HMFYzuvLhdVS2boUySJEnS4Gz2BjEAVXVGkqcAS4G9gBuBs6rqc32GkyRJkoai1XZ7Sf4EWA78GDgfWAN8LMnLeswmSZIkDUarM9bAm4CnV9U3NgwkWQ6cB3ywj2CSJEnSkLS9QcyuwHenjF0D7NJtHEmSJGmY2hbrK4BTk+wAkGRH4G+AL/YVTJIkSRqStsX65cAjgJ8muRlYBzwS+NO+gkmSJElD0nZXkDXAk5LsDTwQuKmqVncVIsk2wArgxqo6Ism+wDmMlpp8FVhaVXd2NZ8kSZLUtbZnrAGoqtVVdWWXpbrxGuB7Y6/fBby3qvYHbgeO73g+SZIkqVO/V7HuQ3MW/HDgQ83rAE8Fzm3esgx47uykkyRJktqZ9WIN/C2j7fzWN693BdZV1d3N69WMbkojSZIkzVmbLdZJ7pXkqUm263ryJEcAt1TVVePDG3lrTfP5E5KsSLJi7dq1XceTJEmSWttssa6q9cAFPV08+HjgOUlWMrpY8amMzmDvnGTDhZV7AzdNk+20qlpSVUsWLFjQQzxJkiSpnbZLQS5PckjXk1fVm6tq76paDLwQ+FxVvRj4PPC85m3HAhd0PbckSZLUpba3NL8e+N9JLgBWMbY0o6re2kOuE4Fzkrwd+Bpweg9zSJIkSZ1pW6y3Bz7VPN+7jyBVdRlwWfP8OuDgPuaRJEmS+tD2BjHH9R1EkiRJGrK2Z6xJcgCjdc97VNUrkzwU+A9V9c3e0kmSJEkD0erixSRHA5cz2k/6mGb4vsCpPeWSJEmSBqXtriBvA55eVS8H7mnGvgE8spdUkiRJ0sC0Lda7MyrS8JsdQYppbtwiSZIkzTdti/VVwNIpYy8Eruw2jiRJkjRMbS9efDXwr0mOB3ZM8lngIcAzeksmSZIkDUjb7fauTvIw4AjgIkY3ibmoqu7oM5wkSZI0FK2326uqXyT5AvAj4CZLtSRJkvQbbbfbW5Tk/wArgYuBlUmuSLJPn+EkSZKkoWh78eIyRhcw7lxVuwP3B77SjEuSJEnzXtulIP8JeEZV3QVQVXckORG4rbdkkiRJ0oC0PWP9JeDgKWNLgP/bbRxJkiRpmKY9Y53kbWMvfwh8OsnFjHYEWQg8G/hYv/EkSZKkYdjUUpCFU16f3/zcHfg18EngPn2EkiRJkoZm2mJdVcfNZBBJkiRpyFrvY51kB2A/YKfx8ar6YtehJEmSpKFpVayTHAO8H7gT+OXYoQIW9ZBLkiRJGpS2Z6zfDfxxVV3SZxhJkiRpqNput3cncFmPOSRJkqRBa1us3wKcmmS3PsNIkiRJQ9W2WH8feA5wc5J7msf6JPf0mE2SJEkajLZrrM8EPgos57cvXpQkSZJE+2K9K/DWqqo+w0iSJElD1XYpyIeBpX0GkSRJkoas7Rnrg4FXJvlvwM3jB6rqiZ2nkiRJkgambbH+YPOQJEmStBGtinVVLes7iCRJkjRkbW9p/tLpjlXVGd3FkSRJkoap7VKQqRcuPgB4MPAFwGItSZKkea/tUpCnTB1rzmIf0HkiSZIkaYDabre3MR8Bju8ohyRJkjRobddYTy3gOwAvAdZ1nkiSJEkaoLZrrO8Gpt518UbgZd3GkSRJkoapbbHed8rrn1fVrZNOnmQh8FFGF0OuB06rqvcl2QVYDiwGVgLPr6rbJ51PkiRJ6kurNdZVdf2Ux8SlunE38PqqOgA4BHhFkgOBk4BLq2p/4NLmtSRJkjRnbfKMdZLP87tLQMZVVR26pZNX1RpgTfP835N8D9gLOAp4cvO2ZcBlwIlbOo8kSZLUt80tBTlrmvG9gFczuoixE0kWA48Gvgzs0ZRuqmpNkt27mkeSJEnqwyaLdVWdPv46ya7AmxldtLgceFsXIZLsBJwHvLaqfpak7edOAE4AWLRoURdRJEmSpC3Sao11kvsl+Z/AtcAewGOq6oSqWj1pgCT3ZlSqz66q85vhm5Ps2RzfE7hlY5+tqtOqaklVLVmwYMGkUSRJkqQttslinWT7JG8GrmN0l8UnVNXSqvphF5NndGr6dOB7VXXq2KELgWOb58cCF3QxnyRJktSXza2x/hGwDfBuYAWwR5I9xt9QVZ+bYP7HA0uBbyX5ejP2l8ApwCeSHA/cABw9wRySJElS7zZXrH/FaFeQP5vmeAEP2tLJq+oKYLoF1Vu824gkSZI00zZ38eLiGcohSZIkDVqrixclSZIkbZrFWpIkSeqAxVqSJEnqgMVakiRJ6oDFWpIkSeqAxVqSJEnqgMVakiRJ6oDFWpIkSeqAxVqSJEnqgMVakiRJ6oDFWpIkSeqAxVqSJEnqgMVakiRJ6oDFWpIkSeqAxVqSJEnqgMVakiRJ6oDFWpIkSeqAxVqSJEnqgMVakiRJ6oDFWpIkSeqAxVqSJEnqgMVakiRJ6oDFWpIkSeqAxVqSJEnqgMVakiRJ6oDFWpIkSeqAxVqSJEnqgMVakiRJ6oDFWpIkSeqAxVqSJEnqgMVakiRJ6oDFWpIkSeqAxVqSJEnqwJwt1kkOS3JNkmuTnDTbeSRJkqRNmZPFOsk2wD8AzwIOBF6U5MDZTSVJkiRNb04Wa+Bg4Nqquq6q7gTOAY6a5UySJEnStFJVs53hdyR5HnBYVf1J83op8EdV9cop7zsBOKF5+VDgmhkNqi7tBtw62yGkecjfPWl2+Ls3bPtU1YKpg9vORpIWspGx3/kvgKo6DTit/zjqW5IVVbVktnNI842/e9Ls8Hdv6zRXl4KsBhaOvd4buGmWskiSJEmbNVeL9VeA/ZPsm2Q74IXAhbOcSZIkSZrWnFwKUlV3J3kl8FlgG+CMqvrOLMdSv1zSI80Of/ek2eHv3lZoTl68KEmSJA3NXF0KIkmSJA2KxVqSJEnqgMVakiRJ6sCcvHhRW7ckD2N0J829GO1PfhNwYVV9b1aDSZLUk+bvvr2AL1fVHWPjh1XVZ2YvmbrkGWvNqCQnMrpFfYArGW2tGODjSU6azWzSfJbkuNnOIG2tkrwauAB4FfDtJEeNHX7H7KRSH9wVRDMqyfeBP6iqu6aMbwd8p6r2n51k0vyW5IaqWjTbOaStUZJvAY+tqjuSLAbOBc6sqvcl+VpVPXpWA6ozLgXRTFsPPBC4fsr4ns0xST1J8s3pDgF7zGQWaZ7ZZsPyj6pameTJwLlJ9mH0+6ethMVaM+21wKVJfgCsasYWAfsBr5y1VNL8sAfwTOD2KeMBvjjzcaR548dJHlVVXwdozlwfAZwB/OHsRlOXLNaaUVX1mSQPAQ5mdBFHgNXAV6rqnlkNJ239LgJ22vCX+7gkl818HGneOAa4e3ygqu4GjknyT7MTSX1wjbUkSZLUAXcFkSRJkjpgsZYkSZI6YLGWJEmSOmCxlqStSJKVSZ422zkkaT6yWEuSJpbEXaYkzXsWa0mao5IsTHJ+krVJbkvy/iQPTvK55vWtSc5OsnPz/jMZ7Qv/L0nuSPKmZvyQJF9Msi7JN5qbU2yYY98klyf59yT/luQfkpw1dvw5Sb7TfPayJAeMHVuZ5MTmxjM/T/LGJOdN+Wf4+yR/2++flCTNDRZrSZqDkmzDaN/p64HFjPZ9P4fR3u/vZHQH0wOAhcBfAVTVUuAG4Miq2qmq3p1kL+Bi4O3ALsAbgPOSLGim+hhwJbBr8z1LxzI8BPg4oxs7LQA+zai0bzcW9UXA4cDOwFnAYWNFf1vgBcCZ3fypSNLcZrGWpLnpYEbl+Y1V9fOq+lVVXVFV11bVJVX166paC5wKPGkT3/MS4NNV9emqWl9VlwArgGcnWQQcBLy1qu6sqiuAC8c++wLg4ma+u4D3ANsDjxt7z99V1aqq+mVVrQEuB45ujh0G3FpVV038pyFJA2CxlqS5aSFwfXN3tv8vye5JzklyY5KfMTpLvNsmvmcf4OhmKce6JOuAJwB7MiruP6mqX4y9f9XY8wcyOmMOQFWtb47vNc37AZYxKvM0Pz1bLWnesFhL0ty0Cli0kYsC3wkU8Iiquh+j8pqx41Nvp7sKOLOqdh577FhVpwBrgF2S7DD2/oVjz29iVMwBSJLm+I2bmO9TwCOSPBw4Aji7xT+rJG0VLNaSNDddyaj4npJkxyT3SfJ44L7AHcC6Zv30G6d87mbgQWOvzwKOTPLMJNs03/PkJHtX1fWMloX8VZLtkjwWOHLss58ADk9yaJJ7A68Hfg18cbrQVfUr4FyatdtVdcMEfwaSNCgWa0mag6rqHkYldz9GFySuZrTm+X8AjwF+yuiixPOnfPSdwH9vln28oapWAUcBfwmsZXQG+4385t//LwYeC9zG6ALH5YzKM1V1DaMz4n8P3NrkObKq7txM/GXAH+IyEEnzTKqm/l88SdJ8lWQ5cHVVnTzBdywCrgYeUFU/6yycJM1xnrGWpHksyUHN3tj3SnIYo7Pbn5rg++4FvA44x1Itab7xTlmSNL89gNFykl0ZLTf5s6r62pZ8UZIdGa3xvp7RVnuSNK+4FESSJEnqgEtBJEmSpA5YrCVJkqQOWKwlSZKkDlisJUmSpA5YrCVJkqQOWKwlSZKkDvw/shEKNyjN2C0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnt_pro = df['category'].value_counts()\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('category', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Work Stress Total Data Count:  78\n",
      "Bullying Total Data Count:  97\n",
      "Sexual Harassment Total Data Count:  95\n"
     ]
    }
   ],
   "source": [
    "work_stress_counter, bullying_counter, sexual_harassment_counter = 0, 0, 0\n",
    "for c in df['category']:\n",
    "    if c==0:\n",
    "        work_stress_counter+=1\n",
    "    elif c == 1:\n",
    "        bullying_counter+=1\n",
    "    else:\n",
    "        sexual_harassment_counter+=1\n",
    "print(\"Work Stress Total Data Count: \", work_stress_counter)\n",
    "print(\"Bullying Total Data Count: \", bullying_counter)\n",
    "print(\"Sexual Harassment Total Data Count: \", sexual_harassment_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have been called hurtful names and i have been called black bitch and people are making fun of me for being black by my suppost to be friend t and she has posted rumors about me and i dont know what to do.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(df.story[2])\n",
    "print(df.category[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just like any other day, employees arrived in the workplace sparingly, filling the cubicles and getting their coffees ready. Once more, the manager was already sitting at his desk, grumbling and shouting - You’re way too slow, again! How am I supposed to get my work done with you slowing me down every day? The other employees were staring at each other. They were embarrassed by his outburst but deep inside they knew he was right. Satisfied by the nods in the assembly, the manager calmed down. As usual, he quickly got absorbed by his screen and numerous emails. He was not the moody type, just a normal guy. His team liked him very much and his performance record was exemplary. He was often described as someone caring and trustworthy with a genuine interest in people. Yet once again, his fist hit the desk loudly as he started screaming. That’s it, I’m done! Slow and unreliable old crap! You made me lose five hours of my life, again. World will be better off without you and I’m going to let the board know about that. He stood up suddenly and clenched his fists in anger. He then walked rapidly to the stairs, climbed and disappeared. Nobody saw him again for the day, but they all knew he was going for a confrontation, if not a fight. It was going to get really ugly up there.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df.story[0])\n",
    "print(df.category[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I worked as an office manager, and the only woman, for an industrial insulation company. I had just come back from maternity leave and I was worried about my milk supply. I went into the bathroom to pump for about 15 minutes every two hours, and all of the men in the office would stand in the break area (right in front of the bathroom door) and make baby crying noises to make fun of me. Eventually it progressed to the point that they would make crying noises every time they passed my desk in hopes that I would leak through my shirt. They would also make comments about how much larger my breasts were since having a baby. I felt so harassed and unsafe that I would dread going to work every day, and I even had more than a few nervous breakdowns. My husband was furious and I had to convince him not to take any drastic action so that I could be sure to have a good reference if I needed to find another job. We had a long conversation and looked at our finances and decided the extra money wasn't worth the emotional distress. I ended up quitting my job and staying home with our kids.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(df.story[10])\n",
    "print(df.category[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('x', '')\n",
    "    return text\n",
    "df['story'] = df['story'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH TRAIN_TEST_SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size = .20, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged = train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "test_tagged = test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>i'm 16 and a boy. i really don't know how to d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>hello! first of all, i'm sorry if there are a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>eleven years ago i was seually harassed by a t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>i had been seeing this guy for just over three...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>high school and junior high were hell for me. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>i’m in the 8th grade. ever since i started mid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hi, i'm rahul. i was bullied all through schoo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>when i was in 7th grade, i had a semester of m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i got made fun of for being fat in elementary,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>let me tell you about a few of my eperiences. ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 story  category\n",
       "134  i'm 16 and a boy. i really don't know how to d...         2\n",
       "101  hello! first of all, i'm sorry if there are a ...         1\n",
       "127  eleven years ago i was seually harassed by a t...         2\n",
       "197  i had been seeing this guy for just over three...         2\n",
       "24   high school and junior high were hell for me. ...         1\n",
       "..                                                 ...       ...\n",
       "75   i’m in the 8th grade. ever since i started mid...         1\n",
       "22   hi, i'm rahul. i was bullied all through schoo...         1\n",
       "72   when i was in 7th grade, i had a semester of m...         1\n",
       "15   i got made fun of for being fat in elementary,...         1\n",
       "168  let me tell you about a few of my eperiences. ...         2\n",
       "\n",
       "[216 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>my boyfriend and i were watching a movie in hi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>a male customer stalked me for several weeks l...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>i was very young and i don’t remember much. he...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>i was most definitely bullied in my high schoo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>i am 14 years old and in the 8th grade. i have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>and im 14 years old. all my life, i been bulli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>i have a weird name. i don't even know why i h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>being a single mother is one of the most stres...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>i was in class 11. i was coming back from my t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at my old school kids would hit me and call me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>when i reported my depression to my employer, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>you would not believe the number of times i wa...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>when i got admitted in a well reputed college,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>pretty minor, but funny. i was transferred int...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>there was this one time i was taking the bus w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>right now  i am in work place  it's real good ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>i’ve had to deal with male customers who stop ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>all of my work eperience has been in the los a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i used to work for a call center and the men t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>i am currently 17 years old and a junior in hi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>i’m currently working at starbucks, and as a 2...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>senior year at muhlenberg, my roommate and i w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>actually it is okay to work more when needed b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>it started in year 7, which i think is like 6t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>when i was 77, i was at a brunch and man came ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>the stress got me. i completely mentally check...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>hi, my name is hope and like many people today...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>this happened when i was about 23 and working ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>my story might be short because i only remembe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>my first job is the lehigh valley (mid-’70s) w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i work in law enforcement. i started out as a ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>a girl jumped on me on my friend's house! she ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>i am 15 now and i have been bullied since grad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i worked as an office manager, and the only wo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>i’ve been seually harassed in the past. three ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>ever since first grade, my three best friends ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>i noticed movement out of the corner of my eye...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>i had everything going for me; being popular, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>well it all started in year 7, i told one of m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>i was born with a craniofacial disease called ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>i got beaten up and verbally abused from 1st g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>the first time was the day after my 20th birth...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>i’ve always been super flat-chested and i’m st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>i remember in the fifth grade i was in gym cla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>i was working at petsmart a few years back. a ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>i worked at a well-known hardware store and a ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>well ever since i've been in second grade i wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>i was doing what i thought was my dream job. i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>i'm 13 years old and live in nsw, australia wi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>i'm a bus driver and can confirm. some people ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>i thought i knew my master's thesis adviser we...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>i was around 12-13 years old and at this “meet...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>i was 14 years old at that time and went to vi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>out of high school, i landed a job at a car re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 story  category\n",
       "131  my boyfriend and i were watching a movie in hi...         2\n",
       "183  a male customer stalked me for several weeks l...         2\n",
       "200  i was very young and i don’t remember much. he...         2\n",
       "65   i was most definitely bullied in my high schoo...         1\n",
       "91   i am 14 years old and in the 8th grade. i have...         1\n",
       "115  and im 14 years old. all my life, i been bulli...         1\n",
       "93   i have a weird name. i don't even know why i h...         1\n",
       "242  being a single mother is one of the most stres...         0\n",
       "29   i was in class 11. i was coming back from my t...         1\n",
       "3    at my old school kids would hit me and call me...         1\n",
       "247  when i reported my depression to my employer, ...         0\n",
       "177  you would not believe the number of times i wa...         2\n",
       "35   when i got admitted in a well reputed college,...         1\n",
       "141  pretty minor, but funny. i was transferred int...         0\n",
       "208  there was this one time i was taking the bus w...         2\n",
       "259  right now  i am in work place  it's real good ...         0\n",
       "187  i’ve had to deal with male customers who stop ...         2\n",
       "252  all of my work eperience has been in the los a...         0\n",
       "11   i used to work for a call center and the men t...         2\n",
       "120  i am currently 17 years old and a junior in hi...         1\n",
       "186  i’m currently working at starbucks, and as a 2...         2\n",
       "163  senior year at muhlenberg, my roommate and i w...         2\n",
       "233  actually it is okay to work more when needed b...         0\n",
       "112  it started in year 7, which i think is like 6t...         1\n",
       "160  when i was 77, i was at a brunch and man came ...         2\n",
       "256  the stress got me. i completely mentally check...         0\n",
       "104  hi, my name is hope and like many people today...         1\n",
       "184  this happened when i was about 23 and working ...         2\n",
       "85   my story might be short because i only remembe...         1\n",
       "153  my first job is the lehigh valley (mid-’70s) w...         2\n",
       "13   i work in law enforcement. i started out as a ...         2\n",
       "30   a girl jumped on me on my friend's house! she ...         2\n",
       "74   i am 15 now and i have been bullied since grad...         1\n",
       "10   i worked as an office manager, and the only wo...         2\n",
       "130  i’ve been seually harassed in the past. three ...         2\n",
       "54   ever since first grade, my three best friends ...         1\n",
       "201  i noticed movement out of the corner of my eye...         2\n",
       "106  i had everything going for me; being popular, ...         1\n",
       "89   well it all started in year 7, i told one of m...         1\n",
       "23   i was born with a craniofacial disease called ...         1\n",
       "66   i got beaten up and verbally abused from 1st g...         1\n",
       "128  the first time was the day after my 20th birth...         2\n",
       "53   i’ve always been super flat-chested and i’m st...         1\n",
       "99   i remember in the fifth grade i was in gym cla...         1\n",
       "173  i was working at petsmart a few years back. a ...         2\n",
       "188  i worked at a well-known hardware store and a ...         2\n",
       "114  well ever since i've been in second grade i wa...         1\n",
       "251  i was doing what i thought was my dream job. i...         0\n",
       "94   i'm 13 years old and live in nsw, australia wi...         1\n",
       "226  i'm a bus driver and can confirm. some people ...         0\n",
       "155  i thought i knew my master's thesis adviser we...         2\n",
       "71   i was around 12-13 years old and at this “meet...         1\n",
       "41   i was 14 years old at that time and went to vi...         2\n",
       "225  out of high school, i landed a job at a car re...         0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134    (['m, 16, and, boy, really, do, n't, know, how...\n",
       "101    ([hello, first, of, all, 'm, sorry, if, there,...\n",
       "127    ([eleven, years, ago, was, seually, harassed, ...\n",
       "197    ([had, been, seeing, this, guy, for, just, ove...\n",
       "24     ([high, school, and, junior, high, were, hell,...\n",
       "                             ...                        \n",
       "75     ([in, the, 8th, grade, ever, since, started, m...\n",
       "22     ([hi, 'm, rahul, was, bullied, all, through, s...\n",
       "72     ([when, was, in, 7th, grade, had, semester, of...\n",
       "15     ([got, made, fun, of, for, being, fat, in, ele...\n",
       "168    ([let, me, tell, you, about, few, of, my, eper...\n",
       "Length: 216, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['eleven', 'years', 'ago', 'was', 'seually', 'harassed', 'by', 'teenage', 'boy', 'whom', 'trusted', 'and', 'was', 'very', 'close', 'to', 'we', 'were', 'in', 'his', 'basement', 'playing', 'ping', 'pong', 'and', 'some', 'other', 'boys', 'were', 'there', 'too', 'once', 'they', 'left', 'to', 'go', 'upstairs', 'he', 'started', 'asking', 'me', 'if', 'could', 'unzip', 'my', 'pants', 'was', 'seven', 'years', 'old', 'at', 'the', 'time', 'so', 'only', 'remember', 'chunks', 'of', 'what', 'happened', 'but', 'the', 'one', 'thing', 'remember', 'very', 'clearly', 'was', 'when', 'he', 'rubbed', 'his', 'finger', 'very', 'gently', 'over', 'the', 'skin', 'that', 'was', 'beneath', 'my', 'underpants', 'remember', 'just', 'standing', 'there', 'frozen', 'and', 'feeling', 'confused', 'that', 'night', 'remember', 'laying', 'in', 'bed', 'net', 'to', 'my', 'mom', 'and', 'thought', 'to', 'myself', 'do', 'tell', 'her', 'what', 'do', 'say', 'later', 'that', 'night', 'told', 'my', 'parents', 'what', 'had', 'happened', 'and', 'my', 'father', 'said', 'he', 'would', 'discuss', 'it', 'with', 'the', 'boy', 'mother', 'they', 'allowed', 'me', 'to', 'talk', 'about', 'it', 'with', 'them', 'but', 'also', 'encouraged', 'me', 'to', 'move', 'on', 'reminding', 'me', 'that', 'no', 'one', 'is', 'allowed', 'to', 'touch', 'me', 'like', 'that'], tags=[2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged.values[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['male', 'customer', 'stalked', 'me', 'for', 'several', 'weeks', 'late', 'last', 'year', 'culminating', 'in', 'an', 'incident', 'where', 'he', 'followed', 'me', 'around', 'the', 'shop', 'shortly', 'before', 'close', 'casually', 'showing', 'me', 'what', 'appeared', 'to', 'be', 'gun', 'tucked', 'into', 'his', 'waistband', 'some', 'of', 'my', 'male', 'coworkers', 'told', 'me', 'was', 'eaggerating', 'or', 'that', 'the', 'guy', 'wasn', 'so', 'bad', 'another', 'my', 'manager', 'at', 'the', 'time', 'and', 'one', 'of', 'the', 'best', 'men', 've', 'ever', 'known', 'bought', 'me', 'knife', 'and', 'taught', 'me', 'how', 'to', 'use', 'it', 'when', 'told', 'him', 'what', 'had', 'happened'], tags=[2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tagged.values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PV-DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 643901.68it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 442800.42it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1517537.13it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 905064.60it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 184628.01it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1225940.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 389 ms, sys: 17.4 ms, total: 406 ms\n",
      "Wall time: 183 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(5):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.8518518518518519\n",
      "Testing F1 score: 0.8513733371614509\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  1  1]\n",
      " [ 0 20  2]\n",
      " [ 2  2 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80        10\n",
      "           1       0.87      0.91      0.89        22\n",
      "           2       0.86      0.82      0.84        22\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.84      0.84      0.84        54\n",
      "weighted avg       0.85      0.85      0.85        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "print('\\nTesting Confusion Matrix: ')\n",
    "print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "print('Testing Classification Report: ')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PV-DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 1532943.59it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 906876.54it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1016800.97it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1074697.11it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1326456.32it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 163208.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 523 ms, sys: 17.9 ms, total: 541 ms\n",
      "Wall time: 301 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(5):\n",
    "    model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dmm.alpha -= 0.002\n",
    "    model_dmm.min_alpha = model_dmm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dmm, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dmm, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.8518518518518519\n",
      "Testing F1 score: 0.8507970833002446\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 7  3  0]\n",
      " [ 0 20  2]\n",
      " [ 1  2 19]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.70      0.78        10\n",
      "           1       0.80      0.91      0.85        22\n",
      "           2       0.90      0.86      0.88        22\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.86      0.82      0.84        54\n",
      "weighted avg       0.86      0.85      0.85        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "print('\\nTesting Confusion Matrix: ')\n",
    "print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "print('Testing Classification Report: ')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = get_vectors(new_model, train_tagged)\n",
    "y_test, X_test = get_vectors(new_model, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.8148148148148148\n",
      "Testing F1 score: 0.806561996779388\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 5  2  3]\n",
      " [ 0 20  2]\n",
      " [ 1  2 19]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.50      0.62        10\n",
      "           1       0.83      0.91      0.87        22\n",
      "           2       0.79      0.86      0.83        22\n",
      "\n",
      "    accuracy                           0.81        54\n",
      "   macro avg       0.82      0.76      0.77        54\n",
      "weighted avg       0.82      0.81      0.81        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "print('\\nTesting Confusion Matrix: ')\n",
    "print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "print('Testing Classification Report: ')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH K-FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  270\n",
      "Type:  <class 'pandas.core.series.Series'>\n",
      "First Ten Values:\n",
      " 0    just like any other day, employees arrived in ...\n",
      "1    my so-called ‘friends’ in middle school used t...\n",
      "2    i have been called hurtful names and i have be...\n",
      "3    at my old school kids would hit me and call me...\n",
      "4    i had debilitating migraines for three years b...\n",
      "5    i love my work, but hate going each day becaus...\n",
      "6    i have a chronic illness which was doing well ...\n",
      "7    the other part is that sense of worthlessness....\n",
      "8    i feel my whole body hurting. my mental health...\n",
      "9    as a librarian, i've been threatened with stal...\n",
      "Name: story, dtype: object\n",
      "just like any other day, employees arrived in the workplace sparingly, filling the cubicles and getting their coffees ready. once more, the manager was already sitting at his desk, grumbling and shouting - you’re way too slow, again! how am i supposed to get my work done with you slowing me down every day? the other employees were staring at each other. they were embarrassed by his outburst but deep inside they knew he was right. satisfied by the nods in the assembly, the manager calmed down. as usual, he quickly got absorbed by his screen and numerous emails. he was not the moody type, just a normal guy. his team liked him very much and his performance record was eemplary. he was often described as someone caring and trustworthy with a genuine interest in people. yet once again, his fist hit the desk loudly as he started screaming. that’s it, i’m done! slow and unreliable old crap! you made me lose five hours of my life, again. world will be better off without you and i’m going to let the board know about that. he stood up suddenly and clenched his fists in anger. he then walked rapidly to the stairs, climbed and disappeared. nobody saw him again for the day, but they all knew he was going for a confrontation, if not a fight. it was going to get really ugly up there.\n"
     ]
    }
   ],
   "source": [
    "X = df['story']\n",
    "print(\"Length: \", len(X))\n",
    "print(\"Type: \", type(X))\n",
    "print(\"First Ten Values:\\n\", X[:10])\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  270\n",
      "Type:  <class 'pandas.core.series.Series'>\n",
      "First Ten Values:  0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "8    0\n",
      "9    2\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y = df['category']\n",
    "print(\"Length: \", len(y))\n",
    "print(\"Type: \", type(y))\n",
    "print(\"First Ten Values: \", y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=5, random_state=42, shuffle=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten_fold = KFold(n_splits=5, shuffle = True, random_state=42)\n",
    "ten_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold:  5\n",
      "X_train:  (216,) X_test:  (54,)\n",
      "X_train:  (216,) X_test:  (54,)\n",
      "X_train:  (216,) X_test:  (54,)\n",
      "X_train:  (216,) X_test:  (54,)\n",
      "X_train:  (216,) X_test:  (54,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Fold: \", ten_fold.get_n_splits(X))\n",
    "fold_no = 1\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"X_train: \", X_train.shape, \"X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "total_fold = ten_fold.get_n_splits(X)\n",
    "print(total_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-FOLD PV-DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold:  5\n",
      "Train Fold No.:  [  0   1   2   3   4   5   7   8  10  11  12  13  14  16  17  18  20  21\n",
      "  23  26  27  28  29  31  32  33  34  35  36  37  38  39  40  41  43  44\n",
      "  47  48  49  50  51  52  53  54  55  56  57  58  59  61  62  63  64  65\n",
      "  66  69  70  71  72  73  74  75  76  78  80  81  83  84  85  86  87  88\n",
      "  89  91  92  94  95  96  97  98  99 100 101 102 103 105 106 107 108 110\n",
      " 111 112 114 115 117 121 122 123 124 126 128 129 130 131 132 133 134 135\n",
      " 136 138 139 140 141 142 143 145 147 148 149 150 151 153 154 155 156 157\n",
      " 159 160 161 162 163 164 166 167 168 169 170 171 172 173 174 175 176 177\n",
      " 178 179 181 182 184 186 187 188 189 190 191 192 193 195 197 198 200 201\n",
      " 202 203 204 205 206 207 210 211 212 213 214 215 217 218 219 220 222 223\n",
      " 225 228 229 230 231 233 234 235 236 237 238 239 240 241 242 243 245 246\n",
      " 248 249 251 253 254 255 256 257 258 259 260 261 262 263 264 265 267 269]  Test Fold No.:  [  6   9  15  19  22  24  25  30  42  45  46  60  67  68  77  79  82  90\n",
      "  93 104 109 113 116 118 119 120 125 127 137 144 146 152 158 165 180 183\n",
      " 185 194 196 199 208 209 216 221 224 226 227 232 244 247 250 252 266 268]\n",
      "No. of Training Dataset in Fold:  1  216\n",
      "No. of Testing Dataset in Fold:  1  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 1168993.11it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 984749.63it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1096815.57it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 910522.28it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 792624.38it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1374764.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  1\n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.870320589406611\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  2  2]\n",
      " [ 0 20  1]\n",
      " [ 0  2 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.78      0.88        18\n",
      "           1       0.83      0.95      0.89        21\n",
      "           2       0.81      0.87      0.84        15\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.88      0.87      0.87        54\n",
      "weighted avg       0.88      0.87      0.87        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Fold No.:  [  0   1   3   4   5   6   7   8   9  11  12  13  14  15  17  19  20  21\n",
      "  22  23  24  25  26  27  28  29  30  31  32  34  35  36  39  40  41  42\n",
      "  43  44  45  46  47  48  49  50  51  52  53  54  56  57  58  59  60  61\n",
      "  62  63  64  65  67  68  70  71  72  77  79  80  81  82  83  85  87  88\n",
      "  89  90  91  93  94  95  98  99 100 102 103 104 105 106 107 109 110 113\n",
      " 114 116 117 118 119 120 121 122 123 125 127 128 129 130 131 132 133 134\n",
      " 135 136 137 138 141 144 145 146 149 150 151 152 153 156 157 158 159 160\n",
      " 161 162 163 164 165 166 167 168 169 171 172 174 175 176 177 178 179 180\n",
      " 183 185 186 187 188 189 190 191 192 194 196 198 199 202 203 204 205 206\n",
      " 207 208 209 211 212 213 214 215 216 217 218 219 220 221 222 223 224 226\n",
      " 227 228 230 231 232 234 235 236 237 238 239 240 241 242 243 244 245 246\n",
      " 247 248 249 250 252 254 255 256 258 259 261 263 264 265 266 267 268 269]  Test Fold No.:  [  2  10  16  18  33  37  38  55  66  69  73  74  75  76  78  84  86  92\n",
      "  96  97 101 108 111 112 115 124 126 139 140 142 143 147 148 154 155 170\n",
      " 173 181 182 184 193 195 197 200 201 210 225 229 233 251 253 257 260 262]\n",
      "No. of Training Dataset in Fold:  2  216\n",
      "No. of Testing Dataset in Fold:  2  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 981548.93it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 976260.41it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1492536.51it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1222631.13it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 573035.84it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1263556.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  2\n",
      "Testing accuracy 0.9074074074074074\n",
      "Testing F1 score: 0.9085317907495054\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[12  2  0]\n",
      " [ 1 21  0]\n",
      " [ 2  0 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83        14\n",
      "           1       0.91      0.95      0.93        22\n",
      "           2       1.00      0.89      0.94        18\n",
      "\n",
      "    accuracy                           0.91        54\n",
      "   macro avg       0.90      0.90      0.90        54\n",
      "weighted avg       0.91      0.91      0.91        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Fold No.:  [  1   2   3   6   7   8   9  10  13  14  15  16  17  18  19  20  21  22\n",
      "  23  24  25  30  33  34  37  38  39  40  42  43  44  45  46  47  48  49\n",
      "  50  52  53  54  55  57  58  59  60  62  63  64  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  86  87  88  89  90\n",
      "  91  92  93  94  96  97  99 101 102 103 104 105 106 107 108 109 110 111\n",
      " 112 113 115 116 118 119 120 121 123 124 125 126 127 128 129 130 131 133\n",
      " 134 135 137 138 139 140 142 143 144 145 146 147 148 149 151 152 153 154\n",
      " 155 156 158 160 161 162 163 165 166 168 169 170 171 173 174 180 181 182\n",
      " 183 184 185 186 187 188 189 190 191 193 194 195 196 197 198 199 200 201\n",
      " 203 204 205 206 207 208 209 210 211 213 214 216 217 218 219 220 221 222\n",
      " 223 224 225 226 227 228 229 232 233 235 236 237 238 239 240 242 243 244\n",
      " 247 248 249 250 251 252 253 254 255 257 258 260 261 262 265 266 268 269]  Test Fold No.:  [  0   4   5  11  12  26  27  28  29  31  32  35  36  41  51  56  61  65\n",
      "  85  95  98 100 114 117 122 132 136 141 150 157 159 164 167 172 175 176\n",
      " 177 178 179 192 202 212 215 230 231 234 241 245 246 256 259 263 264 267]\n",
      "No. of Training Dataset in Fold:  3  216\n",
      "No. of Testing Dataset in Fold:  3  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 300886.64it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 633102.49it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 767120.80it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1020236.11it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 935919.07it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 195716.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  3\n",
      "Testing accuracy 0.8518518518518519\n",
      "Testing F1 score: 0.8515432098765432\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[12  0  3]\n",
      " [ 2 13  2]\n",
      " [ 1  0 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80        15\n",
      "           1       1.00      0.76      0.87        17\n",
      "           2       0.81      0.95      0.88        22\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.87      0.84      0.85        54\n",
      "weighted avg       0.87      0.85      0.85        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Fold No.:  [  0   2   4   5   6   8   9  10  11  12  13  14  15  16  17  18  19  20\n",
      "  21  22  24  25  26  27  28  29  30  31  32  33  35  36  37  38  41  42\n",
      "  45  46  48  50  51  52  54  55  56  57  58  59  60  61  63  65  66  67\n",
      "  68  69  71  72  73  74  75  76  77  78  79  82  84  85  86  87  88  89\n",
      "  90  92  93  95  96  97  98  99 100 101 102 104 106 107 108 109 111 112\n",
      " 113 114 115 116 117 118 119 120 121 122 124 125 126 127 129 130 131 132\n",
      " 134 136 137 139 140 141 142 143 144 146 147 148 149 150 151 152 154 155\n",
      " 157 158 159 160 164 165 166 167 169 170 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 187 188 189 191 192 193 194 195 196 197 199 200\n",
      " 201 202 203 207 208 209 210 212 214 215 216 218 219 221 224 225 226 227\n",
      " 228 229 230 231 232 233 234 235 238 239 241 244 245 246 247 248 249 250\n",
      " 251 252 253 254 255 256 257 258 259 260 262 263 264 265 266 267 268 269]  Test Fold No.:  [  1   3   7  23  34  39  40  43  44  47  49  53  62  64  70  80  81  83\n",
      "  91  94 103 105 110 123 128 133 135 138 145 153 156 161 162 163 168 171\n",
      " 186 190 198 204 205 206 211 213 217 220 222 223 236 237 240 242 243 261]\n",
      "No. of Training Dataset in Fold:  4  216\n",
      "No. of Testing Dataset in Fold:  4  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 1157049.38it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 900566.27it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1358275.36it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 924458.84it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 834995.08it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1422244.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  4\n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8671700264239719\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  0  0]\n",
      " [ 0 15  1]\n",
      " [ 3  3 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91        15\n",
      "           1       0.83      0.94      0.88        16\n",
      "           2       0.94      0.74      0.83        23\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.87      0.89      0.87        54\n",
      "weighted avg       0.88      0.87      0.87        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Fold No.:  [  0   1   2   3   4   5   6   7   9  10  11  12  15  16  18  19  22  23\n",
      "  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41\n",
      "  42  43  44  45  46  47  49  51  53  55  56  60  61  62  64  65  66  67\n",
      "  68  69  70  73  74  75  76  77  78  79  80  81  82  83  84  85  86  90\n",
      "  91  92  93  94  95  96  97  98 100 101 103 104 105 108 109 110 111 112\n",
      " 113 114 115 116 117 118 119 120 122 123 124 125 126 127 128 132 133 135\n",
      " 136 137 138 139 140 141 142 143 144 145 146 147 148 150 152 153 154 155\n",
      " 156 157 158 159 161 162 163 164 165 167 168 170 171 172 173 175 176 177\n",
      " 178 179 180 181 182 183 184 185 186 190 192 193 194 195 196 197 198 199\n",
      " 200 201 202 204 205 206 208 209 210 211 212 213 215 216 217 220 221 222\n",
      " 223 224 225 226 227 229 230 231 232 233 234 236 237 240 241 242 243 244\n",
      " 245 246 247 250 251 252 253 256 257 259 260 261 262 263 264 266 267 268]  Test Fold No.:  [  8  13  14  17  20  21  48  50  52  54  57  58  59  63  71  72  87  88\n",
      "  89  99 102 106 107 121 129 130 131 134 149 151 160 166 169 174 187 188\n",
      " 189 191 203 207 214 218 219 228 235 238 239 248 249 254 255 258 265 269]\n",
      "No. of Training Dataset in Fold:  5  216\n",
      "No. of Testing Dataset in Fold:  5  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 958698.06it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1015661.06it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1225940.01it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1212810.80it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1195210.64it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 937856.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  5\n",
      "Testing accuracy 0.8333333333333334\n",
      "Testing F1 score: 0.8327721661054994\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 1 18  2]\n",
      " [ 2  2 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85        16\n",
      "           1       0.86      0.86      0.86        21\n",
      "           2       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.83        54\n",
      "   macro avg       0.83      0.83      0.83        54\n",
      "weighted avg       0.83      0.83      0.83        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Fold: \", total_fold)\n",
    "fold_no = 1\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "    print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold: \", fold_no, \"\", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold: \", fold_no, \"\", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg.fit(X_trained, y_trained)\n",
    "    y_pred = logreg.predict(X_tested)\n",
    "    \n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-FOLD PV-DM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold:  5\n",
      "No. of Training Dataset in Fold  1 :  216\n",
      "No. of Testing Dataset in Fold  1 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 937856.80it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 911438.29it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 811074.01it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1111619.22it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1305431.79it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 161175.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  1\n",
      "Testing accuracy 0.8333333333333334\n",
      "Testing F1 score: 0.8304550885196046\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[12  2  4]\n",
      " [ 0 20  1]\n",
      " [ 1  1 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.67      0.77        18\n",
      "           1       0.87      0.95      0.91        21\n",
      "           2       0.72      0.87      0.79        15\n",
      "\n",
      "    accuracy                           0.83        54\n",
      "   macro avg       0.84      0.83      0.82        54\n",
      "weighted avg       0.85      0.83      0.83        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  216\n",
      "No. of Testing Dataset in Fold  2 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 1088905.85it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1058375.78it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1100813.69it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1385274.72it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1007752.69it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1482765.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  2\n",
      "Testing accuracy 0.8518518518518519\n",
      "Testing F1 score: 0.8499473832807166\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  1  3]\n",
      " [ 0 21  1]\n",
      " [ 2  1 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.71      0.77        14\n",
      "           1       0.91      0.95      0.93        22\n",
      "           2       0.79      0.83      0.81        18\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.85      0.83      0.84        54\n",
      "weighted avg       0.85      0.85      0.85        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  216\n",
      "No. of Testing Dataset in Fold  3 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 520971.63it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 895226.94it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 983680.42it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 547413.69it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1119863.61it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1581098.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  3\n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8895405669599218\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  0  2]\n",
      " [ 2 15  0]\n",
      " [ 1  1 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84        15\n",
      "           1       0.94      0.88      0.91        17\n",
      "           2       0.91      0.91      0.91        22\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.89      0.89      0.89        54\n",
      "weighted avg       0.89      0.89      0.89        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  216\n",
      "No. of Testing Dataset in Fold  4 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 1454204.92it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1190498.90it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1217701.16it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1229266.84it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 154919.57it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 171487.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  4\n",
      "Testing accuracy 0.8333333333333334\n",
      "Testing F1 score: 0.8298941798941799\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  0  4]\n",
      " [ 0 16  0]\n",
      " [ 2  3 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.73      0.79        15\n",
      "           1       0.84      1.00      0.91        16\n",
      "           2       0.82      0.78      0.80        23\n",
      "\n",
      "    accuracy                           0.83        54\n",
      "   macro avg       0.84      0.84      0.83        54\n",
      "weighted avg       0.83      0.83      0.83        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  216\n",
      "No. of Testing Dataset in Fold  5 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 1023694.54it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 165473.91it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1230937.04it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1444927.69it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 725356.02it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 532923.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No.:  5\n",
      "Testing accuracy 0.8518518518518519\n",
      "Testing F1 score: 0.8515169617320155\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 0 19  2]\n",
      " [ 1  3 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90        16\n",
      "           1       0.83      0.90      0.86        21\n",
      "           2       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.86      0.85      0.85        54\n",
      "weighted avg       0.85      0.85      0.85        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold: \", total_fold)\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg.fit(X_trained, y_trained)\n",
    "    y_pred = logreg.predict(X_tested)\n",
    "    \n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n\\n\")\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAIRED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 5\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  1 :  216\n",
      "No. of Testing Dataset in Fold  1 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 745040.84it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1301680.55it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 825861.13it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1211189.39it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 845120.96it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 695829.24it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1268865.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8894500561167228\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  2  1]\n",
      " [ 0 20  1]\n",
      " [ 0  2 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91        18\n",
      "           1       0.83      0.95      0.89        21\n",
      "           2       0.87      0.87      0.87        15\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.90      0.88      0.89        54\n",
      "weighted avg       0.90      0.89      0.89        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 1352193.53it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1013388.89it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1073423.77it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1290555.08it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 741989.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8700851683509112\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  1  4]\n",
      " [ 0 20  1]\n",
      " [ 0  1 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.72      0.84        18\n",
      "           1       0.91      0.95      0.93        21\n",
      "           2       0.74      0.93      0.82        15\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.88      0.87      0.86        54\n",
      "weighted avg       0.89      0.87      0.87        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model: \n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8708964646464645\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  3]\n",
      " [ 0 20  1]\n",
      " [ 0  2 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.78      0.88        18\n",
      "           1       0.87      0.95      0.91        21\n",
      "           2       0.76      0.87      0.81        15\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.88      0.87      0.87        54\n",
      "weighted avg       0.88      0.87      0.87        54\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  216\n",
      "No. of Testing Dataset in Fold  2 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 938828.67it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 870287.86it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 721313.43it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1307315.53it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 872803.14it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1497470.52it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1378949.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.869906943981018\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  2  1]\n",
      " [ 0 20  2]\n",
      " [ 2  0 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.81        14\n",
      "           1       0.91      0.91      0.91        22\n",
      "           2       0.84      0.89      0.86        18\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.87      0.86      0.86        54\n",
      "weighted avg       0.87      0.87      0.87        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 1214436.55it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1261796.19it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1458888.35it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1224283.33it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1048576.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8333333333333334\n",
      "Testing F1 score: 0.825839217143565\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 8  2  4]\n",
      " [ 0 21  1]\n",
      " [ 1  1 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.57      0.70        14\n",
      "           1       0.88      0.95      0.91        22\n",
      "           2       0.76      0.89      0.82        18\n",
      "\n",
      "    accuracy                           0.83        54\n",
      "   macro avg       0.84      0.80      0.81        54\n",
      "weighted avg       0.84      0.83      0.83        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model: \n",
      "Testing accuracy 0.8333333333333334\n",
      "Testing F1 score: 0.8289176132654394\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  2  3]\n",
      " [ 0 21  1]\n",
      " [ 2  1 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.64      0.72        14\n",
      "           1       0.88      0.95      0.91        22\n",
      "           2       0.79      0.83      0.81        18\n",
      "\n",
      "    accuracy                           0.83        54\n",
      "   macro avg       0.83      0.81      0.81        54\n",
      "weighted avg       0.83      0.83      0.83        54\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  216\n",
      "No. of Testing Dataset in Fold  3 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 1094166.26it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 977313.55it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1013388.89it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 697973.55it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 640261.25it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1326456.32it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1402429.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8698806104205327\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  0  2]\n",
      " [ 2 13  2]\n",
      " [ 1  0 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84        15\n",
      "           1       1.00      0.76      0.87        17\n",
      "           2       0.84      0.95      0.89        22\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.88      0.86      0.87        54\n",
      "weighted avg       0.88      0.87      0.87        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 1229266.84it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1540764.73it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 632660.38it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 603979.78it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1224283.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8893044517167938\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  0  2]\n",
      " [ 2 14  1]\n",
      " [ 1  0 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84        15\n",
      "           1       1.00      0.82      0.90        17\n",
      "           2       0.88      0.95      0.91        22\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.90      0.88      0.88        54\n",
      "weighted avg       0.90      0.89      0.89        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model: \n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8893044517167938\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  0  2]\n",
      " [ 2 14  1]\n",
      " [ 1  0 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84        15\n",
      "           1       1.00      0.82      0.90        17\n",
      "           2       0.88      0.95      0.91        22\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.90      0.88      0.88        54\n",
      "weighted avg       0.90      0.89      0.89        54\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  216\n",
      "No. of Testing Dataset in Fold  4 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 1083695.77it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1227601.17it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 578894.35it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1007752.69it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1247892.10it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1195210.64it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1442626.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9074074074074074\n",
      "Testing F1 score: 0.90981704200095\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  0]\n",
      " [ 0 16  0]\n",
      " [ 0  4 19]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.97        15\n",
      "           1       0.76      1.00      0.86        16\n",
      "           2       1.00      0.83      0.90        23\n",
      "\n",
      "    accuracy                           0.91        54\n",
      "   macro avg       0.92      0.92      0.91        54\n",
      "weighted avg       0.93      0.91      0.91        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 971028.58it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1332308.33it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 645737.47it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 167803.23it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1249613.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9074074074074074\n",
      "Testing F1 score: 0.9078189300411523\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[12  2  1]\n",
      " [ 0 16  0]\n",
      " [ 0  2 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89        15\n",
      "           1       0.80      1.00      0.89        16\n",
      "           2       0.95      0.91      0.93        23\n",
      "\n",
      "    accuracy                           0.91        54\n",
      "   macro avg       0.92      0.90      0.90        54\n",
      "weighted avg       0.92      0.91      0.91        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model: \n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8708089668615984\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  0]\n",
      " [ 0 16  0]\n",
      " [ 1  5 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        15\n",
      "           1       0.73      1.00      0.84        16\n",
      "           2       1.00      0.74      0.85        23\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.89      0.89      0.88        54\n",
      "weighted avg       0.90      0.87      0.87        54\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  216\n",
      "No. of Testing Dataset in Fold  5 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 978368.97it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 952649.49it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1283243.15it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1079820.82it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1128231.21it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1232611.79it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1473121.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8518518518518519\n",
      "Testing F1 score: 0.8521255919629903\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 1 18  2]\n",
      " [ 2  1 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85        16\n",
      "           1       0.90      0.86      0.88        21\n",
      "           2       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.85      0.85      0.85        54\n",
      "weighted avg       0.85      0.85      0.85        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 967916.31it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1309204.72it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1129637.99it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1254805.63it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1239356.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.7777777777777778\n",
      "Testing F1 score: 0.7792181069958848\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  1  2]\n",
      " [ 1 16  4]\n",
      " [ 2  2 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81        16\n",
      "           1       0.84      0.76      0.80        21\n",
      "           2       0.68      0.76      0.72        17\n",
      "\n",
      "    accuracy                           0.78        54\n",
      "   macro avg       0.78      0.78      0.78        54\n",
      "weighted avg       0.78      0.78      0.78        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model: \n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8695847362514029\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  2  1]\n",
      " [ 0 20  1]\n",
      " [ 1  2 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.87        16\n",
      "           1       0.83      0.95      0.89        21\n",
      "           2       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.88      0.86      0.87        54\n",
      "weighted avg       0.87      0.87      0.87        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: {}\\n\\n\" .format(total_fold))\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    logreg_dbow = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg_dbow.fit(X_trained, y_trained)\n",
    "    y_pred = logreg_dbow.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    logreg_dm = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg_dm.fit(X_trained, y_trained)\n",
    "    y_pred = logreg_dm.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    logreg_paired = LogisticRegression(n_jobs=1, C=1e5)\n",
    "    logreg_paired.fit(X_train, y_train)\n",
    "    y_pred = logreg_paired.predict(X_test)\n",
    "    \n",
    "    print(\"FOR Paired Model: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Using Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 5\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  1 :  216\n",
      "No. of Testing Dataset in Fold  1 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 973114.57it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 966883.31it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1175057.93it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 903259.88it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1176583.98it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1020236.11it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 594858.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Entropy: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.870320589406611\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  2  2]\n",
      " [ 0 20  1]\n",
      " [ 0  2 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.78      0.88        18\n",
      "           1       0.83      0.95      0.89        21\n",
      "           2       0.81      0.87      0.84        15\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.88      0.87      0.87        54\n",
      "weighted avg       0.88      0.87      0.87        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 1118481.07it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1152633.16it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1232611.79it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1179648.00it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1487634.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Entropy:  \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.9259259259259259\n",
      "Testing F1 score: 0.9254307783719548\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  3  0]\n",
      " [ 1 20  0]\n",
      " [ 0  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.83      0.88        18\n",
      "           1       0.87      0.95      0.91        21\n",
      "           2       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           0.93        54\n",
      "   macro avg       0.94      0.93      0.93        54\n",
      "weighted avg       0.93      0.93      0.93        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Entropy: \n",
      "Testing accuracy 0.9259259259259259\n",
      "Testing F1 score: 0.9254307783719548\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  3  0]\n",
      " [ 1 20  0]\n",
      " [ 0  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.83      0.88        18\n",
      "           1       0.87      0.95      0.91        21\n",
      "           2       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           0.93        54\n",
      "   macro avg       0.94      0.93      0.93        54\n",
      "weighted avg       0.93      0.93      0.93        54\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  216\n",
      "No. of Testing Dataset in Fold  2 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 958698.06it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 991214.07it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1350178.34it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 964823.92it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1305431.79it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 904161.34it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1332308.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Entropy: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8713207995485461\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[12  1  1]\n",
      " [ 0 20  2]\n",
      " [ 3  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83        14\n",
      "           1       0.95      0.91      0.93        22\n",
      "           2       0.83      0.83      0.83        18\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.86      0.87      0.86        54\n",
      "weighted avg       0.87      0.87      0.87        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 1164485.43it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1263556.02it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 978368.97it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1155573.55it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 608033.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Entropy:  \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8333333333333334\n",
      "Testing F1 score: 0.8323169681309216\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  1  4]\n",
      " [ 0 20  2]\n",
      " [ 2  0 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.64      0.72        14\n",
      "           1       0.95      0.91      0.93        22\n",
      "           2       0.73      0.89      0.80        18\n",
      "\n",
      "    accuracy                           0.83        54\n",
      "   macro avg       0.83      0.81      0.82        54\n",
      "weighted avg       0.84      0.83      0.83        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Entropy: \n",
      "Testing accuracy 0.8333333333333334\n",
      "Testing F1 score: 0.8323169681309216\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  1  4]\n",
      " [ 0 20  2]\n",
      " [ 2  0 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.64      0.72        14\n",
      "           1       0.95      0.91      0.93        22\n",
      "           2       0.73      0.89      0.80        18\n",
      "\n",
      "    accuracy                           0.83        54\n",
      "   macro avg       0.83      0.81      0.82        54\n",
      "weighted avg       0.84      0.83      0.83        54\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  216\n",
      "No. of Testing Dataset in Fold  3 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 958698.06it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 959713.63it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1336238.44it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 904161.34it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1246175.60it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1256546.00it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 553432.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Entropy: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8694142142876552\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[12  1  2]\n",
      " [ 1 14  2]\n",
      " [ 1  0 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.80      0.83        15\n",
      "           1       0.93      0.82      0.87        17\n",
      "           2       0.84      0.95      0.89        22\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.88      0.86      0.87        54\n",
      "weighted avg       0.87      0.87      0.87        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 1167486.68it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 148812.36it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1118481.07it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 831164.83it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1001071.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Entropy:  \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8148148148148148\n",
      "Testing F1 score: 0.818342151675485\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[12  2  1]\n",
      " [ 2 14  1]\n",
      " [ 4  0 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73        15\n",
      "           1       0.88      0.82      0.85        17\n",
      "           2       0.90      0.82      0.86        22\n",
      "\n",
      "    accuracy                           0.81        54\n",
      "   macro avg       0.81      0.81      0.81        54\n",
      "weighted avg       0.83      0.81      0.82        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Entropy: \n",
      "Testing accuracy 0.8148148148148148\n",
      "Testing F1 score: 0.818342151675485\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[12  2  1]\n",
      " [ 2 14  1]\n",
      " [ 4  0 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73        15\n",
      "           1       0.88      0.82      0.85        17\n",
      "           2       0.90      0.82      0.86        22\n",
      "\n",
      "    accuracy                           0.81        54\n",
      "   macro avg       0.81      0.81      0.81        54\n",
      "weighted avg       0.83      0.81      0.82        54\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  216\n",
      "No. of Testing Dataset in Fold  4 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 943718.40it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 518289.28it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1372681.31it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 675592.59it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 763243.19it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1281428.10it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 558550.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Entropy: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8676651739779431\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  0  0]\n",
      " [ 0 15  1]\n",
      " [ 4  2 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88        15\n",
      "           1       0.88      0.94      0.91        16\n",
      "           2       0.94      0.74      0.83        23\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.87      0.89      0.87        54\n",
      "weighted avg       0.88      0.87      0.87        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 934953.21it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 606811.56it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 217050.71it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1234291.10it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 803877.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Entropy:  \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8894992553529141\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  0]\n",
      " [ 0 16  0]\n",
      " [ 1  4 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        15\n",
      "           1       0.76      1.00      0.86        16\n",
      "           2       1.00      0.78      0.88        23\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.90      0.91      0.89        54\n",
      "weighted avg       0.91      0.89      0.89        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Entropy: \n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8894992553529141\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  0]\n",
      " [ 0 16  0]\n",
      " [ 1  4 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        15\n",
      "           1       0.76      1.00      0.86        16\n",
      "           2       1.00      0.78      0.88        23\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.90      0.91      0.89        54\n",
      "weighted avg       0.91      0.89      0.89        54\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  216\n",
      "No. of Testing Dataset in Fold  5 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 896112.43it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 886467.38it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1383159.79it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1054679.47it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 622231.91it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1037765.94it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 661774.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Entropy: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8690885105535643\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 0 20  1]\n",
      " [ 1  3 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90        16\n",
      "           1       0.83      0.95      0.89        21\n",
      "           2       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.88      0.86      0.87        54\n",
      "weighted avg       0.87      0.87      0.87        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 964823.92it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1192065.35it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 464123.80it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1239356.59it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1033032.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Entropy:  \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.7962962962962963\n",
      "Testing F1 score: 0.792552767821585\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  2  1]\n",
      " [ 0 19  2]\n",
      " [ 3  3 11]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81        16\n",
      "           1       0.79      0.90      0.84        21\n",
      "           2       0.79      0.65      0.71        17\n",
      "\n",
      "    accuracy                           0.80        54\n",
      "   macro avg       0.80      0.79      0.79        54\n",
      "weighted avg       0.80      0.80      0.79        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Entropy: \n",
      "Testing accuracy 0.7962962962962963\n",
      "Testing F1 score: 0.792552767821585\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  2  1]\n",
      " [ 0 19  2]\n",
      " [ 3  3 11]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81        16\n",
      "           1       0.79      0.90      0.84        21\n",
      "           2       0.79      0.65      0.71        17\n",
      "\n",
      "    accuracy                           0.80        54\n",
      "   macro avg       0.80      0.79      0.79        54\n",
      "weighted avg       0.80      0.80      0.79        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: {}\\n\\n\" .format(total_fold))\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"entropy\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using Decision Tree with Entropy: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"entropy\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using Decision Tree with Entropy:  \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"entropy\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR Paired Model Using Decision Tree with Entropy: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Using Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 5\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  1 :  216\n",
      "No. of Testing Dataset in Fold  1 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 1182728.02it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 972070.45it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1253070.07it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1254805.63it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1078535.31it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 185535.46it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 603979.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Gini: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8903248167954051\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[16  2  0]\n",
      " [ 0 19  2]\n",
      " [ 0  2 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94        18\n",
      "           1       0.83      0.90      0.86        21\n",
      "           2       0.87      0.87      0.87        15\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.90      0.89      0.89        54\n",
      "weighted avg       0.90      0.89      0.89        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 1028342.41it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 885600.84it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1564714.45it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1219340.06it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 535759.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Gini:  \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8674242424242423\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  3  2]\n",
      " [ 0 20  1]\n",
      " [ 1  0 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.72      0.81        18\n",
      "           1       0.87      0.95      0.91        21\n",
      "           2       0.82      0.93      0.87        15\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.87      0.87      0.87        54\n",
      "weighted avg       0.88      0.87      0.87        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Gini: \n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8674242424242423\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  3  2]\n",
      " [ 0 20  1]\n",
      " [ 1  0 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.72      0.81        18\n",
      "           1       0.87      0.95      0.91        21\n",
      "           2       0.82      0.93      0.87        15\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.87      0.87      0.87        54\n",
      "weighted avg       0.88      0.87      0.87        54\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  216\n",
      "No. of Testing Dataset in Fold  2 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 941756.41it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 924458.84it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1142458.59it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1442626.85it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1219340.06it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 597211.38it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 610080.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Gini: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8518518518518519\n",
      "Testing F1 score: 0.8505023241865346\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  2  2]\n",
      " [ 0 20  2]\n",
      " [ 2  0 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.71      0.77        14\n",
      "           1       0.91      0.91      0.91        22\n",
      "           2       0.80      0.89      0.84        18\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.85      0.84      0.84        54\n",
      "weighted avg       0.85      0.85      0.85        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 1175057.93it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 736560.70it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 915120.87it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1198372.57it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1408973.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Gini:  \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8692729766803841\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  1  2]\n",
      " [ 0 21  1]\n",
      " [ 2  1 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.81        14\n",
      "           1       0.91      0.95      0.93        22\n",
      "           2       0.83      0.83      0.83        18\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.86      0.86      0.86        54\n",
      "weighted avg       0.87      0.87      0.87        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Gini: \n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8692729766803841\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  1  2]\n",
      " [ 0 21  1]\n",
      " [ 2  1 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.81        14\n",
      "           1       0.91      0.95      0.93        22\n",
      "           2       0.83      0.83      0.83        18\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.86      0.86      0.86        54\n",
      "weighted avg       0.87      0.87      0.87        54\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  216\n",
      "No. of Testing Dataset in Fold  3 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 979426.66it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1145347.24it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 990130.78it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1157049.38it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1220983.37it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1170503.44it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 474826.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Gini: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8518518518518519\n",
      "Testing F1 score: 0.8499927903431796\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  0  2]\n",
      " [ 2 12  3]\n",
      " [ 1  0 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84        15\n",
      "           1       1.00      0.71      0.83        17\n",
      "           2       0.81      0.95      0.88        22\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.87      0.84      0.85        54\n",
      "weighted avg       0.87      0.85      0.85        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 946676.76it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 165504.14it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 590208.25it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1118481.07it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1246175.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Gini:  \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8518518518518519\n",
      "Testing F1 score: 0.8499927903431796\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  0  2]\n",
      " [ 2 12  3]\n",
      " [ 1  0 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84        15\n",
      "           1       1.00      0.71      0.83        17\n",
      "           2       0.81      0.95      0.88        22\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.87      0.84      0.85        54\n",
      "weighted avg       0.87      0.85      0.85        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Gini: \n",
      "Testing accuracy 0.8518518518518519\n",
      "Testing F1 score: 0.8499927903431796\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  0  2]\n",
      " [ 2 12  3]\n",
      " [ 1  0 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84        15\n",
      "           1       1.00      0.71      0.83        17\n",
      "           2       0.81      0.95      0.88        22\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.87      0.84      0.85        54\n",
      "weighted avg       0.87      0.85      0.85        54\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  216\n",
      "No. of Testing Dataset in Fold  4 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 992299.74it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1199959.82it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 955664.20it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 649906.50it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 916046.17it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1013388.89it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 911438.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Gini: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8868748933265062\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  0  1]\n",
      " [ 0 16  0]\n",
      " [ 2  3 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        15\n",
      "           1       0.84      1.00      0.91        16\n",
      "           2       0.95      0.78      0.86        23\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.89      0.91      0.89        54\n",
      "weighted avg       0.90      0.89      0.89        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 1468346.29it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1077252.87it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1102152.88it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 795407.96it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1461241.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Gini:  \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.7777777777777778\n",
      "Testing F1 score: 0.7804017722716909\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  3  2]\n",
      " [ 0 16  0]\n",
      " [ 0  7 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80        15\n",
      "           1       0.62      1.00      0.76        16\n",
      "           2       0.89      0.70      0.78        23\n",
      "\n",
      "    accuracy                           0.78        54\n",
      "   macro avg       0.83      0.79      0.78        54\n",
      "weighted avg       0.84      0.78      0.78        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Gini: \n",
      "Testing accuracy 0.7777777777777778\n",
      "Testing F1 score: 0.7804017722716909\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  3  2]\n",
      " [ 0 16  0]\n",
      " [ 0  7 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80        15\n",
      "           1       0.62      1.00      0.76        16\n",
      "           2       0.89      0.70      0.78        23\n",
      "\n",
      "    accuracy                           0.78        54\n",
      "   macro avg       0.83      0.79      0.78        54\n",
      "weighted avg       0.84      0.78      0.78        54\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  216\n",
      "No. of Testing Dataset in Fold  5 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 966883.31it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 944702.47it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 502479.01it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1196789.52it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1012256.61it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 814720.92it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1312999.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Decision Tree with Gini: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8148148148148148\n",
      "Testing F1 score: 0.8148148148148148\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 1 17  3]\n",
      " [ 1  3 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88        16\n",
      "           1       0.81      0.81      0.81        21\n",
      "           2       0.76      0.76      0.76        17\n",
      "\n",
      "    accuracy                           0.81        54\n",
      "   macro avg       0.82      0.82      0.82        54\n",
      "weighted avg       0.81      0.81      0.81        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 915120.87it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1054679.47it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 765823.89it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1198372.57it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1348169.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Decision Tree with Gini:  \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8897690794242519\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  1  2]\n",
      " [ 0 19  2]\n",
      " [ 0  1 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.81      0.90        16\n",
      "           1       0.90      0.90      0.90        21\n",
      "           2       0.80      0.94      0.86        17\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.90      0.89      0.89        54\n",
      "weighted avg       0.90      0.89      0.89        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR Paired Model Using Decision Tree with Gini: \n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8897690794242519\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  1  2]\n",
      " [ 0 19  2]\n",
      " [ 0  1 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.81      0.90        16\n",
      "           1       0.90      0.90      0.90        21\n",
      "           2       0.80      0.94      0.86        17\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.90      0.89      0.89        54\n",
      "weighted avg       0.90      0.89      0.89        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: {}\\n\\n\" .format(total_fold))\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using Decision Tree with Gini: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using Decision Tree with Gini:  \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    dt_dbow_entropy = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                                             random_state = 10, \n",
    "                                             max_depth = 3, \n",
    "                                             min_samples_leaf = 5)\n",
    "    dt_dbow_entropy.fit(X_trained, y_trained)\n",
    "    y_pred = dt_dbow_entropy.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR Paired Model Using Decision Tree with Gini: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbour with Minkowski Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 5\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  1 :  216\n",
      "No. of Testing Dataset in Fold  1 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 1059613.64it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1062098.08it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1242756.74it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 912356.16it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 800326.56it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1064594.20it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 573398.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Minkowski Distance: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8888388049983723\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  2  1]\n",
      " [ 1 20  0]\n",
      " [ 0  2 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.83      0.88        18\n",
      "           1       0.83      0.95      0.89        21\n",
      "           2       0.93      0.87      0.90        15\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.90      0.88      0.89        54\n",
      "weighted avg       0.89      0.89      0.89        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 905064.60it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1461241.39it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 848286.20it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 156823.55it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1387396.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Minkowski Distance: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8897551092318534\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  2  1]\n",
      " [ 0 19  2]\n",
      " [ 0  1 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91        18\n",
      "           1       0.86      0.90      0.88        21\n",
      "           2       0.82      0.93      0.87        15\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.90      0.89      0.89        54\n",
      "weighted avg       0.90      0.89      0.89        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Minkowski Distance: \n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8897551092318534\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  2  1]\n",
      " [ 0 19  2]\n",
      " [ 0  1 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91        18\n",
      "           1       0.86      0.90      0.88        21\n",
      "           2       0.82      0.93      0.87        15\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.90      0.89      0.89        54\n",
      "weighted avg       0.90      0.89      0.89        54\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  216\n",
      "No. of Testing Dataset in Fold  2 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 971028.58it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 972070.45it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 774333.05it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1229266.84it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 906876.54it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1290555.08it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 472105.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Minkowski Distance: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.9259259259259259\n",
      "Testing F1 score: 0.9260901295384053\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  1  0]\n",
      " [ 0 21  1]\n",
      " [ 2  0 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90        14\n",
      "           1       0.95      0.95      0.95        22\n",
      "           2       0.94      0.89      0.91        18\n",
      "\n",
      "    accuracy                           0.93        54\n",
      "   macro avg       0.92      0.92      0.92        54\n",
      "weighted avg       0.93      0.93      0.93        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 968951.51it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1279618.17it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 577786.78it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1433496.30it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1152633.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Minkowski Distance: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8518518518518519\n",
      "Testing F1 score: 0.8515579071134627\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  3  0]\n",
      " [ 0 20  2]\n",
      " [ 3  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79        14\n",
      "           1       0.87      0.91      0.89        22\n",
      "           2       0.88      0.83      0.86        18\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.85      0.84      0.84        54\n",
      "weighted avg       0.85      0.85      0.85        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Minkowski Distance: \n",
      "Testing accuracy 0.8518518518518519\n",
      "Testing F1 score: 0.8515579071134627\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[11  3  0]\n",
      " [ 0 20  2]\n",
      " [ 3  0 15]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79        14\n",
      "           1       0.87      0.91      0.89        22\n",
      "           2       0.88      0.83      0.86        18\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.85      0.84      0.84        54\n",
      "weighted avg       0.85      0.85      0.85        54\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  216\n",
      "No. of Testing Dataset in Fold  3 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 952649.49it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 520971.63it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1461241.39it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1054679.47it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1395947.09it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 925403.13it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 887335.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Minkowski Distance: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8518518518518519\n",
      "Testing F1 score: 0.8503266478558174\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[12  1  2]\n",
      " [ 2 13  2]\n",
      " [ 1  0 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80        15\n",
      "           1       0.93      0.76      0.84        17\n",
      "           2       0.84      0.95      0.89        22\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.86      0.84      0.84        54\n",
      "weighted avg       0.86      0.85      0.85        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 969988.93it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1297950.81it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 959713.63it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 523379.36it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 598394.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Minkowski Distance: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.9074074074074074\n",
      "Testing F1 score: 0.9066059338908801\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  0]\n",
      " [ 1 14  2]\n",
      " [ 1  0 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        15\n",
      "           1       0.93      0.82      0.87        17\n",
      "           2       0.91      0.95      0.93        22\n",
      "\n",
      "    accuracy                           0.91        54\n",
      "   macro avg       0.91      0.90      0.90        54\n",
      "weighted avg       0.91      0.91      0.91        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Minkowski Distance: \n",
      "Testing accuracy 0.9074074074074074\n",
      "Testing F1 score: 0.9066059338908801\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  0]\n",
      " [ 1 14  2]\n",
      " [ 1  0 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        15\n",
      "           1       0.93      0.82      0.87        17\n",
      "           2       0.91      0.95      0.93        22\n",
      "\n",
      "    accuracy                           0.91        54\n",
      "   macro avg       0.91      0.90      0.90        54\n",
      "weighted avg       0.91      0.91      0.91        54\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  216\n",
      "No. of Testing Dataset in Fold  4 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 1062098.08it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1307315.53it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1062098.08it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 924458.84it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1422244.37it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1155573.55it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1091529.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Minkowski Distance: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8691893506409635\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  0]\n",
      " [ 0 16  0]\n",
      " [ 2  4 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        15\n",
      "           1       0.76      1.00      0.86        16\n",
      "           2       1.00      0.74      0.85        23\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.88      0.89      0.87        54\n",
      "weighted avg       0.89      0.87      0.87        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 975209.54it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1261796.19it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1338212.21it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 732985.17it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 960731.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Minkowski Distance: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8858281893004116\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  0  0]\n",
      " [ 0 16  0]\n",
      " [ 2  4 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94        15\n",
      "           1       0.80      1.00      0.89        16\n",
      "           2       1.00      0.74      0.85        23\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.89      0.91      0.89        54\n",
      "weighted avg       0.91      0.89      0.89        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Minkowski Distance: \n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8858281893004116\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  0  0]\n",
      " [ 0 16  0]\n",
      " [ 2  4 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94        15\n",
      "           1       0.80      1.00      0.89        16\n",
      "           2       1.00      0.74      0.85        23\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.89      0.91      0.89        54\n",
      "weighted avg       0.91      0.89      0.89        54\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  216\n",
      "No. of Testing Dataset in Fold  5 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 955664.20it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 958698.06it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1187378.33it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 691052.38it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1468346.29it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1458888.35it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 511847.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Minkowski Distance: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8333333333333334\n",
      "Testing F1 score: 0.8239489032327906\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 0 21  0]\n",
      " [ 4  3 10]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.82        16\n",
      "           1       0.84      1.00      0.91        21\n",
      "           2       0.91      0.59      0.71        17\n",
      "\n",
      "    accuracy                           0.83        54\n",
      "   macro avg       0.84      0.82      0.82        54\n",
      "weighted avg       0.84      0.83      0.82        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 959713.63it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1204746.89it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1444927.69it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 672084.32it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 577050.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Minkowski Distance: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8518518518518519\n",
      "Testing F1 score: 0.8486658701712465\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 0 20  1]\n",
      " [ 2  3 12]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88        16\n",
      "           1       0.83      0.95      0.89        21\n",
      "           2       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.86      0.84      0.85        54\n",
      "weighted avg       0.85      0.85      0.85        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Minkowski Distance: \n",
      "Testing accuracy 0.8518518518518519\n",
      "Testing F1 score: 0.8486658701712465\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 0 20  1]\n",
      " [ 2  3 12]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88        16\n",
      "           1       0.83      0.95      0.89        21\n",
      "           2       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.86      0.84      0.85        54\n",
      "weighted avg       0.85      0.85      0.85        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: {}\\n\\n\" .format(total_fold))\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "        \n",
    "    knn_dbow = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski')\n",
    "    knn_dbow.fit(X_trained, y_trained)\n",
    "    y_pred = knn_dbow.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using KNN with Minkowski Distance: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    knn_dm = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski')\n",
    "    knn_dm.fit(X_trained, y_trained)\n",
    "    y_pred = knn_dm.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using KNN with Minkowski Distance: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    knn_mixed = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski')\n",
    "    knn_mixed.fit(X_trained, y_trained)\n",
    "    y_pred = knn_mixed.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PAIRED Model Using KNN with Minkowski Distance: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbour with Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 5\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  1 :  216\n",
      "No. of Testing Dataset in Fold  1 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 1173535.83it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1016800.97it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 968951.51it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 965852.52it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1216066.66it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1190498.90it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 925403.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Euclidean Distance: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.87071697365815\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  2  1]\n",
      " [ 1 19  1]\n",
      " [ 0  2 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.83      0.88        18\n",
      "           1       0.83      0.90      0.86        21\n",
      "           2       0.87      0.87      0.87        15\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.88      0.87      0.87        54\n",
      "weighted avg       0.87      0.87      0.87        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 979426.66it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 647121.19it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1114353.83it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 948659.33it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 941756.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Euclidean Distance: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8876409443269908\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  2  2]\n",
      " [ 0 20  1]\n",
      " [ 1  0 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.78      0.85        18\n",
      "           1       0.91      0.95      0.93        21\n",
      "           2       0.82      0.93      0.87        15\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.89      0.89      0.88        54\n",
      "weighted avg       0.89      0.89      0.89        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Euclidean Distance: \n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8876409443269908\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  2  2]\n",
      " [ 0 20  1]\n",
      " [ 1  0 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.78      0.85        18\n",
      "           1       0.91      0.95      0.93        21\n",
      "           2       0.82      0.93      0.87        15\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.89      0.89      0.88        54\n",
      "weighted avg       0.89      0.89      0.89        54\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  216\n",
      "No. of Testing Dataset in Fold  2 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 961751.24it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1334270.49it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1235974.98it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 574853.85it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 155611.42it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1475520.63it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1136724.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Euclidean Distance: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.9074074074074074\n",
      "Testing F1 score: 0.9077192669814964\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  1  0]\n",
      " [ 0 20  2]\n",
      " [ 2  0 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90        14\n",
      "           1       0.95      0.91      0.93        22\n",
      "           2       0.89      0.89      0.89        18\n",
      "\n",
      "    accuracy                           0.91        54\n",
      "   macro avg       0.90      0.91      0.91        54\n",
      "weighted avg       0.91      0.91      0.91        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 883872.84it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 971028.58it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 926349.35it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 688426.80it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1292396.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Euclidean Distance: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.8518518518518519\n",
      "Testing F1 score: 0.8507025544062583\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  2  2]\n",
      " [ 1 20  1]\n",
      " [ 2  0 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.71      0.74        14\n",
      "           1       0.91      0.91      0.91        22\n",
      "           2       0.84      0.89      0.86        18\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.84      0.84      0.84        54\n",
      "weighted avg       0.85      0.85      0.85        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Euclidean Distance: \n",
      "Testing accuracy 0.8518518518518519\n",
      "Testing F1 score: 0.8507025544062583\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[10  2  2]\n",
      " [ 1 20  1]\n",
      " [ 2  0 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.71      0.74        14\n",
      "           1       0.91      0.91      0.91        22\n",
      "           2       0.84      0.89      0.86        18\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.84      0.84      0.84        54\n",
      "weighted avg       0.85      0.85      0.85        54\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  216\n",
      "No. of Testing Dataset in Fold  3 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 972070.45it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 896112.43it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1172017.68it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1084993.61it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1035393.90it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 817662.15it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 573398.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Euclidean Distance: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8704710144927537\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  0  2]\n",
      " [ 1 14  2]\n",
      " [ 1  1 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87        15\n",
      "           1       0.93      0.82      0.87        17\n",
      "           2       0.83      0.91      0.87        22\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.88      0.87      0.87        54\n",
      "weighted avg       0.87      0.87      0.87        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 1190498.90it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 688426.80it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 148422.29it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 68931.73it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 716748.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Euclidean Distance: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8704608812135695\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  1  1]\n",
      " [ 2 14  1]\n",
      " [ 1  1 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84        15\n",
      "           1       0.88      0.82      0.85        17\n",
      "           2       0.91      0.91      0.91        22\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.87      0.87      0.87        54\n",
      "weighted avg       0.87      0.87      0.87        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Euclidean Distance: \n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8704608812135695\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  1  1]\n",
      " [ 2 14  1]\n",
      " [ 1  1 20]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84        15\n",
      "           1       0.88      0.82      0.85        17\n",
      "           2       0.91      0.91      0.91        22\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.87      0.87      0.87        54\n",
      "weighted avg       0.87      0.87      0.87        54\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  216\n",
      "No. of Testing Dataset in Fold  4 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 1119863.61it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 912356.16it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1322583.45it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1219340.06it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 784389.32it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1461241.39it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1152633.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Euclidean Distance: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8894992553529141\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  0]\n",
      " [ 0 16  0]\n",
      " [ 1  4 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        15\n",
      "           1       0.76      1.00      0.86        16\n",
      "           2       1.00      0.78      0.88        23\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.90      0.91      0.89        54\n",
      "weighted avg       0.91      0.89      0.89        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 1100813.69it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 915120.87it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1212810.80it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1372681.31it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1267090.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Euclidean Distance: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.9074074074074074\n",
      "Testing F1 score: 0.9052998774035359\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  0  0]\n",
      " [ 0 16  0]\n",
      " [ 2  3 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94        15\n",
      "           1       0.84      1.00      0.91        16\n",
      "           2       1.00      0.78      0.88        23\n",
      "\n",
      "    accuracy                           0.91        54\n",
      "   macro avg       0.91      0.93      0.91        54\n",
      "weighted avg       0.92      0.91      0.91        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Euclidean Distance: \n",
      "Testing accuracy 0.9074074074074074\n",
      "Testing F1 score: 0.9052998774035359\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[15  0  0]\n",
      " [ 0 16  0]\n",
      " [ 2  3 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94        15\n",
      "           1       0.84      1.00      0.91        16\n",
      "           2       1.00      0.78      0.88        23\n",
      "\n",
      "    accuracy                           0.91        54\n",
      "   macro avg       0.91      0.93      0.91        54\n",
      "weighted avg       0.92      0.91      0.91        54\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  216\n",
      "No. of Testing Dataset in Fold  5 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 511558.25it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1041344.44it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 855495.43it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1256546.00it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 584873.90it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1023694.54it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 597999.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using KNN with Euclidean Distance: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8333333333333334\n",
      "Testing F1 score: 0.8285127588711091\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 1 20  0]\n",
      " [ 3  3 11]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.82        16\n",
      "           1       0.83      0.95      0.89        21\n",
      "           2       0.92      0.65      0.76        17\n",
      "\n",
      "    accuracy                           0.83        54\n",
      "   macro avg       0.84      0.82      0.82        54\n",
      "weighted avg       0.84      0.83      0.83        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 1048576.00it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 575218.83it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 800326.56it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 835765.37it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1499949.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using KNN with Euclidean Distance: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8333333333333334\n",
      "Testing F1 score: 0.8344086021505376\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 3 18  0]\n",
      " [ 2  2 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.88      0.80        16\n",
      "           1       0.86      0.86      0.86        21\n",
      "           2       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.83        54\n",
      "   macro avg       0.84      0.83      0.83        54\n",
      "weighted avg       0.84      0.83      0.83        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using KNN with Euclidean Distance: \n",
      "Testing accuracy 0.8333333333333334\n",
      "Testing F1 score: 0.8344086021505376\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 3 18  0]\n",
      " [ 2  2 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.88      0.80        16\n",
      "           1       0.86      0.86      0.86        21\n",
      "           2       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.83        54\n",
      "   macro avg       0.84      0.83      0.83        54\n",
      "weighted avg       0.84      0.83      0.83        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: {}\\n\\n\" .format(total_fold))\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "        \n",
    "    knn_dbow = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\n",
    "    knn_dbow.fit(X_trained, y_trained)\n",
    "    y_pred = knn_dbow.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using KNN with Euclidean Distance: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    knn_dm = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\n",
    "    knn_dm.fit(X_trained, y_trained)\n",
    "    y_pred = knn_dm.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using KNN with Euclidean Distance: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    knn_mixed = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\n",
    "    knn_mixed.fit(X_trained, y_trained)\n",
    "    y_pred = knn_mixed.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PAIRED Model Using KNN with Euclidean Distance: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold With Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold No.: 5\n",
      "\n",
      "\n",
      "No. of Training Dataset in Fold  1 :  216\n",
      "No. of Testing Dataset in Fold  1 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 607625.53it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 963797.51it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1254805.63it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1354214.74it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 916973.34it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 541847.89it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 632218.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Gaussian Naive Bayes: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8518518518518519\n",
      "Testing F1 score: 0.8509433492632418\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  2  3]\n",
      " [ 0 20  1]\n",
      " [ 0  2 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.72      0.84        18\n",
      "           1       0.83      0.95      0.89        21\n",
      "           2       0.76      0.87      0.81        15\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.87      0.85      0.85        54\n",
      "weighted avg       0.87      0.85      0.85        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 507830.53it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 831928.07it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1449551.46it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1187378.33it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 925403.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Gaussian Naive Bayes: \n",
      "Fold No.:  1\n",
      "Testing accuracy 0.8518518518518519\n",
      "Testing F1 score: 0.8509433492632418\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  2  3]\n",
      " [ 0 20  1]\n",
      " [ 0  2 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.72      0.84        18\n",
      "           1       0.83      0.95      0.89        21\n",
      "           2       0.76      0.87      0.81        15\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.87      0.85      0.85        54\n",
      "weighted avg       0.87      0.85      0.85        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using Gaussian Naive Bayes: \n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8695530890298332\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  2  2]\n",
      " [ 1 19  1]\n",
      " [ 0  1 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.78      0.85        18\n",
      "           1       0.86      0.90      0.88        21\n",
      "           2       0.82      0.93      0.87        15\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.87      0.87      0.87        54\n",
      "weighted avg       0.88      0.87      0.87        54\n",
      "\n",
      "No. of Training Dataset in Fold  2 :  216\n",
      "No. of Testing Dataset in Fold  2 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 967916.31it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 959713.63it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 554449.00it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1470729.97it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1139584.48it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1225940.01it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1254805.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Gaussian Naive Bayes: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.9074074074074074\n",
      "Testing F1 score: 0.9072310405643739\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[12  2  0]\n",
      " [ 0 21  1]\n",
      " [ 2  0 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86        14\n",
      "           1       0.91      0.95      0.93        22\n",
      "           2       0.94      0.89      0.91        18\n",
      "\n",
      "    accuracy                           0.91        54\n",
      "   macro avg       0.90      0.90      0.90        54\n",
      "weighted avg       0.91      0.91      0.91        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 978368.97it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 963797.51it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 678121.01it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1263556.02it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1338212.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Gaussian Naive Bayes: \n",
      "Fold No.:  2\n",
      "Testing accuracy 0.9074074074074074\n",
      "Testing F1 score: 0.9072310405643739\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[12  2  0]\n",
      " [ 0 21  1]\n",
      " [ 2  0 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86        14\n",
      "           1       0.91      0.95      0.93        22\n",
      "           2       0.94      0.89      0.91        18\n",
      "\n",
      "    accuracy                           0.91        54\n",
      "   macro avg       0.90      0.90      0.90        54\n",
      "weighted avg       0.91      0.91      0.91        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using Gaussian Naive Bayes: \n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8888888888888888\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[12  2  0]\n",
      " [ 0 20  2]\n",
      " [ 2  0 16]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86        14\n",
      "           1       0.91      0.91      0.91        22\n",
      "           2       0.89      0.89      0.89        18\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.89      0.89      0.89        54\n",
      "weighted avg       0.89      0.89      0.89        54\n",
      "\n",
      "No. of Training Dataset in Fold  3 :  216\n",
      "No. of Testing Dataset in Fold  3 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 831164.83it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1029510.98it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1024852.56it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1023694.54it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1081109.38it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 150243.73it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 616305.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Gaussian Naive Bayes: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8891557995881948\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  0  2]\n",
      " [ 1 14  2]\n",
      " [ 1  0 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87        15\n",
      "           1       1.00      0.82      0.90        17\n",
      "           2       0.84      0.95      0.89        22\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.90      0.88      0.89        54\n",
      "weighted avg       0.90      0.89      0.89        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 971028.58it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1261796.19it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 813258.23it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1428974.23it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1072153.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Gaussian Naive Bayes: \n",
      "Fold No.:  3\n",
      "Testing accuracy 0.8888888888888888\n",
      "Testing F1 score: 0.8891557995881948\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  0  2]\n",
      " [ 1 14  2]\n",
      " [ 1  0 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87        15\n",
      "           1       1.00      0.82      0.90        17\n",
      "           2       0.84      0.95      0.89        22\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.90      0.88      0.89        54\n",
      "weighted avg       0.90      0.89      0.89        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using Gaussian Naive Bayes: \n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8700617283950618\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[13  0  2]\n",
      " [ 1 13  3]\n",
      " [ 1  0 21]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87        15\n",
      "           1       1.00      0.76      0.87        17\n",
      "           2       0.81      0.95      0.88        22\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.89      0.86      0.87        54\n",
      "weighted avg       0.88      0.87      0.87        54\n",
      "\n",
      "No. of Training Dataset in Fold  4 :  216\n",
      "No. of Testing Dataset in Fold  4 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 1063344.68it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 963797.51it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 699590.47it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 967916.31it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1230937.04it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1270644.69it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 529497.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Gaussian Naive Bayes: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8691893506409635\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  0]\n",
      " [ 0 16  0]\n",
      " [ 2  4 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        15\n",
      "           1       0.76      1.00      0.86        16\n",
      "           2       1.00      0.74      0.85        23\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.88      0.89      0.87        54\n",
      "weighted avg       0.89      0.87      0.87        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 929199.66it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1119863.61it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 944702.47it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 682720.17it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1303553.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Gaussian Naive Bayes: \n",
      "Fold No.:  4\n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8691893506409635\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  0]\n",
      " [ 0 16  0]\n",
      " [ 2  4 17]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        15\n",
      "           1       0.76      1.00      0.86        16\n",
      "           2       1.00      0.74      0.85        23\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.88      0.89      0.87        54\n",
      "weighted avg       0.89      0.87      0.87        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using Gaussian Naive Bayes: \n",
      "Testing accuracy 0.8703703703703703\n",
      "Testing F1 score: 0.8717019612502105\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  0  1]\n",
      " [ 0 15  1]\n",
      " [ 0  5 18]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.97        15\n",
      "           1       0.75      0.94      0.83        16\n",
      "           2       0.90      0.78      0.84        23\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.88      0.88      0.88        54\n",
      "weighted avg       0.88      0.87      0.87        54\n",
      "\n",
      "No. of Training Dataset in Fold  5 :  216\n",
      "No. of Testing Dataset in Fold  5 :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 894343.20it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1167486.68it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 751842.04it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1091529.72it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1261796.19it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 847492.67it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 778993.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DBOW Using Gaussian Naive Bayes: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8148148148148148\n",
      "Testing F1 score: 0.8148148148148148\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 1 17  3]\n",
      " [ 1  3 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88        16\n",
      "           1       0.81      0.81      0.81        21\n",
      "           2       0.76      0.76      0.76        17\n",
      "\n",
      "    accuracy                           0.81        54\n",
      "   macro avg       0.82      0.82      0.82        54\n",
      "weighted avg       0.81      0.81      0.81        54\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 954657.18it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 838084.80it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 159642.23it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1157049.38it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1001071.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PV_DM Using Gaussian Naive Bayes: \n",
      "Fold No.:  5\n",
      "Testing accuracy 0.8148148148148148\n",
      "Testing F1 score: 0.8148148148148148\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  1  1]\n",
      " [ 1 17  3]\n",
      " [ 1  3 13]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88        16\n",
      "           1       0.81      0.81      0.81        21\n",
      "           2       0.76      0.76      0.76        17\n",
      "\n",
      "    accuracy                           0.81        54\n",
      "   macro avg       0.82      0.82      0.82        54\n",
      "weighted avg       0.81      0.81      0.81        54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOR PAIRED Model Using Gaussian Naive Bayes: \n",
      "Testing accuracy 0.8333333333333334\n",
      "Testing F1 score: 0.8346707818930041\n",
      "\n",
      "Testing Confusion Matrix: \n",
      "[[14  0  2]\n",
      " [ 1 17  3]\n",
      " [ 1  2 14]] \n",
      "\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88        16\n",
      "           1       0.89      0.81      0.85        21\n",
      "           2       0.74      0.82      0.78        17\n",
      "\n",
      "    accuracy                           0.83        54\n",
      "   macro avg       0.84      0.84      0.83        54\n",
      "weighted avg       0.84      0.83      0.83        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "print(\"Total Fold No.: {}\\n\\n\" .format(total_fold))\n",
    "for train_index, test_index in ten_fold.split(X):\n",
    "#     print(\"Train Fold No.: \", train_index, \" Test Fold No.: \", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"No. of Training Dataset in Fold \", fold_no, \": \", len(X_train))\n",
    "    print(\"No. of Testing Dataset in Fold \", fold_no, \": \", len(X_test))\n",
    "    \n",
    "    X_train = list(zip(X_train,y_train))\n",
    "    X_train = pd.DataFrame(X_train, columns=['story', 'category'])\n",
    "    \n",
    "    X_test = list(zip(X_test,y_test))\n",
    "    X_test = pd.DataFrame(X_test, columns=['story', 'category'])\n",
    "    \n",
    "    train_tagged = X_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    test_tagged = X_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['story']), tags=[r['category']]), axis=1)\n",
    "    \n",
    "    #PV_DBOW using DT with Entropy\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    \n",
    "    def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs.values\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dbow, test_tagged)\n",
    "        \n",
    "    nb_gaussian_pv_dbow = GaussianNB()\n",
    "    nb_gaussian_pv_dbow.fit(X_trained, y_trained)\n",
    "    y_pred = nb_gaussian_pv_dbow.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DBOW Using Gaussian Naive Bayes: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PV_DM\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    \n",
    "    y_trained, X_trained = vec_for_learning(model_dmm, train_tagged)\n",
    "    y_tested, X_tested = vec_for_learning(model_dmm, test_tagged)\n",
    "    \n",
    "    nb_gaussian_pv_dm = GaussianNB()\n",
    "    nb_gaussian_pv_dm.fit(X_trained, y_trained)\n",
    "    y_pred_pv_dm = nb_gaussian_pv_dm.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PV_DM Using Gaussian Naive Bayes: \")\n",
    "    print(\"Fold No.: \", fold_no)\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #FOR PAIRED_MODEL\n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(new_model, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(new_model, test_tagged)\n",
    "    \n",
    "    nb_gaussian_mixed = GaussianNB()\n",
    "    nb_gaussian_mixed.fit(X_trained, y_trained)\n",
    "    y_pred = nb_gaussian_mixed.predict(X_tested)\n",
    "    \n",
    "    print(\"FOR PAIRED Model Using Gaussian Naive Bayes: \")\n",
    "    print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    print('\\nTesting Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "    print('Testing Classification Report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
